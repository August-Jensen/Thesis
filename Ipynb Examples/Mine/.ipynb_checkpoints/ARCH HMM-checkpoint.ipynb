{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing packages we need\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt #pyplot is used to plot the data\n",
    "import scipy.optimize as opt #used to numerically optimize\n",
    "from datetime import time, timedelta, datetime\n",
    "import scipy.stats as stats\n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GaussianDensity(t,mu, phi, sigma):\n",
    "    epsilon = y[t] - mu - phi * y[t-1]\n",
    "    density = stats.norm.pdf(epsilon, 0, sigma)\n",
    "    return density\n",
    "\n",
    "# AR(1) likelihood contribution at time t\n",
    "def ar1_likelihood_contribution(t, mu, phi, sigma):\n",
    "    if t == 0:\n",
    "        return 0  # No contribution for the first element\n",
    "    eps = y[t] - (mu + phi * y[t-1])\n",
    "    log_likelihood = -0.5 * np.log(2 * np.pi * sigma**2) - (1/(2 * sigma**2)) * eps**2\n",
    "    return -log_likelihood  # Negative likelihood for minimization\n",
    "\n",
    "# GARCH(1,1) likelihood contribution at time t\n",
    "def GarchDensity(t, sigma2, omega, alpha, beta):\n",
    "    #print(t, simga2)\n",
    "    if t == 0:\n",
    "        sigma2[t] = 0.5 # Initial variance\n",
    "        return 0, sigma2[t]\n",
    "    else:\n",
    "        sigma2[t] = omega + alpha * y[t-1]**2 + beta * sigma2[t-1]\n",
    "    log_likelihood = -0.5 * (np.log(sigma2[t]) + y[t]**2 / sigma2[t])\n",
    "    return -log_likelihood  # Negative likelihood for minimization\n",
    "\n",
    "# Function to calculate total likelihood\n",
    "def total_likelihood(likelihood_function, params, data):\n",
    "    if likelihood_function == ar1_likelihood_contribution:\n",
    "        return sum(likelihood_function(params, data, t) for t in range(len(data)))\n",
    "    elif likelihood_function == garch_likelihood_contribution:\n",
    "        sigma2 = np.zeros(len(data))\n",
    "        return sum(likelihood_function(params, data, sigma2, t) for t in range(len(data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Initial parameter guesses\n",
    "# initial_ar1_params = [0, 0, 1]  # [alpha, beta, sigma]\n",
    "# initial_garch_params = [0.01, 0.1, 0.8]  # [omega, alpha, beta]\n",
    "\n",
    "# # Minimize the negative total likelihood\n",
    "# ar1_result = minimize(lambda params: total_likelihood(ar1_likelihood_contribution, params, Y), \n",
    "#                       initial_ar1_params, method='L-BFGS-B')\n",
    "# garch_result = minimize(lambda params: total_likelihood(garch_likelihood_contribution, params, Y), \n",
    "#                         initial_garch_params, method='L-BFGS-B')\n",
    "\n",
    "# print(\"AR(1) model parameters:\", ar1_result.x)\n",
    "# print(\"GARCH(1,1) model parameters:\", garch_result.x)\n",
    "# # def GaussianDensity(t,mu, phi, sigma):\n",
    "# #     return np.exp(-0.5*np.log(np.pi)-0.5*np.log(sigma)-0.5*((y[t] - mu - phi * y[t-1])**2)/sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(gamma):\n",
    "    #parameters\n",
    "    p00    = gamma[0]\n",
    "    p11    = gamma[1]\n",
    "    omega = gamma[2:4];\n",
    "    alpha = gamma[4:6];\n",
    "    beta = gamma[6:8]**2;\n",
    "    T      = len(y)\n",
    "    # print(mu, phi, sigma2)\n",
    "    #//transition matrix\n",
    "    P = np.zeros([2,2])\n",
    "    P[0]=p00, 1-p11\n",
    "    P[1]=1-p00, p11\n",
    "\n",
    "    #//bookkeeping\n",
    "    xi_10      = np.zeros([2,T+1])\n",
    "    xi_11      = np.zeros([2,T])\n",
    "    xi_1T      = np.zeros([2,T])\n",
    "    sigma2     = np.zeros([2,T])\n",
    "    lik        = np.zeros(T)\n",
    "\n",
    "    #//regression:\n",
    "    A  = np.vstack(((np.identity(2)-P),np.ones([1,2])))\n",
    "    pi_first = np.linalg.inv(A.T.dot(A)).dot(A.T)\n",
    "    pi_second=np.vstack((np.zeros([2,1]),np.ones([1,1])))\n",
    "    pi=pi_first.dot(pi_second)\n",
    "    xi_10[[0,1],0] = pi.T\n",
    "    # print(f'Pi: {pi.T}')\n",
    "    #//forward filter recursion\n",
    "    eta=np.zeros(2)\n",
    "\n",
    "\n",
    "    for t in range(T):\n",
    "        #//state densities\n",
    "        eta[0]=GarchDensity(t, omega[0], alpha[0], beta[0])\n",
    "        eta[1]=GarchDensity(t, omega[1], alpha[1], beta[1])\n",
    "        \n",
    "        #likelihood\n",
    "        lik[t] = xi_10[0,t] * eta[0] + xi_10[1,t] * eta[1]\n",
    "        \n",
    "        #filtering\n",
    "        num0=eta[0]*xi_10[0,t]/(eta[0]*xi_10[0,t]+eta[1]*xi_10[1,t])\n",
    "        num1=eta[1]*xi_10[1,t]/(eta[0]*xi_10[0,t]+eta[1]*xi_10[1,t])\n",
    "\n",
    "        #prediction\n",
    "        xi_10[[0,1],t+1] = P.dot(xi_11[[0,1],t])\n",
    "    print(np.sum(lik))\n",
    "    return -np.sum(lik) #We wish to minimize the likelihood in our scipy opt function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.59160816]\n",
      " [ 0.31108326]\n",
      " [ 0.05453718]\n",
      " ...\n",
      " [ 1.27496879]\n",
      " [ 0.86677408]\n",
      " [-0.25643447]]\n",
      "Now transformed:\n",
      "[ 1.59160816  0.31108326  0.05453718 ...  1.27496879  0.86677408\n",
      " -0.25643447]\n"
     ]
    }
   ],
   "source": [
    "data=np.genfromtxt('SP_500.csv', delimiter=',',usecols=np.arange(0,4)) #loading in first 4 columns\n",
    "y = data[15097:, 3:4]*100 # 100 times log-returns of the S&P 500 index. January 4, 2010 - till end\n",
    "print(y)\n",
    "print(\"Now transformed:\")\n",
    "y=y.T[0,:] #unpacking numpy array\n",
    "print(y)\n",
    "T = len(y) #length of time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'tuple'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# y = data['pct'].to_numpy()\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#sigma2 = np.var(y) #used for initial guesses for sigma2 vals\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#mean = np.mean(y)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# initial_garch_params = [0.01, 0.1, 0.8]  # [omega, alpha, beta]\u001b[39;00m\n\u001b[1;32m      5\u001b[0m Gamma0  \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.95\u001b[39m,\u001b[38;5;241m0.95\u001b[39m, \u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m0.7\u001b[39m]) \u001b[38;5;66;03m#initial guesses\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m res\u001b[38;5;241m=\u001b[39m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlikelihood\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGamma0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.9999\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.9999\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.9999\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.9999\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#optimizing. We use L-BFGS-B as it allows for bounds and can compute the standard errors (from the inverse hessian) right away\u001b[39;00m\n\u001b[1;32m      7\u001b[0m res\u001b[38;5;241m.\u001b[39mx\n\u001b[1;32m      8\u001b[0m v_hessian\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mhess_inv\u001b[38;5;241m.\u001b[39mtodense() \u001b[38;5;66;03m#retrieves the negative inverse hessian matrix (note we have minimized the negative log likelihood function)\u001b[39;00m\n",
      "File \u001b[0;32m~/.venvs/jupyter/lib/python3.11/site-packages/scipy/optimize/_minimize.py:710\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    707\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    708\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 710\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    713\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    714\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/.venvs/jupyter/lib/python3.11/site-packages/scipy/optimize/_lbfgsb_py.py:307\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m         iprint \u001b[38;5;241m=\u001b[39m disp\n\u001b[0;32m--> 307\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_scalar_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m func_and_grad \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun_and_grad\n\u001b[1;32m    313\u001b[0m fortran_int \u001b[38;5;241m=\u001b[39m _lbfgsb\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mintvar\u001b[38;5;241m.\u001b[39mdtype\n",
      "File \u001b[0;32m~/.venvs/jupyter/lib/python3.11/site-packages/scipy/optimize/_optimize.py:383\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    379\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[0;32m--> 383\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43mScalarFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[0;32m~/.venvs/jupyter/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:158\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl \u001b[38;5;241m=\u001b[39m update_fun\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# Gradient evaluation\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(grad):\n",
      "File \u001b[0;32m~/.venvs/jupyter/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[0;32m--> 251\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.venvs/jupyter/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venvs/jupyter/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "Cell \u001b[0;32mIn[23], line 35\u001b[0m, in \u001b[0;36mlikelihood\u001b[0;34m(gamma)\u001b[0m\n\u001b[1;32m     30\u001b[0m eta\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(T):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m#//state densities\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     \u001b[43meta\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m=\u001b[39mGarchDensity(t,sigma2, omega[\u001b[38;5;241m0\u001b[39m], alpha[\u001b[38;5;241m0\u001b[39m], beta[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     36\u001b[0m     eta[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m=\u001b[39mGarchDensity(t,sigma2, omega[\u001b[38;5;241m1\u001b[39m], alpha[\u001b[38;5;241m1\u001b[39m], beta[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m#likelihood\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "# y = data['pct'].to_numpy()\n",
    "#sigma2 = np.var(y) #used for initial guesses for sigma2 vals\n",
    "#mean = np.mean(y)\n",
    "# initial_garch_params = [0.01, 0.1, 0.8]  # [omega, alpha, beta]\n",
    "Gamma0  = np.array([0.95,0.95, 0.01, 0.1, 0.2, 0.1, 0.8, 0.7]) #initial guesses\n",
    "res=opt.minimize(likelihood, Gamma0, method='L-BFGS-B',bounds=((0.001,0.9999),(0.001,0.9999),(0.001,None),(0.001,None),(0.001,0.9999),(0.001,0.9999),(0.01,1),(0.01,1))) #optimizing. We use L-BFGS-B as it allows for bounds and can compute the standard errors (from the inverse hessian) right away\n",
    "res.x\n",
    "v_hessian=res.hess_inv.todense() #retrieves the negative inverse hessian matrix (note we have minimized the negative log likelihood function)\n",
    "se_hessian=np.sqrt(np.diagonal(v_hessian))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P11=0.95, std.errors=1.0\n",
      "P22=0.95, std.errors=1.0\n",
      "mu1=0.01, std.errors=1.0\n",
      "mu2=0.1, std.errors=1.0\n",
      "phi1=0.2, std.errors=1.0\n",
      "phi2=0.1, std.errors=1.0\n",
      "sigma1=0.8, std.errors=1.0\n",
      "sigma2=0.7, std.errors=1.0\n"
     ]
    }
   ],
   "source": [
    "#result of optimization\n",
    "Gamma_hat=res.x\n",
    "se=se_hessian\n",
    "print('P11='+str(Gamma_hat[0])+', std.errors='+str(se[0]))\n",
    "print('P22='+str(Gamma_hat[1])+', std.errors='+str(se[1]))\n",
    "print('mu1='+str(Gamma_hat[2])+', std.errors='+str(se[2]))\n",
    "print('mu2='+str(Gamma_hat[3])+', std.errors='+str(se[3]))\n",
    "print('phi1='+str(Gamma_hat[4])+', std.errors='+str(se[4]))\n",
    "print('phi2='+str(Gamma_hat[5])+', std.errors='+str(se[5]))\n",
    "print('sigma1='+str(Gamma_hat[6])+', std.errors='+str(se[6]))\n",
    "print('sigma2='+str(Gamma_hat[7])+', std.errors='+str(se[7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting returns and filtered probabilities\n",
    "gamma=Gamma_hat\n",
    "#parameters\n",
    "#parameters\n",
    "p00    = gamma[0]\n",
    "p11    = gamma[1]\n",
    "mu = gamma[2:4];\n",
    "phi = gamma[4:6];\n",
    "sigma2 = gamma[6:8]**2;\n",
    "T      = len(y)\n",
    "#//transition matrix\n",
    "P = np.zeros([2,2])\n",
    "P[0]=p00, 1-p11\n",
    "P[1]=1-p00, p11\n",
    "    \n",
    "#//bookkeeping\n",
    "xi_10      = np.zeros([2,T+1])\n",
    "xi_11      = np.zeros([2,T])\n",
    "xi_1T      = np.zeros([2,T])\n",
    "lik        = np.zeros(T)\n",
    "\n",
    "#//regression:\n",
    "A  = np.vstack(((np.identity(2)-P),np.ones([1,2])))\n",
    "pi_first = np.linalg.inv(A.T.dot(A)).dot(A.T)\n",
    "pi_second=np.vstack((np.zeros([2,1]),np.ones([1,1])))\n",
    "pi=pi_first.dot(pi_second)\n",
    "xi_10[[0,1],0] = pi.T\n",
    "#//forward filter recursion\n",
    "eta=np.zeros(2)\n",
    "for t in range(T):\n",
    "    #//state densities\n",
    "    eta[0]=GaussianDensity(t,mu[0], phi[0], sigma2[0])\n",
    "    eta[1]=GaussianDensity(t,mu[1], phi[1], sigma2[1])\n",
    "        \n",
    "    #likelihood\n",
    "    #print(np.log(xi_10[[0,1],t]))\n",
    "    lik[t]   = np.log(xi_10[0,t]*eta[0]+xi_10[1,t]*eta[1])\n",
    "        \n",
    "    #filtering\n",
    "    num0=eta[0]*xi_10[0,t]/(eta[0]*xi_10[0,t]+eta[1]*xi_10[1,t])\n",
    "    num1=eta[1]*xi_10[1,t]/(eta[0]*xi_10[0,t]+eta[1]*xi_10[1,t])\n",
    "    xi_11[[0,1],t] = num0,num1\n",
    "\n",
    "    #prediction\n",
    "    xi_10[[0,1],t+1] = P.dot(xi_11[[0,1],t])\n",
    "    \n",
    "    #Backward smoother (not needed for likelihood)\n",
    "    xi_1T[:,T-1]=xi_11[:,T-1]\n",
    "    for t in range(T-2,0,-1):\n",
    "        xi_1T[:,t]=xi_11[:,t]*(P.T.dot(xi_1T[:,t+1]/xi_10[:,t+1]))\n",
    "        \n",
    "vol=np.zeros(len(y))\n",
    "for i in range(T):\n",
    "    vol[i]=xi_11[[0],i]*sigma2[0]+ (1-xi_11[[0],i])*sigma2[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(2, figsize=(14,7))\n",
    "#fig.set_figheight=(9)\n",
    "#fig.set_figwidth=(16)\n",
    "fig.suptitle('log-return and filtered volatility')\n",
    "ax[0].plot(y,color='r')\n",
    "ax[1].plot(np.sqrt(vol))\n",
    "\n",
    "#Setting titles\n",
    "ax[0].title.set_text('Log-return, $x_t$')\n",
    "ax[1].title.set_text('Filtered volatility, $E[\\sigma_t|x_t,x_{t-1},...,x_1]$')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicted state probability, Filtered state probability and smoothed state probability\n",
    "fig, ax=plt.subplots(3, figsize=(16,9))\n",
    "#fig.tight_layout() \n",
    "\n",
    "#Adjusting size between subplots\n",
    "fig.subplots_adjust(left=None, bottom=0.025, right=None, top=None, wspace=None, hspace=None)\n",
    "#default\n",
    "#left  = 0.125  # the left side of the subplots of the figure\n",
    "#right = 0.9    # the right side of the subplots of the figure\n",
    "#bottom = 0.1   # the bottom of the subplots of the figure\n",
    "#top = 0.9      # the top of the subplots of the figure\n",
    "#wspace = 0.2   # the amount of width reserved for blank space between subplots\n",
    "#hspace = 0.2   # the amount of height reserved for white space between subplots\n",
    "\n",
    "\n",
    "ax[0].plot(1-xi_10[0,:])\n",
    "ax[1].plot(1-xi_11[0,:])\n",
    "ax[2].plot(1-xi_1T[0,:])\n",
    "\n",
    "#Setting limits on x axis\n",
    "ax[0].set_xlim(0, T)\n",
    "ax[1].set_xlim(0, T)\n",
    "ax[2].set_xlim(0, T)\n",
    "\n",
    "#Setting titles\n",
    "ax[0].title.set_text('Predicted state probability, $P(s_t=1|x_{t-1},x_{t-2},...,x_{1})$')\n",
    "ax[1].title.set_text('Filtered state probability, $P(s_t=1|x_{t},x_{t-1},...,x_{1})$')\n",
    "ax[2].title.set_text('Smoothed state probability, $P(s_t=1|x_{T},x_{T-1},...,x_{1})$')\n",
    "\n",
    "#Setting lines at 0 and 1\n",
    "ax[0].axhline(0,color='black', linestyle=\"--\")\n",
    "ax[0].axhline(1,color='black', linestyle=\"--\")\n",
    "\n",
    "ax[1].axhline(0,color='black', linestyle=\"--\")\n",
    "ax[1].axhline(1,color='black', linestyle=\"--\")\n",
    "\n",
    "ax[2].axhline(0,color='black', linestyle=\"--\")\n",
    "ax[2].axhline(1,color='black', linestyle=\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
