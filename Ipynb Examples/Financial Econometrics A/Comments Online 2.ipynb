{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24092ed2-3400-442c-8f3b-ee97fa80cc68",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Expectation Maximization Algorithm Report\"\n",
    "author: \"Dominic Scruton\"\n",
    "date: \"19 February 2021\"\n",
    "output: html_document\n",
    "---\n",
    "\n",
    "```{r setup, include=FALSE}\n",
    "knitr::opts_chunk$set(echo = TRUE)\n",
    "```\n",
    "\n",
    "# Introduction\n",
    "\n",
    "## Problem Description\n",
    "\n",
    "The Expectation-Maximization is a semi-supervised cluster method used to categorize observations based on the characteristics of other observations whose cluster/ group is known. Hence, the method is semi-supervised, since one only knows which cluster an observation belongs to for a subset of observations, with the goal to use information from such obsrvations to inform the cluster for observations whose cluster is unknown.\n",
    "\n",
    "For example, we can consider a case in the insurance industry\n",
    "\n",
    "A common modelling problem is how to estimate a joint probability distribution for a dataset. The method of Maximum Likelihhod selects the parameters that maximize the probability of seeing the data. A disadvantage of Maximum Likelihood is that it assumes that all variables that are relevant to the problem of density estimation and clustering are present. However, there may be some datasets where only the relevant variables are observed and others are not, and although they influence other random variables in the dataset, they remain hidden. These are known as _latent_ variables. \"Many real-world problems have hidden variables (sometimes called latent variables), which are not observable in the data that are available for learning\" (Russell, 2015). \n",
    "The Expectation-Maximization Algorithm is \"a general technique for findings maximum likelihood estimators in Latent Variable models\". It is linked to both Hidden Markov Models. Suppose we have a sequence of choices made that are independent. The processes used to generate each sequence of choices represent a latent variable, e.g. process $k \\in K$. In the EM-Algorithm, the Expectation Step estimates probability that each sequence belongs to each of the k process generating steps. The Maximization Step then optimizes the parameters of the distribtuion using maximum likelihood. \n",
    "\n",
    "## Latent Variables and Processes\n",
    "\n",
    "The Expectation-Maximization Algorithm is particularly useful for identifying and modelling latent variables. \n",
    "\n",
    "## Mixture Models\n",
    "\n",
    "The most common use of Expectation-Maximization Algorithms in the literature is for the estimation of density functions and the use of such density functions to cluster observations. \n",
    "\n",
    "A _Gaussian Mixture Model_ (GMM) is a probabilistic model that assumes that the instances were generated from a mixture of several Gaussian distributions whose parameters are unknown. All the instances from a single Gaussian distribution form a cluster that typically looks like an ellipsoid. Each cluster can have a different ellipsoidal shape, size, density and orientationm (since the mean and standard deviation parameters for each mixture are themselves vectors). \n",
    "\n",
    "Let $X_i$ represent the variables for observation $i$ and let $G_i$ by the cluster $i$ is in. We assume $X_i$ is normally distributed with mean $\\textbf{\\mu_k}$ and variance $\\textbf{\\sigma_k^2}$ when $G_i = k$. We also let $\\lambda_k$ be the probability that the observation is in group $k$.\n",
    "\n",
    "The (multivariate) probability density function for $X_i$ is then given by:\n",
    "\n",
    "$$f(x) = \\sum_{k = 1}^K \\lambda_k \\phi(x|\\mu_k, \\sigma_k^2)$$\n",
    "\n",
    "where $\\phi(.| \\mu_k, \\sigma_k^2)$ is a normal probability density function with mean $\\mu_k$ and variance $\\sigma_k^2$. Not that the above formulation could be applied to any parametrized distribution. The aim is thus to identify the parameters $\\lambda_k$, $\\mu_k$ and $\\sigma_k^2$ for every group, k. These estimates are then used to categorize the remaining observations with no prior cluster assignment. Note that we must specify the number of groups a priori. \n",
    "\n",
    "The d-dimensional Multivariate Normal distribution is given as follows:\n",
    "\n",
    "$$f(\\textbf{x}; \\boldsymbol{\\mu}, \\boldsymbol{\\sigma}) = \\frac{1}{(2 \\pi)^{\\frac{d}{2}}|\\Sigma|^{\\frac{1}{2}}} exp(- \\frac{1}{2}(\\mathbf{x} - \\boldsymbol{\\mu})^T \\Sigma^{-1}(\\mathbf{x} - \\boldsymbol{\\mu}))$$\n",
    "\n",
    "In order for a vector to be Multivariate Normally distributed, the following must apply:\n",
    "\n",
    "\n",
    "To check whether a random vector is Multivariate Normally distributed, we can consider the Mahalanobis Distance. One can also transform individual variables to ensure they are approximately multivariate normally distibuted. \n",
    "\n",
    "Whilst using a unsupervised Clustering algorithm may also be useful, it is worthwhile using the property of Normality for the data in order to identify such clusters and find a distribution for each cluster.\n",
    "\n",
    "# Expectation-Maximization Algorithm\n",
    "\n",
    "The Expectation-Maximization Algorithm consists of distributional assumptions regarding the clustering of the data, with each cluster typically considered to be (multivariate) normal. To solve this problem, we assign values of each explanatory variable to each cluster, estimate the mean and standard deviation for these groups and then iteratively consider whether to change the grouping or not. \n",
    "\n",
    "The model depends on some unobserved latent variable.\n",
    "\n",
    "## Pseudo-Code\n",
    "\n",
    "The EM-Algorithm consists of assigning an initial grouping, followed by iteration between an expectation and maximization step, with a test for convergence determining when iteration can stop.  \n",
    "The pseudo-code for the general implementation of the Expectation-Maximization function is as follows:\n",
    "\n",
    "1) Initialize \n",
    "2) Repeat until convergence:\n",
    "  a) Expectation Step\n",
    "  b) Maximization Step\n",
    "  c) Check for Convergence\n",
    "  \n",
    "### Initialize\n",
    "\n",
    "Suppose we have target category $\\textbf{y}$, which for a proportion of observations, $p$, is known and for the other $N (1 - p)$ observations is unknown, where $N$ is the total number of observations. Each observation has an associated set of other variables, $\\textbf{x}$. For example, $y$ could be the age group of the identified fish and $x$ could be the length of each fish. \n",
    "\n",
    "1) Using data from the observations of known category, assign labels (categories) to each of the $N (1 - p)$ observations in the dataset that have unknown $y$.\n",
    "2) Given these initial assignments, compute initial values of $\\hat{\\mu_k}$ and $\\hat{\\sigma}_k$ for each cluster, $k = 1,...K$\n",
    "3) Compute Initial Values of $\\hat{\\lambda}_k$\n",
    "\n",
    "In multiple dimensions, we have the following:\n",
    "\n",
    "There are several ways one could initialize model parameters. Random assignment is a simple approach that randomly assigns a cluster to each observation with a priori unknown cluster. A problem with this approach is that we expect to assign clusters in equal proportion to the observations with known cluster. However, a better alternative is to assign observations using a standard cluster method, for example K-Means. This method is non-parametric in that it does NOT assume a distribution for fish lengths. We can use pre-existing packages to carry out this step for initialization of model parameters. The default initialization is therefore set to use the K-Means cluster algorithm.\n",
    "\n",
    "Other initialization methods include fitting supervised classification models for the observations with iknown cluster group and using such models to predict the cluster group for the remaining observations. A variety of models could be used, with a simpler multinomial model perhaps more time-effective for initialization. The bene\n",
    "\n",
    "We compare the performance of different assignments. This step is arguably the most crucial in ensuring good performance of the EM-Algorithm.\n",
    "\n",
    "\n",
    "### Expectation Step\n",
    "\n",
    "The expectation step quantifies the chance an observation belongs to each group. We do this by calculating the probability that $\\textbf{x}_i$ belongs to group k using Bayes Rule:\n",
    "\n",
    "$$P(x_i \\in  k | x_i) = \\frac{P(x_i | x_i \\in k)P(k)}{P(x_i)}$$\n",
    "This posterior probability that observation $i$ belongs to group $k$ is computed for each observation $x_i$ based on our current best estimate of the mean and variance parameters. $P(x_i | x_i \\in k)$ is given by the (Gaussian Multivariate) density function with mean $\\mu_k$ and variance $\\sigma_k^2$. $P(x_i)$ is the total probability, summed over all groups, found using the Partition Theorem:\n",
    "\n",
    "$$P(x_i) = \\sum_{k = 1}^K P(x_i|x_i \\in k) P(k)$$\n",
    "\n",
    "\n",
    "### Maximization Step\n",
    "\n",
    "In this step, the best estimates of the mean and variance are updated, given the current grouping. Given the probabilities of membership estimated above, we compute another iteration of the estimate $\\hat{\\mu}_k$ as:\n",
    "\n",
    "$$\\hat{\\mu}_k = \\frac{\\sum_{i = 1}^N P(x_i \\in k | x_i)x_i}{\\sum_{i = 1}^{N}P(x_i \\in k | x_i)}$$\n",
    "\n",
    "Note that this is a weighted mean, with the weights corresponding to the probability of an observation belonging to each class for each observation. The new estimate of $\\sigma_k$ is based on $\\hat{\\mu}_k$:\n",
    "\n",
    "$$\\hat{\\sigma}_k = \\sqrt{\\frac{\\sum_{i = 1}^N P(x_i \\in k | x_i)(x_i - \\hat{\\mu}_k)^2}{\\sum_{i = 1}^N P(x_i \\in k |x_i)}}$$\n",
    "\n",
    "Finally, \n",
    "\n",
    "$$\\hat{\\lambda_k} = \\frac{1}{N}\\sum_{i = 1}^N P(x_i \\in k | x_i)$$\n",
    "\n",
    "### Convergence Check\n",
    "\n",
    "There are a number of ways to check whether we have converged to the optimal grouping and parameter estimates. One such method is to compute the _log-likelihood_ at each iteration and determine if it has changed by a large enough amount to imply the algorithm has not converged. Once we have converged, the log-likelihood will have only changed by a small amount.\n",
    "\n",
    "The likelihood of our data given these new values of $\\hat{\\lambda}$, $\\hat{\\mu}$ and $\\hat{\\sigma}$ is:\n",
    "\n",
    "$$\\ell(\\mu, \\lambda, \\sigma | \\textbf{X}) = \\prod_{i = 1}^N \\sum_{k = 1}^K \\lambda_k \\phi(x_i|\\mu_k, \\sigma_k^2)$$\n",
    "\n",
    "The log-likelihood is the logarithm of this quantity. We must be careful how we choose to compute this- problems with convergence since the log of a negligible number is strongly negative.\n",
    "\n",
    "Convergence is reached when:\n",
    "\n",
    "$$ln(\\ell^{i + 1}) - ln(\\ell^i) < \\epsilon$$\n",
    "\n",
    "where $ln(\\ell^i)$ is the log-likelihood from the $i^{th}$ iteration from the EM-Algorithm and $\\epsilon$ is a desired tolerance. \n",
    "\n",
    "# Example\n",
    "\n",
    "Consider the following mathematical example to clarify the EM-Algorithm. Suppose we have data on the width and length of 4 fish that belong to 3 age groups, where 1 represents \"young\" fish, 2 \"adult\" fish and 3 \"old\" fish:\n",
    "\n",
    "| fishID | Width | Length | Age Group |\n",
    "|--------|-------|--------|-----------|\n",
    "| 1      |  5 | 20 | 1 |\n",
    "| 2 | 7 | 30 | 3 |\n",
    "| 3|4|10|1|\n",
    "|4|5|17|2|\n",
    "\n",
    "We assume that each age group has lengths and widths that belong to a Multivariate Gaussian Mixture Model. The length and width of several species of fish are know to increase with age. There will of course be some overlap in the distributions, for example for unusually large 'young' fish and unusually small 'adult' fish. The goal is to classify the age group of the 2 unassigned fish using the attributes of the 4 fish with known age group. Assume the data are multivariate normally distributed.\n",
    "\n",
    "## 1) Initialization\n",
    "\n",
    "Firstly the parameters for each 2-dimensional Gaussian distribution are initialized.\n",
    "\n",
    "# R Code\n",
    "\n",
    "\n",
    "## One-Dimensional Fisheries Application\n",
    "\n",
    "## Multi-Dimensional Insurance Application\n",
    "\n",
    "# Python Code\n",
    "\n",
    "## One-Dimensional Fisheries Application\n",
    "\n",
    "For the sake of a good programming exercise, the Expectation-Maximization Algorithm implemented in R was also coded in Python.\n",
    "\n",
    "## Multi-Dimensional Insurance Application\n",
    "\n",
    "# Use Cases\n",
    "\n",
    "## a) Fisheries\n",
    "\n",
    "A common problem in fisheries and other ecological studies is using a sub-sample of animals to then categorize other animals. In particular, this could be used to set environmental policy on the size of fish that can be caught, in order to protect fish stocks.\n",
    "\n",
    "In order to estimate how a fish population is developing over time, one can monitor the age structure of fish over time. In particular, fish can be classified as young, adult or old. One way to do this is to determine the age of each fish manually by growth layers on a small earbone called the otolith. However, this approach is time-consuming and costly.\n",
    "\n",
    "An alternative approach is to exploit the knowledge that the age of a fish and its length are correlated. Given this information, we can sample a smaller subset of fish, measure their length and then their age by counting the growth layers on the otolith and use an Expectation-Maximization Algorithm to identify the age group of the remaining fish based on their length. This approach is far more time-efficient and economical than manually counting growth layers for all fish in the population.\n",
    "\n",
    "To identify the age groups of the remaining fish, we assume each fish is drawn from one of 3 distributions which represents their age: young, adult and old. Combining these distributions results in a mixture model, in which there is some natural overlap between age groups. For example, young fish will have some longer fish than those in the adult group. Each of these mixtures has some expectation and standard deviation.\n",
    "\n",
    "The empirical distribution function indicates three clear 'peaks', each corresponding to the 3 Gaussian mixtures for fish ages 'young', 'adult' and 'old. Figure ? also shows a Normal Quantile-Quantile (QQ) plot of fish lengths. Whilst this plot clearly shows deviation from the assumption from normality, the three sub-figures identify three separate Gaussian Mixtures. Therefore, we make the reasonable assumption that fish lengths come from a tri-modal Gaussian Mixture. A Gaussian Mixture Model (GMM) \"assumes all data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters\".\n",
    "\n",
    "## b) Insurance Risk Pricing\n",
    "\n",
    "In this example, the EM-Algorithm is extended to multiple dimensions. In this case, we assume that each mixture is a multi-variate Gaussian Mixture. The appropriateness of this assumption is not considered here but is discussed in [], although the Gaussian is considered a good generic choice due to its spherical nature and the conclusions of the Central Limit Theorem.  \n",
    "\n",
    "For this example, rather than use real-world data, we simulate data for which we know the true category for all observations. We then sample 10% of the observations, which we consider to be the known categories and for the remaining 90% of observations, we add some random variation (might not be necessary) and assess how well the algorithm performs in recovering the categories for each group.\n",
    "\n",
    "Assume that \n",
    "\n",
    "### Binomial /Multinomial Distribution\n",
    "\n",
    " Assume that we observe a time series of individual actions. At each time point an individual can do one of $k \\in K$ things. We assume that at each point in time their choice is independent of their choice in all other time perios (i.e. we have independent trials hence there is no point looking at some sort of time series analysis). We would like to use the choices made by individuals to group them within a cluster. We know the clusters of the 1st 100 people, however for the remaining 900 people we do not know which cluster they will belong to. We do not have any other information about them other than the choices they make. \n",
    " \n",
    "The point is that this method can be extended to almost any distribution with a tractable likelihood function. \n",
    "\n",
    "First, one must check whether the distribution for each cluster is Multivariate Normal and we can do this using the Mahalanobis Distance.\n",
    "\n",
    "## c) Other Use Cases\n",
    "\n",
    "A variety of other use cases of the Expectation-Maximization exist in very different industries. Any problems that involve the clustering of observations when only some observations have known clusters can benefit from the use of this Algorithm. Such categorization for every single individual is either:\n",
    "\n",
    "a) Extremely time-consuming, or economically infeasible\n",
    "b) impossible, since data may only exist or be available for a subset of the population\n",
    "\n",
    "## Advantages and disadvantages of the Expectation-Maximization Algorithm\n",
    "\n",
    "- We need (at least some of) the $\\textbf{x}$ variables to be sufficiently correlated with the groups, $y$. The higher the level of correlation, the more accurate the algorithm will be in classifying observations into the different groups\n",
    "- If there isn't much correlation between groups, there will be little independent variation in the cluster centroids and there will be a large number of misclassified observations\n",
    "- One could also use PCA to cluster observations and then identify the loss ratios within each cluster. \n",
    "\n",
    "Insurance pricing is certainly an interesting case. Most variables are used to aggregate policyholders, and assume all policyholders in a group behave the same way. This might make sense when predicting claim severity (for example, we might expect those with more occupants or those with higher contents value to make larger claims, on average), however for claim frequency it can be dubious. In fact, the vast majority of information used in insurance pricing is based on correlation rather than causation, and can result in overcharging and undercharging many individuals within a given group. Behavioural data is thus particularly important in the prediction of claim frequency and allows for greater explanatory power. For example, a policyholder with a Google search history of how to to become a benefits cheat is far more likely to make a Contents Home Insurance claim.\n",
    "\n",
    "# R and Python Packages\n",
    "\n",
    "There also exist R and Python packages that carry out the discussed algorithm. The scikic-learn GaussianMixture.\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "In this report we have discussed and implemented the Expectation-Maximization Algorithm to solve problems where only a sample of the population has been categorized, and we would like to use the characteristics of those individuals (correlations with the cluster) to inform the clusters of the remaining observations. \n",
    "\n",
    "We believe this method is something that is seriously under-utilized. In particular it can result in large business value, by making use of data that is only available for a subset of the population. We have seen the case of insurance, where the use of such an algorithm can be hugely beneficial for risk pricing, in that it can help to identify behavioural characteristics of the population that a standard Generalized or Additive Model would fail to consider. It also enables one to engineer new feature which have a clear interpretation, a particularly important concept in insurance, where rate changes to such a variable by pricing analysts must have a logical structure.\n",
    "\n",
    "The algorithm is particularly useful in identifying unobserved variables and hidden processes.\n",
    "\n",
    "In real life, it is likely that a lot of the data might be uncorrelated with each factor level of categorization, and the data is also unlikely to be normally distributed.\n",
    "\n",
    "# References\n",
    "\n",
    "Chuong Do, Serafim Batzoglu (2008). 'What is the Expectation Maximization Algorithm?'. Nature Biotechnology, Vol 6, No. 8, pp 897-899.\n",
    "\n",
    "Aurelien Geron (2019). 'Hands-On Machine Learning with Sikit-Learn, Keras & Tensorflow'. O'Reilly.\n",
    "\n",
    "Berlingerio, M., Bonchi, F., Gartner, T., Hurley, N., Ifrom, G. (2018). ‘Machine Learning and Knowledge Discovery in Databases’, European Conference, Dublin, Ireland, September 10-14, Proceedings, Part 2. pp379-381.\n",
    "\n",
    "\n",
    "\n",
    "# Appendix\n",
    "\n",
    "## R Markdown\n",
    "\n",
    "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.\n",
    "\n",
    "When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n",
    "\n",
    "```{r cars}\n",
    "summary(cars)\n",
    "```\n",
    "\n",
    "## Including Plots\n",
    "\n",
    "You can also embed plots, for example:\n",
    "\n",
    "```{r pressure, echo=FALSE}\n",
    "plot(pressure)\n",
    "```\n",
    "\n",
    "Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eb766a-e93d-4909-a7a6-2e5e4e171d1e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
