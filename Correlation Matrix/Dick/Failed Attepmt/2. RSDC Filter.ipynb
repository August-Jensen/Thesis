{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bd41412-03b3-429c-9122-021e829d4bed",
   "metadata": {},
   "source": [
    "# The Hamilton Filter for the RSDC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9820aa8a-baba-4e22-b9f3-fdcaf70ee59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "# =======================================================\n",
    "# |                Initail and Basic                    |\n",
    "# =======================================================\n",
    "\n",
    "# Create a a numpy array and list of labels from dataframe \n",
    "def df_to_array(dataframe):\n",
    "    \"\"\"\n",
    "    Turns a Dataframe into list of column names, and np array of the content\n",
    "\n",
    "    Takes dataframe\n",
    "\n",
    "    Creates a list of labels from the columns\n",
    "\n",
    "    Creates a numpy array of the content\n",
    "\n",
    "    Returns numpy array, labels\n",
    "    \"\"\"\n",
    "    # Convert the DataFrame to a numpy array\n",
    "    data_array = df.to_numpy()\n",
    "    \n",
    "    # Extract the column names as a list\n",
    "    labels = df.columns.tolist()\n",
    "    \n",
    "    return data_array, labels\n",
    "\n",
    "\n",
    "# Find the number of parameters in the Correlation Matrix by the number of timeseries\n",
    "def number_of_corr_params(N):\n",
    "    \"\"\"\n",
    "    Takes the data.shape's N, \n",
    "\n",
    "    Returns N*(N-1)/2\n",
    "    \"\"\"\n",
    "    number_of_correlation_parameters = N * (N - 1) / 2\n",
    "    \n",
    "    return number_of_correlation_parameters\n",
    "\n",
    "\n",
    "# Create RSDC_guess, by 2 states plus number of correlation parameters\n",
    "def initial_RSDC_Guess(number_of_correlation_parameters):\n",
    "    \"\"\"\n",
    "    Generates initial guesses for parameters in an optimization process, including\n",
    "    transition parameters and correlation parameters.\n",
    "\n",
    "    Parameters:\n",
    "    - number_of_correlation_parameters (int): The number    \n",
    "\n",
    "    Returns:\n",
    "    - list: A list of initial guesses for the parameters, starting with two transition\n",
    "            parameters followed by 2 times the number of correlation parameters, each\n",
    "            with an initial guess of 0.3.\n",
    "\n",
    "    Example:\n",
    "    >>> initial_guesses = initial_RSDC_guess(3)\n",
    "    >>> print(initial_guesses)\n",
    "    [0.95, 0.95, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3]\n",
    "    \"\"\"\n",
    "    # Initial guesses for the two transition parameters\n",
    "    initial_guesses = [0.95, 0.95]\n",
    "    \n",
    "    # Ensure number_of_correlation_parameters is an integer\n",
    "    number_of_correlation_parameters = int(number_of_correlation_parameters)\n",
    "    \n",
    "    # # Append initial guesses for the correlation parameters\n",
    "    # initial_guesses += [0.3] * (2 * number_of_correlation_parameters)\n",
    "    # return initial_guesses\n",
    "    \n",
    "    # Append random initial guesses for the correlation parameters\n",
    "    # Assuming the random values should be between -1 and 1 for the correlation parameters\n",
    "    random_guesses = np.random.uniform(-0.5, 0.5, 2 * number_of_correlation_parameters).tolist()\n",
    "    initial_guesses += random_guesses\n",
    "    return initial_guesses\n",
    "\n",
    "# Create Bounds for the RSDC parameters\n",
    "def set_RSDC_bounds(number_of_correlation_parameters):\n",
    "    \"\"\"\n",
    "    Creates bounds for the RSDC parameters that are to be minimized.\n",
    "\n",
    "    The first two are for the Transition Probabilities, which are (0.001, 0.999)\n",
    "\n",
    "    The rest are for the Correlation Parameters, which are (-1, 1)\n",
    "\n",
    "    Parameters:\n",
    "    - number_of_correlation_parameters (int): The number of correlation parameters.\n",
    "\n",
    "    Returns:\n",
    "    - list of tuples: Each tuple represents the lower and upper bound for a parameter,\n",
    "                      with the first two parameters (transition probabilities) bounded\n",
    "                      by (0.001, 0.999) and the correlation parameters bounded by (-1, 1).\n",
    "    \"\"\"\n",
    "    # Ensure number_of_correlation_parameters is treated as an integer\n",
    "    number_of_correlation_parameters = int(number_of_correlation_parameters)\n",
    "    \n",
    "    # Bounds for the two transition parameters\n",
    "    bounds = [(0.001, 0.999), (0.001, 0.999)]\n",
    "    \n",
    "    # Append bounds for the correlation parameters\n",
    "    bounds += [(-0.99, 0.99)] * (2 * number_of_correlation_parameters)\n",
    "    \n",
    "    return bounds\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e54f2b53-1513-4ce5-9dab-7a90010d14c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.86548355,  1.52829592],\n",
       "       [ 0.22650105, -0.56946868],\n",
       "       [-0.25169526, -0.49279609],\n",
       "       ...,\n",
       "       [ 0.28483143,  1.75645602],\n",
       "       [ 0.        , -0.72894961],\n",
       "       [-0.18652383, -0.90607843]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('2.csv') * 100\n",
    "data, labels = df_to_array(df)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3a0845ca-981e-4fa9-b2e8-ad062aa1764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# |             Univariate Model Estimates              |\n",
    "# =======================================================\n",
    "\n",
    "# Find the log-likelihood contributions of the univariate volatility\n",
    "def univariate_log_likelihood_contribution(x, sigma):\n",
    "    \"\"\"\n",
    "    Calculates the Log Likelihood contribution of a univariate GARCH(1,1) in absolute terms\n",
    "\n",
    "    It first sets sigma to be greater that zero.\n",
    "    Then calculates the Log-Likelihood contribution as:\n",
    "        -0.5 * np.log(2 * np.pi) - np.log(sigma) - (x ** 2) / (2 * sigma ** 2)\n",
    "\n",
    "    Parameters:\n",
    "    - x (real): The observation of the timeseries, at time t.\n",
    "    - sigma (real): The value of sigma, at time t.\n",
    "\n",
    "    Returns:\n",
    "    - the log_likelihood_contribution\n",
    "    \"\"\"\n",
    "    sigma = max(sigma, 1e-8)\n",
    "    return -0.5 * np.log(2 * np.pi) - np.log(sigma) - (x ** 2) / (2 * sigma ** 2)\n",
    "\n",
    "\n",
    "# Calculate the total log-likelihood of the univariate volatility\n",
    "def total_univariate_log_likelihood(GARCH_guess, x):\n",
    "    \"\"\"\n",
    "    Creates the values of sigma, and then calculates the total_log_likelihood by summing over \n",
    "    univariate_log_likelihood_contributions\n",
    "    \n",
    "    Parameters:\n",
    "    - GARCH_guess: omega, alpha and beta from minimize function\n",
    "\n",
    "    Returns:\n",
    "    - Negative total_log_likelihood\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set Number of Observations\n",
    "    T = len(x)\n",
    "\n",
    "    # Set Parameters\n",
    "    omega, alpha, beta = GARCH_guess\n",
    "    sigma = np.zeros(T)\n",
    "\n",
    "    # Set the Initial Sigma to be Total Unconditional Variance of data\n",
    "    sigma[0] = np.sqrt(np.var(x))\n",
    "\n",
    "    # Calculate sigma[t] for the described model\n",
    "    for t in range(1, T):\n",
    "        sigma[t] = omega + alpha * np.abs(x[t-1]) + beta * np.abs(sigma[t-1])\n",
    "\n",
    "    # Calculate the sum of the Log-Likelihood contributions\n",
    "    univariate_log_likelihood = sum(univariate_log_likelihood_contribution(x[t], sigma[t]) for t in range(T))\n",
    "\n",
    "    # Return the Negative Log-Likelihood\n",
    "    return -univariate_log_likelihood\n",
    "\n",
    "\n",
    "\n",
    "# Minimize - total log-likelihood of the univariate volatility\n",
    "def estimate_univariate_models(x):\n",
    "    \"\"\"\n",
    "    Minimizes total_univariate_log_likelihood\n",
    "\n",
    "    Parameters:\n",
    "    - data: one dimensional array of time series data.\n",
    "\n",
    "    Returns:\n",
    "    - result parameters, and information about accuracy\n",
    "    \n",
    "    \"\"\"\n",
    "    # Initial Guess for omega, alpha, beta\n",
    "    GARCH_guess = [0.002, 0.2, 0.7]\n",
    "\n",
    "    # Minimize the Negative Log-Likelihood Function\n",
    "    result = minimize(fun=total_univariate_log_likelihood, x0=GARCH_guess, args=(x,), bounds=[(0, None), (0, 1), (0, 1)])\n",
    "    print(f\"Estimated parameters: omega = {result.x[0]}, alpha = {result.x[1]}, beta = {result.x[2]}\")\n",
    "\n",
    "    # Set Parameters\n",
    "    result_parameters = result.x\n",
    "\n",
    "    # Set Variance-Covariance Hessian\n",
    "    result_hessian = result.hess_inv.todense()  \n",
    "\n",
    "    # Set Standard Errors\n",
    "    result_se = np.sqrt(np.diagonal(result_hessian))\n",
    "\n",
    "\n",
    "    # Return Parameters and Information\n",
    "    return result_parameters, result_hessian, result_se\n",
    "\n",
    "# Get an array of univariate model parameters for all timeseries\n",
    "def estimate_univariate_parameters(data):\n",
    "    \"\"\"\n",
    "    Calculates the Univariate estimate for each timeseries in data\n",
    "    then appends the estimated parameters to estimated_univariate_parameters, \n",
    "    and appends the hessian, standard error etc to another list.\n",
    "    Then it creates a numpy array of these which are returned. \n",
    "\n",
    "    Parameters:\n",
    "    - \n",
    "\n",
    "    Returns:\n",
    "    - \n",
    "    \n",
    "    \"\"\"\n",
    "    # Create list to store univariate parameters, hessians, and standard errors\n",
    "    univariate_parameters = []\n",
    "    univariate_hessians = []\n",
    "    univariate_standard_errors = []\n",
    "\n",
    "    # Iterate over each time series in 'data' and estimate parameters\n",
    "    for i in range(data.shape[1]):  # data.shape[1] gives the number of time series (columns) in 'data'\n",
    "        result_parameters, result_hessian, result_se = estimate_univariate_models(data[i,:])\n",
    "        univariate_parameters.append(result_parameters)\n",
    "        univariate_hessians.append(result_hessian)\n",
    "        univariate_standard_errors.append(result_se)\n",
    "\n",
    "    # Convert the lists of results to numpy arrays\n",
    "    univariate_parameters_array = np.array(univariate_parameters)\n",
    "    univariate_hessians_array = np.array(univariate_hessians)\n",
    "    univariate_standard_errors_array = np.array(univariate_standard_errors)\n",
    "\n",
    "    # Return the results\n",
    "    return univariate_parameters_array, univariate_hessians_array, univariate_standard_errors_array\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d38ce661-e970-47b9-adf5-3aa146b958d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated parameters: omega = 0.07254604602262112, alpha = 0.15881365417783364, beta = 0.8199728059291468\n",
      "Estimated parameters: omega = 0.403204395247546, alpha = 0.35337750832222614, beta = 0.39348284367881425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.07254605, 0.15881365, 0.81997281],\n",
       "       [0.4032044 , 0.35337751, 0.39348284]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "univ_params, univ_hess, univ_std = estimate_univariate_parameters(data)\n",
    "univ_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1eed0a89-654e-4137-9c52-874c8eab99ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# |      Functions For Multivariate GARCH Setup         |\n",
    "# =======================================================\n",
    "\n",
    "# Forms the Correlation Matrix from RSDC_correlation_guess\n",
    "def form_correlation_matrix(RSDC_correlation_guess):\n",
    "    \"\"\"\n",
    "    Creates a square matrix with ones on the diagonal and symmetric off-diagonal elements\n",
    "    based on the input list of parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    - params: A list of numbers to fill into the off-diagonal elements. The length of this list\n",
    "              should be n(n-1)/2 for a square matrix of size n.\n",
    "    \n",
    "    Returns:\n",
    "    - A numpy array representing the square matrix with the specified properties.\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine the size of the matrix\n",
    "    n = int(np.sqrt(len(RSDC_correlation_guess) * 2)) + 1\n",
    "    if len(RSDC_correlation_guess) != n*(n-1)//2:\n",
    "        raise ValueError(\"Invalid number of parameters for any symmetric matrix.\")\n",
    "    \n",
    "    # Create an identity matrix of size n\n",
    "    matrix = np.eye(n)\n",
    "    \n",
    "    # Fill in the off-diagonal elements\n",
    "    param_index = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            matrix[i, j] = matrix[j, i] = RSDC_correlation_guess[param_index]\n",
    "            param_index += 1\n",
    "            \n",
    "    return matrix\n",
    "\n",
    "# Create Parameters for the RSDC Model\n",
    "def parameterize(RSDC_guess):\n",
    "    \"\"\"\n",
    "    Unfolds the RSDC_guess into Transition Probabilities, and the Parameters for the Correlation Matrix in state 0 and 1\n",
    "    First, it takes the parameters 0, 1 and sets p_00, p_11 as these.\n",
    "    Then is separates the rest into 2 lists. \n",
    "    It passes these to form_correlation_matrix, which retusn a correlation matrix of the values.\n",
    "    Finally, it creates an array of the correlation matrix for each state.\n",
    "\n",
    "    Parameters:\n",
    "    - RSDC_guess: the list of guesses for the parameters in the model.\n",
    "\n",
    "    Returns:\n",
    "    - p_00, p_11: The Transition Probabilities that forms the Transition Matrix\n",
    "    - correlation...\n",
    "    \n",
    "    \"\"\"\n",
    "    # Extract Transition Probabilities\n",
    "    p_00, p_11 = RSDC_guess[0], RSDC_guess[1]\n",
    "\n",
    "    # Find where to split the parameters, for the remaining parameters.\n",
    "    split_index = len(RSDC_guess[2:]) // 2\n",
    "\n",
    "    # Create the arrays of Parameters for Correlation Matrix 0, 1\n",
    "    correlation_parameters_0 = RSDC_guess[2: 2 + split_index]\n",
    "    correlation_parameters_1 = RSDC_guess[2 + split_index:]\n",
    "\n",
    "    # Form the correlation matrix for each state\n",
    "    correlation_matrix_0 = form_correlation_matrix(correlation_parameters_0)\n",
    "    correlation_matrix_1 = form_correlation_matrix(correlation_parameters_0)\n",
    "\n",
    "    # Collect into a single array\n",
    "    correlation_matrix = [correlation_matrix_0, correlation_matrix_1]\n",
    "\n",
    "    return p_00, p_11, correlation_matrix\n",
    "\n",
    "\n",
    "\n",
    "# Create the Trasition Matrix from p_00 & p_11\n",
    "def create_transition_matrix(p_00, p_11):\n",
    "    \"\"\"\n",
    "    Create the Transition Matrix from p_00 & p_11 with the shape\n",
    "    p_00, 1-p_11\n",
    "    1-p_00, p_11\n",
    "\n",
    "    Returns:\n",
    "    - transition matrix\n",
    "    \n",
    "    \"\"\"\n",
    "    transition_matrix = np.zeros([2,2])\n",
    "    transition_matrix[0] = p_00, 1 - p_11\n",
    "    transition_matrix[1] = 1 - p_00, p_11\n",
    "\n",
    "    # Return the Transition Matrix\n",
    "    return transition_matrix\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Calculate Initial State Probabilities by Transition Matrix\n",
    "def calculate_initial_probabilities(transition_matrix):\n",
    "    \"\"\"\n",
    "    Determine the best guess of the Initial State Probabilities, from Transition Matrix\n",
    "\n",
    "    Returns:\n",
    "    - An array of initial probabilities at time t=0\n",
    "    \n",
    "    \"\"\"\n",
    "    # Needs Comments and expansion\n",
    "    A_matrix = np.vstack(((np.identity(2)- transition_matrix), np.ones([1,2])))\n",
    "    pi_first = np.linalg.inv(A_matrix.T.dot(A_matrix)).dot(A_matrix.T)\n",
    "    pi_second = np.vstack((np.zeros([2,1]), np.ones([1,1])))\n",
    "    initial_probs = pi_first.dot(pi_second)\n",
    "    initial_probabilities = initial_probs.T\n",
    "\n",
    "    return initial_probabilities\n",
    "\n",
    "\n",
    "# Calculate the Standard Deviations, sigma, from Univariate Estimates\n",
    "    # This could be done outside of the objective function? \n",
    "def calculate_standard_deviations(data, univariate_estimates):\n",
    "    \"\"\"\n",
    "    Calculates the standard deviations, Sigma[t] based on the estimated parameters and the data.\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "    - data: The array of data we estimate on.\n",
    "    - unviariate_estimates: The estimates from the univariate GARCH\n",
    "\n",
    "    Returns:\n",
    "    - An array of standard deviations for each timeseries.    \n",
    "    \"\"\"\n",
    "    # Get Data Dimensions\n",
    "    T, N = data.shape\n",
    "\n",
    "    # Create Array for Standard Deviations\n",
    "    standard_deviations = np.zeros((T,N))\n",
    "\n",
    "    # Calculate Sigmas for each timeseries\n",
    "    for i in range(N):\n",
    "        # Unpack Univariate Estimates\n",
    "        omega, alpha, beta = univariate_estimates[i]\n",
    "\n",
    "        # Create array for Sigma values\n",
    "        sigma = np.zeros(T)\n",
    "\n",
    "        # Set first observation of Sigma to Sample Variance\n",
    "        sigma[0] = np.sqrt(np.var(data[:, i]))\n",
    "\n",
    "        # Calculate Sigma[t]\n",
    "        for t in range(1, T):\n",
    "            sigma[t] = omega + alpha * np.abs(data[t-1, i]) + beta * np.abs(sigma[t-1])\n",
    "\n",
    "        # Save Sigmas to Standard Deviation Array\n",
    "        standard_deviations[:, i] = sigma\n",
    "\n",
    "    # Return array of all Standard Deviations\n",
    "    return standard_deviations\n",
    "\n",
    "\n",
    "# Calculate the Standardized Residuals from Univariate Estimates\n",
    "    # This could be done outside of the objective function? \n",
    "def calculate_standardized_residuals(data, univariate_estimates):\n",
    "    \"\"\"\n",
    "    Calculates the standard deviations, Sigma[t] based on the estimated parameters and the data.\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "    - data: The array of data we estimate on.\n",
    "    - unviariate_estimates: The estimates from the univariate GARCH\n",
    "\n",
    "    Returns:\n",
    "    - An array of standard deviations for each timeseries.    \n",
    "    \"\"\"\n",
    "    # Get Data Dimensions\n",
    "    T, N = data.shape\n",
    "\n",
    "    # Create Array for Standardized Residuals\n",
    "    standardized_residuals = np.zeros((T,N))\n",
    "\n",
    "    # Calculate Sigmas for each timeseries\n",
    "    for i in range(N):\n",
    "        # Unpack Univariate Estimates\n",
    "        omega, alpha, beta = univariate_estimates[i]\n",
    "\n",
    "        # Create array for Sigma values\n",
    "        sigma = np.zeros(T)\n",
    "\n",
    "        # Set first observation of Sigma to Sample Variance\n",
    "        sigma[0] = np.sqrt(np.var(data[:, i]))\n",
    "\n",
    "        # Calculate Sigma[t]\n",
    "        for t in range(1, T):\n",
    "            sigma[t] = omega + alpha * np.abs(data[t-1, i]) + beta * np.abs(sigma[t-1])\n",
    "\n",
    "        # Save Sigmas to Standard Deviation Array\n",
    "        standardized_residuals[:, i] = data[:, i] / sigma\n",
    "\n",
    "    # Return array of all Standard Deviations\n",
    "    return standardized_residuals\n",
    "\n",
    "# Creates a Diagonal Matrix of (N x N), with Standard Deviations on Diagonal, and zeros off the Diagonal\n",
    "\n",
    "def create_diagonal_matrix(t, std_array):\n",
    "    \"\"\"\n",
    "    Creates an N x N diagonal matrix with standard deviations at time t on the diagonal,\n",
    "    and zeros elsewhere. Here, N is the number of time series.\n",
    "\n",
    "    :param t: Integer, the time index for which the diagonal matrix is created.\n",
    "    :param standard_deviations: List of numpy arrays, each array contains the standard deviations over time for a variable.\n",
    "    :return: Numpy array, an N x N diagonal matrix with the standard deviations at time t on the diagonal.\n",
    "    \"\"\"\n",
    "    # Extract the standard deviations at time t for each series\n",
    "    stds_at_t = np.array(std_array[t, :])\n",
    "    \n",
    "    # Create a diagonal matrix with these values\n",
    "    diagonal_matrix = np.diag(stds_at_t)\n",
    "    \n",
    "    return diagonal_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check if a Correlation Matrix is PSD, Elements in [-1,1], and symmetric.\n",
    "def check_correlation_matrix_is_valid(correlation_matrix):\n",
    "    \"\"\"\n",
    "    Ensure that the Correlation Matrix satisfies the following:\n",
    "    1. Diagonal Elements Are 1.\n",
    "    2. Off-Diagonal Elements are between -1 & 1. \n",
    "    3. Check if the Correlation Matrix is Positive Semi-Definite by verifying that the Eigenvalues are non-negative\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "    - Correlation Matrix: The Estimated Matrix from the Hamilton Filter\n",
    "\n",
    "    Returns:\n",
    "    - Valid: True or False\n",
    "    - Message: What is not satisfied\n",
    "    \n",
    "    \"\"\"\n",
    "    # Check diagonal elements are all 1\n",
    "    if not np.all(np.diag(correlation_matrix) == 1):\n",
    "        return False, \"Not all diagonal elements are 1.\"\n",
    "    \n",
    "    # Check off-diagonal elements are between -1 and 1\n",
    "    if not np.all((correlation_matrix >= -1) & (correlation_matrix <= 1)):\n",
    "        return False, \"Not all off-diagonal elements are between -1 and 1.\"\n",
    "    \n",
    "    # Check if the matrix is positive semi-definite\n",
    "    # A matrix is positive semi-definite if all its eigenvalues are non-negative.\n",
    "    eigenvalues = np.linalg.eigvals(correlation_matrix)\n",
    "    if np.any(eigenvalues < -0.5):\n",
    "        print(eigenvalues)\n",
    "        return False, \"The matrix is not positive semi-definite.\"\n",
    "    \n",
    "    return True, \"The matrix meets all criteria.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9bf0cf63-bdd6-4f35-8e28-fa0b03ec2c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =======================================================\n",
    "# |          Hamilton Filter, RSDC Estimation           |\n",
    "# =======================================================\n",
    "\n",
    "# Calculates the Log-Likelihood Contribution of the RSDC Model, at time T\n",
    "def RSDC_log_likelihood_contribution(N, t, data, state, state_correlation_matrix, standard_deviations):\n",
    "    \"\"\"\n",
    "    Calculates the likelihood contribution, \n",
    "\n",
    "    NOT Log_Likelihood!\n",
    "    \n",
    "    Calculates log likelihood contribution,\n",
    "    Defines D as create_diagonal_matrix(t, standard_deviations)\n",
    "    Then finds the inverse of D\n",
    "    And the determinant of D\n",
    "    It can then calculate the log_likelihood_contribution \n",
    "    Then takes the exponential of it, to make filterering step easier.\n",
    "    \n",
    "\n",
    "    Parameters:\n",
    "    - t: Integer of time\n",
    "    - state: The state that it is calculated at.\n",
    "    - correlation_matrix, the array of correlation matrix in the states. Selected at state by correlation_matrix[state]\n",
    "\n",
    "    Returns:\n",
    "    - \n",
    "    \"\"\"\n",
    "    # Create D matrix, D_t at time t,\n",
    "    D = create_diagonal_matrix(t, standard_deviations)\n",
    "    # print(D)\n",
    "    # # Determinant of D\n",
    "    inv_D = np.linalg.inv(D)\n",
    "    det_D = np.linalg.det(D)\n",
    "\n",
    "    R = state_correlation_matrix[state]\n",
    "    det_R = np.linalg.det(R)\n",
    "    inv_R = np.linalg.inv(R)\n",
    "    # Calculate H_t, the conditional covariance matrix\n",
    "    # H_t = D @ R @ D\n",
    "\n",
    "    # # Compute the likelihood contribution for this time t\n",
    "    # inv_H_t = np.linalg.inv(H_t)  # Inverse of H_t\n",
    "    # det_H_t = np.linalg.det(H_t)  # Determinant of H_t\n",
    "    new_data = data.T\n",
    "    z_t = inv_D @ new_data[t]  # z_t at time t\n",
    "    \n",
    "    # Log likelihood contribution for time t\n",
    "    log_likelihood_contribution = -0.5 * (N * np.log(2 * np.pi) + 2 * np.log(det_D) + np.log(det_R) + z_t @ inv_R @ z_t.T)\n",
    "    #print(f' The Log Likelihood: {log_likelihood_contribution} \\n The Exponential: {np.exp(log_likelihood_contribution)}')\n",
    "    return log_likelihood_contribution\n",
    "\n",
    "\n",
    "# Calculates the Total Log Likelihood of the RSDC Model\n",
    "    # Including Predicted Probability and Filtered Probability\n",
    "def RSDC_total_log_likelihood(data, RSDC_guess,standard_deviations):\n",
    "    \"\"\"\n",
    "    This Function is Minimized in the fit function.\n",
    "    It should \n",
    "    \"\"\"\n",
    "    # Get Shape of Data\n",
    "    T, N = data.shape\n",
    "    data = data.T\n",
    "    n_states = 2\n",
    "    # Array for Predicted Probabilities\n",
    "    predicted_probabilities = np.zeros([n_states, T + 1])\n",
    "    \n",
    "    # Array for Filtered Probabilities\n",
    "    filtered_probabilities = np.zeros([n_states, T])\n",
    "\n",
    "    # Array for Log-Likelihood Contributions\n",
    "    log_likelihood_contributions = np.zeros(T)\n",
    "\n",
    "    # Form Model Parameters (With State Correlation Matrix)\n",
    "    p_00, p_11, state_correlation_matrix = parameterize(RSDC_guess)\n",
    "    \n",
    "    # Form Transition Matrix\n",
    "    transition_matrix = create_transition_matrix(p_00, p_11)\n",
    "\n",
    "    # Form Initial Probabilities, predicted Probabilities at time t=0\n",
    "    predicted_probabilities[[0,1],0] = calculate_initial_probabilities(transition_matrix)\n",
    "    #print(predicted_probabilities[[0,1],0])\n",
    "    \n",
    "    # To hold values of RSDC_likelihood_contributions\n",
    "    eta = np.zeros(n_states)\n",
    "\n",
    "    # The Partila RSDC Likelihood Contributions\n",
    "    partial_likelihood = np.zeros(n_states)\n",
    "\n",
    "    # The Hamilton Filter Loop\n",
    "    for t in range(T):\n",
    "        \n",
    "        # Calculate the state Densities, Eta and the Partial Likelihoods\n",
    "        for state in range(n_states):\n",
    "            # At this stage Eta is log.\n",
    "            eta[state] = RSDC_log_likelihood_contribution(N, t, data, state, state_correlation_matrix, standard_deviations, )\n",
    "        # Applying the log-sum-exp trick\n",
    "        \n",
    "        M = np.max(eta)\n",
    "        # At this stage Eta is log\n",
    "        log_sum_exp = M + np.log(np.sum(np.exp(eta - M)))\n",
    "        # print(log_sum_exp)\n",
    "        #Compute log(L0 / (L0 + L1))\n",
    "        \n",
    "        # At this stage Eta is log\n",
    "        log_fraction_L0 = eta[0] - log_sum_exp\n",
    "        log_fraction_L1 = eta[1] - log_sum_exp\n",
    "        # print(log_fraction_L0)\n",
    "        \n",
    "        # At this stage Eta is normal\n",
    "        eta[0] = np.exp(log_fraction_L0)\n",
    "        eta[1] = np.exp(log_fraction_L1)\n",
    "\n",
    "        # Now, to use this in normalization:\n",
    "        # normalized_log_likelihoods = eta - log_sum_exp\n",
    "        #print('Norm', normalized_log_likelihoods)\n",
    "        # For comparison, let's also calculate the direct way which should result in underflow\n",
    "        #direct_exp_normalization = np.exp(eta) / np.sum(np.exp(eta))\n",
    "        \n",
    "        #(log_sum_exp, normalized_log_likelihoods, direct_exp_normalization)\n",
    "        \n",
    "        for state in range(n_states):\n",
    "            partial_likelihood[state] = predicted_probabilities[state,t] * eta[state]\n",
    "        # print(f'Eta: {eta[state]}')\n",
    "        #print(eta,partial_likelihood)\n",
    "        # Calculate the log_likelihood_contribution\n",
    "        # ///xxx Error! Changed np.log(np.sum(partial_likelihood)) to np.sum(np.log(partial_likelihood))\n",
    "        log_likelihood_contributions[t] = np.log(np.sum(partial_likelihood))\n",
    "\n",
    "        #Filtering Step\n",
    "        num0 = eta[0] * predicted_probabilities[0,t]\n",
    "        num1 = eta[1] * predicted_probabilities[1,t]\n",
    "        denom = num0 + num1\n",
    "        filter0 = num0 / denom\n",
    "        filter1 = num1 / denom\n",
    "        \n",
    "        # print(f'Filter 1: {filter1}')\n",
    "        filtered_probabilities[[0,1],t] = filter0, filter1\n",
    "\n",
    "        # Prediction Step\n",
    "        predicted_probabilities[[0,1],t+1] = transition_matrix.dot(filtered_probabilities[[0,1],t])\n",
    "    # print(f'eta:  {eta}')\n",
    "    # print(f'partial_likelihood:  {partial_likelihood}')\n",
    "    # print(f'predicted_probabilities:  {predicted_probabilities}')\n",
    "    # print(f'filtered_probabilities:  {filtered_probabilities}')\n",
    "    #Find the Negative Total Log Likelihood\n",
    "    negative_likelihood = - np.sum(log_likelihood_contributions)\n",
    "    # print(negative_likelihood)\n",
    "    # Return Negative Likelihood\n",
    "    return negative_likelihood\n",
    "\n",
    "\n",
    "# Minimize the Models Objective Function\n",
    "    # Remember to Get params, hess_inf.to_dense(), as well as possibly fun, jac, and nit.\n",
    "def fit(dataframe):\n",
    "    \"\"\"\n",
    "    This function minimizes the negative Log-Likelihood\n",
    "    1. Create an array of the data.\n",
    "    2. Set T, N\n",
    "    3. Estimate GARCH\n",
    "    4. Set Number of Correlation Parameters\n",
    "    5. Set RSDC Guess\n",
    "    6. Set RSDC Bounds\n",
    "    7. Define Objective Function\n",
    "    8. Minimize\n",
    "    9. Get Results\n",
    "    10. Check PSD\n",
    "    \"\"\"\n",
    "    # Create Data & Labels\n",
    "    data, labels = df_to_array(df)\n",
    "    \n",
    "    # Get Shape\n",
    "    T, N = data.shape\n",
    "\n",
    "    \n",
    "    # Estimate GARCH\n",
    "    univariate_parameters_array, univariate_hessians_array, univariate_standard_errors_array = estimate_univariate_parameters(data)\n",
    "    \n",
    "    # Set Number of Correlation Parameters\n",
    "    number_of_correlation_parameters = number_of_corr_params(N)\n",
    "    \n",
    "    # Set Initial RSDC Guess\n",
    "    RSDC_guess = initial_RSDC_Guess(number_of_correlation_parameters)\n",
    "    true_correlation = np.corrcoef(data.T)\n",
    "    \n",
    "    # # Calculate Standard Deviations & Standardized Residuals\n",
    "    standard_deviations = calculate_standard_deviations(data, univariate_parameters_array)\n",
    "    # standardized_residuals = calculate_standardized_residuals(data, univariate_parameters_array)\n",
    "    \n",
    "    # #standard_deviations\n",
    "    # standardized_residuals\n",
    "    \n",
    "    # Set Initial RSDC Bounds\n",
    "    RSDC_Bounds = set_RSDC_bounds(number_of_correlation_parameters)\n",
    "    print('Guess', RSDC_guess)#, params)\n",
    "    # Inside Fit or Inside RSDC Likelihood?\n",
    "    def objective_function(RSDC_guess):\n",
    "        return RSDC_total_log_likelihood(data, RSDC_guess,standard_deviations)\n",
    "    \n",
    "    # objective_function(RSDC_guess)\n",
    "    result = minimize(objective_function, RSDC_guess, bounds=RSDC_Bounds, method='L-BFGS-B')\n",
    "    \n",
    "    if result.success:\n",
    "        # Print Sucessful Optimization\n",
    "        print(\"Optimization was successful.\")\n",
    "        # Store Results\n",
    "        RSDC_params = result.x\n",
    "        RSDC_hessian = result.hess_inv.todense()\n",
    "        RSDC_se = np.sqrt(np.diagonal(RSDC_hessian))\n",
    "        RSDC_fun = result.fun\n",
    "        RSDC_jac = result.jac\n",
    "        RSDC_nit = result.nit\n",
    "    \n",
    "        # Create Dictionary of Results\n",
    "        results = {\n",
    "            'labels': labels,\n",
    "            'univariate_parameters_array': univariate_parameters_array,\n",
    "            'univariate_hessians_array': univariate_hessians_array,\n",
    "            'univariate_standard_errors_array': univariate_standard_errors_array,\n",
    "            'RSDC_params': RSDC_params,\n",
    "            'RSDC_hessian': RSDC_hessian,\n",
    "            'RSDC_se': RSDC_se,\n",
    "            'fun': RSDC_fun,\n",
    "            'jac': RSDC_jac,\n",
    "            'nit': RSDC_nit\n",
    "        }\n",
    "        print(f'The Estimated Parameters: \\n {RSDC_params}')\n",
    "        # Find where to split the parameters, for the remaining parameters.\n",
    "        split_index = len(RSDC_guess[2:]) // 2\n",
    "    \n",
    "        # Create the arrays of Parameters for Correlation Matrix 0, 1\n",
    "        correlation_parameters_0 = RSDC_params[2: 2 + split_index]\n",
    "        correlation_parameters_1 = RSDC_params[2 + split_index:]\n",
    "    \n",
    "        # Form the correlation matrix for each state\n",
    "        correlation_matrix_0 = form_correlation_matrix(correlation_parameters_0)\n",
    "        correlation_matrix_1 = form_correlation_matrix(correlation_parameters_1)\n",
    "    \n",
    "        # Collect into a single array\n",
    "        estimated_correlation_matrix = [correlation_matrix_0, correlation_matrix_1]\n",
    "        print(f'The Estimated Correlation Matrix 0: \\n {correlation_matrix_0}')\n",
    "        print(f'The Estimated Correlation Matrix 1: \\n {correlation_matrix_1}')\n",
    "        valid, message = check_correlation_matrix_is_valid(correlation_matrix_0)\n",
    "        valid, message = check_correlation_matrix_is_valid(correlation_matrix_1)\n",
    "        print(message)\n",
    "        return result\n",
    "    else:\n",
    "        print(\"Optimization failed.\")\n",
    "        return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c8fe6ca0-612c-4fd9-9871-059a298ff3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated parameters: omega = 0.32927059200905295, alpha = 0.34865353705396807, beta = 0.7496720510537223\n",
      "Estimated parameters: omega = 0.2590719044912489, alpha = 0.2043798687736123, beta = 0.6636044372612875\n",
      "Guess [0.95, 0.95, -0.30222826983021633, -0.31459937296622653]\n",
      "Optimization was successful.\n",
      "The Estimated Parameters: \n",
      " [ 0.95        0.95       -0.30222827 -0.31459937]\n",
      "The Estimated Correlation Matrix 0: \n",
      " [[ 1.         -0.30222827]\n",
      " [-0.30222827  1.        ]]\n",
      "The Estimated Correlation Matrix 1: \n",
      " [[ 1.         -0.31459937]\n",
      " [-0.31459937  1.        ]]\n",
      "The matrix meets all criteria.\n"
     ]
    }
   ],
   "source": [
    "fitted = fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244c9a44-3eb8-4440-ad1c-9db865226d07",
   "metadata": {},
   "source": [
    "# Running Fit line by line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "dcc8fc4a-00ee-47a7-aa6f-d89a8a3658f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated parameters: omega = 0.07254604602262112, alpha = 0.15881365417783364, beta = 0.8199728059291468\n",
      "Estimated parameters: omega = 0.403204395247546, alpha = 0.35337750832222614, beta = 0.39348284367881425\n",
      "Guess [0.95, 0.95, -0.0571904599442945, -0.31523463922468953]\n",
      "-2839.6649158225623\n",
      "-2839.6649158225623\n",
      "-2839.6649158225623\n",
      "-2839.6649160181937\n",
      "-2839.6649158225623\n",
      "-50065.96891308693\n",
      "-50065.96891308694\n",
      "-50065.96891308693\n",
      "-50065.91950873712\n",
      "-50065.96891308693\n",
      "-50065.96891308693\n",
      "-50065.96891308693\n",
      "-50065.96891308693\n",
      "-50065.91950873712\n",
      "-50065.96891308693\n",
      "-50065.96891308694\n",
      "-50065.96891308694\n",
      "-50065.96891308694\n",
      "-50065.91950873712\n",
      "-50065.96891308694\n",
      "-50065.96891308693\n",
      "-50065.96891308693\n",
      "-50065.96891308693\n",
      "-50065.91950873712\n",
      "-50065.96891308693\n",
      "-50065.96891308694\n",
      "-50065.96891308693\n",
      "-50065.96891308693\n",
      "-50065.91950873712\n",
      "-50065.96891308694\n",
      "-50065.96891308694\n",
      "-50065.96891308693\n",
      "-50065.96891308694\n",
      "-50065.91950873712\n",
      "-50065.96891308694\n",
      "-50065.96891308693\n",
      "-50065.96891308693\n",
      "-50065.96891308693\n",
      "-50065.91950873712\n",
      "-50065.96891308693\n",
      "-50065.96891308694\n",
      "-50065.96891308694\n",
      "-50065.96891308693\n",
      "-50065.91950873712\n",
      "-50065.96891308694\n",
      "Optimization was successful.\n",
      "The Estimated Parameters: \n",
      " [ 0.95001118  0.95        0.99       -0.31523464]\n",
      "The Estimated Correlation Matrix 0: \n",
      " [[1.   0.99]\n",
      " [0.99 1.  ]]\n",
      "The Estimated Correlation Matrix 1: \n",
      " [[ 1.         -0.31523464]\n",
      " [-0.31523464  1.        ]]\n",
      "The matrix meets all criteria.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec261a45-1ff2-485b-a317-a6688ac9647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8f845403-d6c9-4729-a636-015fb7c9cab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated parameters: omega = 0.07254604602262112, alpha = 0.15881365417783364, beta = 0.8199728059291468\n",
      "Estimated parameters: omega = 0.403204395247546, alpha = 0.35337750832222614, beta = 0.39348284367881425\n",
      "Guess [0.95, 0.95, 0.9732140305524497, 0.9836216209034625]\n",
      "[[1.  0.9]\n",
      " [0.9 1. ]]\n",
      "[[1.  0.9]\n",
      " [0.9 1. ]]\n",
      "[[1.  0.9]\n",
      " [0.9 1. ]]\n",
      "[[1.         0.89999999]\n",
      " [0.89999999 1.        ]]\n",
      "[[1.  0.9]\n",
      " [0.9 1. ]]\n",
      "Optimization was successful.\n",
      "The Estimated Parameters: \n",
      " [0.95 0.95 0.9  0.9 ]\n",
      "The Estimated Correlation Matrix 0: \n",
      " [[1.  0.9]\n",
      " [0.9 1. ]]\n",
      "The Estimated Correlation Matrix 1: \n",
      " [[1.  0.9]\n",
      " [0.9 1. ]]\n",
      "The matrix meets all criteria.\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8cd86da4-5710-4705-995c-27b74ea794d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  message: ABNORMAL_TERMINATION_IN_LNSRCH\n",
      "  success: False\n",
      "   status: 2\n",
      "      fun: -690354382693.1489\n",
      "        x: [ 9.500e-01  9.500e-01 ...  9.937e-01  9.215e-01]\n",
      "      nit: 2\n",
      "      jac: [ 3.052e+03  3.052e+03 ...  0.000e+00  0.000e+00]\n",
      "     nfev: 1334\n",
      "     njev: 58\n",
      " hess_inv: <22x22 LbfgsInvHessProduct with dtype=float64>\n"
     ]
    }
   ],
   "source": [
    "print(fitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8d9a43-4998-46d3-b052-8be83c814ba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eb12bce-63c8-400d-9b7a-dd77917baa21",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 13 (34485023.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 14\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 13\n"
     ]
    }
   ],
   "source": [
    "    \"\"\"\n",
    "    What Should Be Returned\n",
    "        Labels, \n",
    "        Univariate Results, Univariate SE and Hessian\n",
    "\n",
    "    \"\"\"    # \"\"\"\n",
    "# =======================================================\n",
    "# |                 Estimation Results                  |\n",
    "# =======================================================\n",
    "\n",
    "# Perform the RSDC_total_log_likelihood function,\n",
    "    # With Smoothing Step, and using Estimated Parameters\n",
    "def smoothing_step():\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# Create a Table of the Total Log-Likelihood, number of observations,\n",
    "    # Parameters, Standardized Residuals,\n",
    "    # Test Statistics, etc.\n",
    "def present_results():\n",
    "    \"\"\"\n",
    "    What it does\n",
    "\n",
    "    Parameters:\n",
    "    - \n",
    "\n",
    "    Returns:\n",
    "    - \n",
    "    \n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def summarize_optimization_results(res, T, log_likelihood):\n",
    "    \"\"\"\n",
    "    Summarizes the optimization results from scipy.optimize.minimize using L-BFGS-B.\n",
    "\n",
    "    Parameters:\n",
    "    - res: The result object from scipy.optimize.minimize.\n",
    "    - n_obs: The number of observations in the dataset.\n",
    "    - log_likelihood: The log-likelihood value at the optimum.\n",
    "\n",
    "    Prints a summary table of the optimization results.\n",
    "    \"\"\"\n",
    "    # Extract parameter estimates and standard errors\n",
    "    params = res.x\n",
    "    se = np.sqrt(np.diagonal(res.hess_inv.todense()))\n",
    "    \n",
    "    # Calculate z-scores and p-values\n",
    "    z_scores = params / se\n",
    "    p_values = np.array([2 * (1 - stats.norm.cdf(np.abs(z))) for z in z_scores])\n",
    "    \n",
    "    # Print the summary table\n",
    "    print(f\"{'Parameter':>10} {'Estimate':>10} {'Std. Error':>12} {'z-Score':>10} {'p-Value':>10}\")\n",
    "    for i, (param, se, z, p) in enumerate(zip(params, se, z_scores, p_values)):\n",
    "        print(f\"{i:>10} {param:>10.4f} {se:>12.4f} {z:>10.4f} {p:>10.4f}\")\n",
    "    \n",
    "    # Print additional information\n",
    "    print(f\"\\nNumber of observations: {n_obs}\")\n",
    "    print(f\"Log-likelihood: {log_likelihood:.4f}\")\n",
    "\n",
    "    # Example usage:\n",
    "    # Assuming `res` is the result from scipy.optimize.minimize,\n",
    "    # `n_obs` is the number of observations, and `log_likelihood` is known:\n",
    "    # summarize_optimization_results(res, n_obs, log_likelihood)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a Plot of the Log-Returns, and the Filtered Volatilities\n",
    "    # For Each Timeseries?\n",
    "def plot_filtered_volatilities():\n",
    "    \"\"\"\n",
    "    What it does\n",
    "\n",
    "    Parameters:\n",
    "    - \n",
    "\n",
    "    Returns:\n",
    "    - \n",
    "    \n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# Create a Plot of Cumulated Log-Returns\n",
    "    # For All Timeseries collected\n",
    "def plot_cumulated_returns():\n",
    "    \"\"\"\n",
    "    What it does\n",
    "\n",
    "    Parameters:\n",
    "    - \n",
    "\n",
    "    Returns:\n",
    "    - \n",
    "    \n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# Create a Heatmap of the Conditional & Unconditional Correlations of Log-Returns\n",
    "def plot_correlation_heatmaps():\n",
    "    \"\"\"\n",
    "    What it does\n",
    "\n",
    "    Parameters:\n",
    "    - \n",
    "\n",
    "    Returns:\n",
    "    - \n",
    "    \n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# Create a Plot of the Standardized Residuals\n",
    "    # For Each Timeseries?\n",
    "def plot_standardized_residuals():\n",
    "    \"\"\"\n",
    "    What it does\n",
    "\n",
    "    Parameters:\n",
    "    - \n",
    "\n",
    "    Returns:\n",
    "    - \n",
    "    \n",
    "    \"\"\"\n",
    "    pass        \n",
    "\n",
    "# Create a Plot of the Predicted-, Filtered-, & Smoothed Probabilities\n",
    "def plot_probabilities():\n",
    "    \"\"\"\n",
    "    What it does\n",
    "\n",
    "    Parameters:\n",
    "    - \n",
    "\n",
    "    Returns:\n",
    "    - \n",
    "    \n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
