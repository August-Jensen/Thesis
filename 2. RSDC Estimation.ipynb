{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f7ccc7a-f3a0-415b-98aa-f111cb18fd71",
   "metadata": {},
   "source": [
    "# The CCC Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c37bf914-d715-4dac-a025-d2795b2e14b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def df_to_array(dataframe):\n",
    "    # Create Numpy Array\n",
    "    data_array = df.to_numpy().T\n",
    "    \n",
    "\n",
    "    # Get titles of columns for plotting\n",
    "    labels = df.columns.tolist()\n",
    "\n",
    "    return data_array, labels\n",
    "\n",
    "# Find the log-likelihood contributions of the univariate volatility\n",
    "def univariate_log_likelihood_contribution(x, sigma):\n",
    "    sigma = max(sigma, 1e-8)\n",
    "    return -0.5 * np.log(2 * np.pi) - np.log(sigma) - (x ** 2) / (2 * sigma ** 2)\n",
    "\n",
    "\n",
    "# Calculate the total log-likelihood of the univariate volatility\n",
    "def total_univariate_log_likelihood(GARCH_guess, x):\n",
    "    # Set Number of Observations\n",
    "    T = len(x)\n",
    "    \n",
    "    # Set Parameters\n",
    "    omega, alpha, beta = GARCH_guess\n",
    "    sigma = np.zeros(T)\n",
    "\n",
    "    # Set the Initial Sigma to be Total Unconditional Variance of data\n",
    "    sigma[0] = np.sqrt(np.var(x))\n",
    "\n",
    "    # Calculate sigma[t] for the described model\n",
    "    for t in range(1, T):\n",
    "        sigma[t] = omega + alpha * np.abs(x[t-1]) + beta * np.abs(sigma[t-1])\n",
    "\n",
    "    # Calculate the sum of the Log-Likelihood contributions\n",
    "    univariate_log_likelihood = sum(univariate_log_likelihood_contribution(x[t], sigma[t]) for t in range(T))\n",
    "\n",
    "    # Return the Negative Log-Likelihood\n",
    "    return -univariate_log_likelihood\n",
    "\n",
    "\n",
    "\n",
    "# Minimize - total log-likelihood of the univariate volatility\n",
    "def estimate_univariate_models(x):\n",
    "    # Initial Guess for omega, alpha, beta\n",
    "    GARCH_guess = [0.002, 0.2, 0.7]\n",
    "\n",
    "    # Minimize the Negative Log-Likelihood Function\n",
    "    result = minimize(fun=total_univariate_log_likelihood, x0=GARCH_guess, args=(x,), bounds=[(0, None), (0, 1), (0, 1)])\n",
    "    #print(f\"Estimated parameters: omega = {result.x[0]}, alpha = {result.x[1]}, beta = {result.x[2]}\")\n",
    "\n",
    "    # Set Parameters\n",
    "    result_parameters = result.x\n",
    "\n",
    "    # Set Variance-Covariance Hessian\n",
    "    result_hessian = result.hess_inv.todense()  \n",
    "\n",
    "    # Set Standard Errors\n",
    "    result_se = np.sqrt(np.diagonal(result_hessian))\n",
    "\n",
    "\n",
    "    # Return Parameters and Information\n",
    "    return result_parameters, result_hessian, result_se\n",
    "\n",
    "# Get an array of univariate model parameters for all timeseries\n",
    "def estimate_univariate_parameters(data, labels):\n",
    "    # Create list to store univariate parameters, hessians, and standard errors\n",
    "    univariate_parameters = []\n",
    "    # univariate_hessians = []\n",
    "    # univariate_standard_errors = []\n",
    "\n",
    "    # Iterate over each time series in 'data' and estimate parameters\n",
    "    for i in range(data.shape[0]):  # data.shape[1] gives the number of time series (columns) in 'data'\n",
    "        result_parameters, result_hessian, result_se = estimate_univariate_models(data[:, i])\n",
    "        univariate_parameters.append(result_parameters)\n",
    "        # univariate_hessians.append(result_hessian)\n",
    "        # univariate_standard_errors.append(result_se)\n",
    "        # Print the label and the estimated parameters for each time series\n",
    "        print(f\"Time Series: {labels[i]}, \\n    Estimated parameters: \\n \\t omega = {result_parameters[0]}, \\n \\t alpha = {result_parameters[1]}, \\n \\t beta = {result_parameters[2]}\")\n",
    "    # Convert the lists of results to numpy arrayst \n",
    "    univariate_parameters_array = np.array(univariate_parameters)\n",
    "    # univariate_hessians_array = np.array(univariate_hessians)\n",
    "    # univariate_standard_errors_array = np.array(univariate_standard_errors)\n",
    "\n",
    "    # Return the results\n",
    "    return univariate_parameters_array# univariate_hessians_array, univariate_standard_errors_array\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f5c4b9-375e-4088-a398-094d78c30d9a",
   "metadata": {},
   "source": [
    "# Prepare Data, and Univariate Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fabf39b-b044-471f-a90b-b3d942045fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.22846927 -0.91904793 ...  0.8286526   0.35905371\n",
      "  -0.80771242]\n",
      " [ 0.         -0.25638113  0.88051068 ...  0.99008008 -0.36108007\n",
      "  -0.04666247]]\n",
      "Time Series: Return 1, \n",
      "    Estimated parameters: \n",
      " \t omega = 0.0, \n",
      " \t alpha = 0.2, \n",
      " \t beta = 0.7\n",
      "Time Series: Return 2, \n",
      "    Estimated parameters: \n",
      " \t omega = 0.0376836574075746, \n",
      " \t alpha = 0.2081526229126051, \n",
      " \t beta = 0.7059538711318182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.2       , 0.7       ],\n",
       "       [0.03768366, 0.20815262, 0.70595387]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('22.csv')\n",
    "\n",
    "data, labels = df_to_array(df)\n",
    "print(data)\n",
    "N, T= data.shape\n",
    "# Estimate Univariate Parameters\n",
    "univ_params = estimate_univariate_parameters(data, labels)\n",
    "N,T\n",
    "univ_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee618f31-b558-4e5b-94b5-f1954a67c08a",
   "metadata": {},
   "source": [
    "# Setup of functions for estimation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74979170-5a0b-4890-ac9e-8d06c04b9882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forms the Correlation Matrix from RSDC_correlation_guess\n",
    "def form_correlation_matrix(multi_guess):\n",
    "    # Determine the size of the matrix\n",
    "    n = int(np.sqrt(len(multi_guess) * 2)) + 1\n",
    "    if len(multi_guess) != n*(n-1)//2:\n",
    "        raise ValueError(\"Invalid number of parameters for any symmetric matrix.\")\n",
    "    \n",
    "    # Create an identity matrix of size n\n",
    "    matrix = np.eye(n)\n",
    "    \n",
    "    # Fill in the off-diagonal elements\n",
    "    param_index = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            matrix[i, j] = matrix[j, i] = multi_guess[param_index]\n",
    "            param_index += 1\n",
    "            \n",
    "    return matrix\n",
    "\n",
    "\n",
    "# Calculate the Standard Deviations, sigma, from Univariate Estimates\n",
    "    # This could be done outside of the objective function? \n",
    "def calculate_standard_deviations(data, univariate_estimates):\n",
    "    # Get Data Dimensions\n",
    "    N,T = data.shape\n",
    "\n",
    "    # Create Array for Standard Deviations\n",
    "    standard_deviations = np.zeros((T,N))\n",
    "\n",
    "    # Calculate Sigmas for each timeseries\n",
    "    for i in range(N):\n",
    "        # Unpack Univariate Estimates\n",
    "        omega, alpha, beta = univariate_estimates[i]\n",
    "\n",
    "        # Create array for Sigma values\n",
    "        sigma = np.zeros(T)\n",
    "\n",
    "        # Set first observation of Sigma to Sample Variance\n",
    "        sigma[0] = np.sqrt(np.var(data[:, i]))\n",
    "\n",
    "        # Calculate Sigma[t]\n",
    "        for t in range(1, T):\n",
    "            sigma[t] = omega + alpha * np.abs(data[i,t-1]) + beta * np.abs(sigma[t-1])\n",
    "\n",
    "        # Save Sigmas to Standard Deviation Array\n",
    "        standard_deviations[:, i] = sigma\n",
    "\n",
    "    # Return array of all Standard Deviations\n",
    "    return standard_deviations\n",
    "\n",
    "\n",
    "# Creates a Diagonal Matrix of (N x N), with Standard Deviations on Diagonal, and zeros off the Diagonal\n",
    "def create_diagonal_matrix(t, std_array):\n",
    "    \"\"\"\n",
    "    Creates an N x N diagonal matrix with standard deviations at time t on the diagonal,\n",
    "    and zeros elsewhere. Here, N is the number of time series.\n",
    "\n",
    "    :param t: Integer, the time index for which the diagonal matrix is created.\n",
    "    :param standard_deviations: List of numpy arrays, each array contains the standard deviations over time for a variable.\n",
    "    :return: Numpy array, an N x N diagonal matrix with the standard deviations at time t on the diagonal.\n",
    "    \"\"\"\n",
    "    # Extract the standard deviations at time t for each series\n",
    "    stds_at_t = np.array(std_array[t,:])\n",
    "    \n",
    "    # Create a diagonal matrix with these values\n",
    "    diagonal_matrix = np.diag(stds_at_t)\n",
    "    \n",
    "    return diagonal_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check if a Correlation Matrix is PSD, Elements in [-1,1], and symmetric.\n",
    "def check_correlation_matrix_is_valid(correlation_matrix):\n",
    "    # Check diagonal elements are all 1\n",
    "    if not np.all(np.diag(correlation_matrix) == 1):\n",
    "        return False, \"Not all diagonal elements are 1.\"\n",
    "    \n",
    "    # Check off-diagonal elements are between -1 and 1\n",
    "    if not np.all((correlation_matrix >= -1) & (correlation_matrix <= 1)):\n",
    "        return False, \"Not all off-diagonal elements are between -1 and 1.\"\n",
    "    \n",
    "    # Check if the matrix is positive semi-definite\n",
    "    # A matrix is positive semi-definite if all its eigenvalues are non-negative.\n",
    "    eigenvalues = np.linalg.eigvals(correlation_matrix)\n",
    "    if np.any(eigenvalues < -0.5):\n",
    "        print(eigenvalues)\n",
    "        return False, \"The matrix is not positive semi-definite.\"\n",
    "    \n",
    "    return True, \"The matrix meets all criteria.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c9c833-3dc4-47dc-81f9-1024ccdb6971",
   "metadata": {},
   "source": [
    "# The Functions for the Hamilton Filter\n",
    "Here we need the following:\n",
    "1. Separate parameters for states and correlations\n",
    "2. Set bounds and initial_guess\n",
    "3. Create Transition Matrix\n",
    "4. Calculate Predicted_probability at t=0\n",
    "5. A way to store 2 correlation Matrices at once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fdb2bab-f45e-4667-88fd-f8101b5f19b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "   # # Calculate the correlation matrix from the data\n",
    "   #  corr_matrix = np.corrcoef(data, rowvar=False)\n",
    "    \n",
    "   #  # Extract the off-diagonal elements of the correlation matrix as parameters\n",
    "   #  off_diagonal_elements = corr_matrix[np.triu_indices_from(corr_matrix, k=1)]\n",
    "    \n",
    "   #  # Define the number of correlation parameters as the length of the off-diagonal elements\n",
    "   #  number_of_correlation_parameters = len(off_diagonal_elements)\n",
    "    \n",
    "   #  # Initialize the parameters list with the first two elements\n",
    "   #  parameters = [0.95, 0.95]\n",
    "    \n",
    "   #  # Use the correlations as uniform_randoms\n",
    "   #  uniform_randoms = off_diagonal_elements\n",
    "    \n",
    "   #  # Take half the value of these for random_between\n",
    "   #  random_between = uniform_randoms / 2\n",
    "    \n",
    "   #  # Extend the parameters list with uniform_randoms and random_between\n",
    "   #  parameters.extend(uniform_randoms)\n",
    "   #  parameters.extend(random_between)\n",
    "    \n",
    "   #  return parameters\n",
    "\n",
    "def initial_parameters(number_of_correlation_parameters):\n",
    "\n",
    "    # Ensure the input is treated as an integer\n",
    "    number_of_correlation_parameters = int(number_of_correlation_parameters)\n",
    "    \n",
    "    # First two elements are 0.95 each\n",
    "    parameters = [0.95, 0.95]\n",
    "    \n",
    "    # Next set of elements, randomly chosen in uniform [0, 0.9]\n",
    "    uniform_randoms = np.random.uniform(0, 0.4, number_of_correlation_parameters)\n",
    "    parameters.extend(uniform_randoms)\n",
    "    \n",
    "    # Last set of elements, randomly chosen between [-0.5, 0.9]\n",
    "    random_between = np.random.uniform(-0.1, 0.4, number_of_correlation_parameters)\n",
    "    parameters.extend(random_between)\n",
    "    \n",
    "    return parameters\n",
    "def set_bounds(number_of_correlation_parameters):\n",
    "    # Ensure the input is treated as an integer\n",
    "    number_of_correlation_parameters = int(number_of_correlation_parameters)\n",
    "    \n",
    "    # First two bounds are (0.01, 0.99)\n",
    "    bounds = [(0.01, 0.99), (0.01, 0.99)]\n",
    "    \n",
    "    # The rest are 2 * number_of_correlation_parameters of bounds (-0.99, 0.99)\n",
    "    bounds.extend([(-0.99, 0.99) for _ in range(2 * number_of_correlation_parameters)])\n",
    "    \n",
    "    return bounds\n",
    "\n",
    "def parameterize(RSDC_guess):\n",
    "    # Extract Transition Probabilities\n",
    "    p_00, p_11 = RSDC_guess[0], RSDC_guess[1]\n",
    "\n",
    "    # Find where to split the parameters, for the remaining parameters.\n",
    "    split_index = len(RSDC_guess[2:]) // 2\n",
    "\n",
    "    # Create the arrays of Parameters for Correlation Matrix 0, 1\n",
    "    correlation_parameters_0 = RSDC_guess[2: 2 + split_index]\n",
    "    correlation_parameters_1 = RSDC_guess[2 + split_index:]\n",
    "\n",
    "    # Form the correlation matrix for each state\n",
    "    correlation_matrix_0 = form_correlation_matrix(correlation_parameters_0)\n",
    "    correlation_matrix_1 = form_correlation_matrix(correlation_parameters_1)\n",
    "\n",
    "    # Collect into a single array\n",
    "    correlation_matrix = [correlation_matrix_0, correlation_matrix_1]\n",
    "    correlation_matrix = np.array(correlation_matrix)\n",
    "\n",
    "    return p_00, p_11, correlation_matrix\n",
    "\n",
    "\n",
    "def parameterize_results(param_estimate):\n",
    "    # Extract Transition Probabilities\n",
    "    p_00, p_11 = param_estimate[0], param_estimate[1]\n",
    "\n",
    "    # Find where to split the parameters, for the remaining parameters.\n",
    "    split_index = len(param_estimate[2:]) // 2\n",
    "\n",
    "    # Create the arrays of Parameters for Correlation Matrix 0, 1\n",
    "    correlation_parameters_0 = param_estimate[2: 2 + split_index]\n",
    "    correlation_parameters_1 = param_estimate[2 + split_index:]\n",
    "\n",
    "    # Form the correlation matrix for each state\n",
    "    correlation_matrix_0 = form_correlation_matrix(correlation_parameters_0)\n",
    "    correlation_matrix_1 = form_correlation_matrix(correlation_parameters_1)\n",
    "\n",
    "    # Collect into a single array\n",
    "    correlation_matrix = [correlation_matrix_0, correlation_matrix_1]\n",
    "    correlation_matrix = np.array(correlation_matrix)\n",
    "\n",
    "    return p_00, p_11, correlation_matrix\n",
    "\n",
    "\n",
    "def create_transition_matrix(p_00, p_11):\n",
    "    transition_matrix = np.zeros([2,2])\n",
    "    transition_matrix[0] = p_00, 1 - p_11\n",
    "    transition_matrix[1] = 1 - p_00, p_11\n",
    "\n",
    "    # Return the Transition Matrix\n",
    "    return transition_matrix\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Calculate Initial State Probabilities by Transition Matrix\n",
    "def calculate_initial_probabilities(transition_matrix):\n",
    "    \"\"\"\n",
    "    Determine the best guess of the Initial State Probabilities, from Transition Matrix\n",
    "\n",
    "    Returns:\n",
    "    - An array of initial probabilities at time t=0\n",
    "    \n",
    "    \"\"\"\n",
    "    # Needs Comments and expansion\n",
    "    A_matrix = np.vstack(((np.identity(2)- transition_matrix), np.ones([1,2])))\n",
    "    pi_first = np.linalg.inv(A_matrix.T.dot(A_matrix)).dot(A_matrix.T)\n",
    "    pi_second = np.vstack((np.zeros([2,1]), np.ones([1,1])))\n",
    "    initial_probs = pi_first.dot(pi_second)\n",
    "    initial_probabilities = initial_probs.T\n",
    "\n",
    "    return initial_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc191cb5-0c23-47c1-9f84-c6934a84c255",
   "metadata": {},
   "source": [
    "# The Likelihood Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4de8b4da-03e7-4aaa-9556-02509d1a2a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ccc_likelihood_contribution(t, data, R, standard_deviations):\n",
    "    # What we need in the terms:\n",
    "    data = data.T\n",
    "    D = create_diagonal_matrix(t, standard_deviations)\n",
    "    # R is defined in Total CCC Likelihood \n",
    "    \n",
    "    # Linear Algebra\n",
    "    det_D = np.linalg.det(D)\n",
    "    if det_D <1e-8:\n",
    "        det_D = 1e-6\n",
    "    inv_D = np.linalg.inv(D)\n",
    "    det_R = np.linalg.det(R)\n",
    "    if det_R < 1e-8:\n",
    "        punish = det_R\n",
    "        det_R = 1e-6\n",
    "    else:\n",
    "        punish = 0\n",
    "    # inv_R = np.linalg.inv(R)\n",
    "    inv_R = np.linalg.inv(R) if det_R >= 0 else np.eye(R.shape[0])  # Fallback for negative det_R\n",
    "    \n",
    "    lambda_penalty = 1000\n",
    "   \n",
    "    # Penalty for negative det_R\n",
    "    penalty = lambda_penalty * min(0, punish)  # This will be non-zero only if det_R is negative\n",
    "\n",
    "    # The Shock Term\n",
    "    z = inv_D @ data[t]\n",
    "\n",
    "    # The Terms of the Log Likelihood Contribution\n",
    "    term_1 = N * np.log(2 * np.pi)\n",
    "    term_2 = 2 * np.log(det_D) \n",
    "    term_3 = np.log(det_R)\n",
    "    term_4 = z.T @ inv_R @ z\n",
    "    \n",
    "    log_likelihood_contribution = -0.5 * (term_1 + term_2 + term_3 + term_4) + penalty\n",
    "    \n",
    "    return np.exp(log_likelihood_contribution)\n",
    "\n",
    "# def Hamilton_Filter(data,random_guesses, standard_deviations):\n",
    "#     # Get Shape of Data\n",
    "#     N, T = data.shape\n",
    "\n",
    "#     # Set n_states to 2, to make expanding to more states easier\n",
    "#     n_states = 2\n",
    "\n",
    "#     # Array for Predicted Probabilities\n",
    "#     predicted_probabilities = np.zeros([n_states, T + 1])\n",
    "    \n",
    "#     # Array for Filtered Probabilities\n",
    "#     filtered_probabilities = np.zeros([n_states, T])\n",
    "#     # print(filtered_probabilities.shape)\n",
    "#     # Array for Log-Likelihood Contributions\n",
    "#     log_likelihood_contributions = np.zeros(T)\n",
    "\n",
    "#     # Get Transition Probabilities & R matrix\n",
    "#     p_00, p_11, R = parameterize(random_guesses)\n",
    "\n",
    "#     #Create The Transition Matrix\n",
    "#     transition_matrix = create_transition_matrix(p_00, p_11)\n",
    "    \n",
    "#     # Set initial_probability by Transition Matrix\n",
    "#     predicted_probabilities[[0,1],0] = calculate_initial_probabilities(transition_matrix)\n",
    "#     # print(predicted_probabilities[[0,1],0])\n",
    "\n",
    "#     # Array for Log-Likelihoods Contributions\n",
    "#     log_likelihood_contributions = np.zeros(T)\n",
    "#     # To hold values of RSDC_likelihood_contributions\n",
    "#     eta = np.zeros(n_states)\n",
    "\n",
    "#     # The Partila RSDC Likelihood Contributions\n",
    "#     partial_likelihood = np.zeros(n_states)\n",
    "    \n",
    "#     # The Hamilton Filter Loop\n",
    "#     for t in range(T):\n",
    "#         # Calculate the state Densities, Eta and the Partial Likelihoods\n",
    "#         for state in range(n_states):\n",
    "#             # At this stage Eta is log.\n",
    "#             corr_mat = R[state]\n",
    "            \n",
    "#             eta[state] = ccc_likelihood_contribution(t, data, corr_mat, standard_deviations)\n",
    "#             # partial_likelihood[state] = predicted_probabilities[state,t] * eta[state]\n",
    "#         # Applying the log-sum-exp trick\n",
    "#         M = np.max(eta)\n",
    "#         # At this stage Eta is log\n",
    "#         log_sum_exp = M + np.log(np.sum(np.exp(eta - M)))\n",
    "#         # print(log_sum_exp)\n",
    "#         #Compute log(L0 / (L0 + L1))\n",
    "        \n",
    "#         # At this stage Eta is log\n",
    "#         log_fraction_L0 = eta[0] - log_sum_exp\n",
    "#         log_fraction_L1 = eta[1] - log_sum_exp\n",
    "        \n",
    "#         # print(log_fraction_L0)\n",
    "        \n",
    "#         # At this stage Eta is normal\n",
    "#         eta[0] = log_fraction_L0\n",
    "#         eta[1] = log_fraction_L1\n",
    "#         # Now, to use this in normalization:\n",
    "#         # normalized_log_likelihoods = eta - log_sum_exp\n",
    "#         #print('Norm', normalized_log_likelihoods)\n",
    "#         # For comparison, let's also calculate the direct way which should result in underflow\n",
    "#         #direct_exp_normalization = np.exp(eta) / np.sum(np.exp(eta))\n",
    "        \n",
    "#         #(log_sum_exp, normalized_log_likelihoods, direct_exp_normalization)\n",
    "        \n",
    "#         # for state in range(n_states):\n",
    "#         #     partial_likelihood[state] = predicted_probabilities[state,t] * eta[state]\n",
    "\n",
    "#         log_predicted_probabilities = np.log(predicted_probabilities[:, t])\n",
    "        \n",
    "#         # Assuming eta is already adjusted for each state's likelihood contribution\n",
    "#         weighted_log_likelihoods = eta + log_predicted_probabilities\n",
    "        \n",
    "#         # Step 2: Apply the log-sum-exp trick to the weighted log likelihoods\n",
    "#         M = np.max(weighted_log_likelihoods)\n",
    "#         log_sum_exp = M + np.log(np.sum(np.exp(weighted_log_likelihoods - M)))\n",
    "        \n",
    "#         # The result is the log likelihood contribution for time t\n",
    "#         log_likelihood_contributions[t] = log_sum_exp\n",
    "#         #log_likelihood_contributions[t] = np.log(np.sum(partial_likelihood))\n",
    "\n",
    "#         filtered_probabilities[[0,1],t] = np.exp(log_fraction_L0), np.exp(log_fraction_L1)\n",
    "#         # #Filtering Step\n",
    "#         # num0 = eta[0] * predicted_probabilities[0,t]\n",
    "#         # num1 = eta[1] * predicted_probabilities[1,t]\n",
    "#         # denom = num0 + num1\n",
    "#         # filter0 = num0 / denom\n",
    "#         # filter1 = num1 / denom\n",
    "#         # filtered_probabilities[[0,1],t] = filter0, filter1\n",
    "\n",
    "#         # Prediction Step\n",
    "#         predicted_probabilities[[0,1],t+1] = transition_matrix.dot(filtered_probabilities[[0,1],t])\n",
    "\n",
    "    \n",
    "#     # print(f'eta:  {eta}')\n",
    "#     # print(f'partial_likelihood:  {partial_likelihood}')\n",
    "#     # print(f'predicted_probabilities:  {predicted_probabilities}')\n",
    "#     # print(f'filtered_probabilities:  {filtered_probabilities}')\n",
    "    \n",
    "#     #Find the Negative Total Log Likelihood\n",
    "#     negative_likelihood = - np.sum(log_likelihood_contributions)\n",
    "#     # print(negative_likelihood)\n",
    "\n",
    "    \n",
    "#     # Return Negative Likelihood\n",
    "#     return negative_likelihood\n",
    "# # standard_deviations = np.zeros((N,T))\n",
    "# # standard_deviations = calculate_standard_deviations(data, univ_params)\n",
    "# # guess = [0.95, 0.95, 0.03026404976969012, 0.2599401568540691, 0.4424802671431618, 0.2956209785258734, 0.0789336159306628, 0.3740201925493622, 0.20507063250295035, 0.157726295631511, 0.05272054968913, 0.219647907330339, -0.38049635093756246, 0.11810087419442217, 0.49998028364185887, 0.05252799553877302, 0.34406057578824767, 0.1002056297127, 0.17737394702802894, -0.097594140473587, 0.2962655755884279, 0.2268062811449]\n",
    "# # Hamilton_Filter(data, guess,standard_deviations)\n",
    "\n",
    "\n",
    "def Hamilton_Filter(data, random_guesses, standard_deviations):\n",
    "    # Get Shape of Data\n",
    "    N, T = data.shape\n",
    "\n",
    "    # Set number of states to 2, for simplicity\n",
    "    n_states = 2\n",
    "\n",
    "    # Array for Predicted Probabilities\n",
    "    predicted_probabilities = np.zeros([n_states, T + 1])\n",
    "    \n",
    "    # Array for Filtered Probabilities\n",
    "    filtered_probabilities = np.zeros([n_states, T])\n",
    "\n",
    "    # Array for Log-Likelihood Contributions\n",
    "    likelihood_contributions = np.zeros(T)\n",
    "\n",
    "    # Get Transition Probabilities & R matrix from your parameterization function\n",
    "    p_00, p_11, R = parameterize(random_guesses)\n",
    "    transition_matrix = create_transition_matrix(p_00, p_11)\n",
    "    predicted_probabilities[:, 0] = calculate_initial_probabilities(transition_matrix)\n",
    "\n",
    "\n",
    "\n",
    "    # To Hold values of Forward Filter Recursions\n",
    "    eta = np.zeros(n_states)\n",
    "    \n",
    "    # To Hold values of Forward Filter Recursions\n",
    "    filters = np.zeros(n_states)\n",
    "    \n",
    "    # To Hold values of Partial Log-Likelihoods.\n",
    "    partial_likelihood = np.zeros(n_states)\n",
    "\n",
    "    # The Hamilton Filter Loop\n",
    "    # The Main For Loop:\n",
    "    for t in range(T):\n",
    "        # Calculate State Densities\n",
    "        for state in range(n_states):\n",
    "            corr_mat = R[state]\n",
    "            eta[state] = ccc_likelihood_contribution(t, data, corr_mat, standard_deviations)\n",
    "            partial_likelihood[state] = predicted_probabilities[state,t] * eta[state]\n",
    "                \n",
    "      \n",
    "        #filtering\n",
    "        filter_0 = eta[0]*predicted_probabilities[0,t]/(eta[0]*predicted_probabilities[0,t]+eta[1]*predicted_probabilities[1,t])\n",
    "        filter_1 = eta[1]*predicted_probabilities[1,t]/(eta[0]*predicted_probabilities[0,t]+eta[1]*predicted_probabilities[1,t])\n",
    "        filtered_probabilities[:,t] = filter_0, filter_1     \n",
    "        \n",
    "       \n",
    "        # Calculate the Log-Likelihood\n",
    "        likelihood_contributions[t] = np.log(np.sum(partial_likelihood[state]))\n",
    " \n",
    "        # Calculate the Prediction step\n",
    "        predicted_probabilities[[0,1],t+1] = transition_matrix.dot(filtered_probabilities[[0,1],t])\n",
    "        #predicted_probabilities[:, t+1] = prediction_step(transition_matrix, filtered_probabilities, t)\n",
    "    \n",
    "    negative_likelihood = -np.sum(likelihood_contributions)\n",
    "\n",
    "    return negative_likelihood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac09d98-c2d6-4882-bd91-f63acff96edf",
   "metadata": {},
   "source": [
    "# Minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26dcc5b6-8f32-42f3-a5f7-3d6f428e3dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(data):\n",
    "    number_of_correlation_parameters = N * (N - 1) / 2\n",
    "    \n",
    "    first_guess = initial_parameters(number_of_correlation_parameters)\n",
    "    m_bounds = set_bounds(number_of_correlation_parameters)\n",
    "    print(first_guess)\n",
    "    standard_deviations = np.zeros((N,T))\n",
    "\n",
    "\n",
    "    standard_deviations = calculate_standard_deviations(data, univ_params)\n",
    "    def objective_function(first_guess):\n",
    "        return Hamilton_Filter(data,first_guess, standard_deviations)\n",
    "    result = minimize(objective_function, first_guess, bounds=m_bounds, method='L-BFGS-B')\n",
    "    return result\n",
    "\n",
    "trie = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5231d219-8760-46af-ab1e-94564937abb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.95, 0.95, 0.33803243887322965, 0.3135926619453838]\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fitted \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m fitted\n\u001b[1;32m      3\u001b[0m p1, p2, result_matrix \u001b[38;5;241m=\u001b[39m parameterize(fitted\u001b[38;5;241m.\u001b[39mx)\n",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobjective_function\u001b[39m(first_guess):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Hamilton_Filter(data,first_guess, standard_deviations)\n\u001b[0;32m---> 13\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_guess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mm_bounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.venvs/jupyter/lib/python3.11/site-packages/scipy/optimize/_minimize.py:710\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    707\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    708\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 710\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    713\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    714\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/.venvs/jupyter/lib/python3.11/site-packages/scipy/optimize/_lbfgsb_py.py:307\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m         iprint \u001b[38;5;241m=\u001b[39m disp\n\u001b[0;32m--> 307\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_scalar_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m func_and_grad \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun_and_grad\n\u001b[1;32m    313\u001b[0m fortran_int \u001b[38;5;241m=\u001b[39m _lbfgsb\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mintvar\u001b[38;5;241m.\u001b[39mdtype\n",
      "File \u001b[0;32m~/.venvs/jupyter/lib/python3.11/site-packages/scipy/optimize/_optimize.py:383\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    379\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[0;32m--> 383\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43mScalarFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[0;32m~/.venvs/jupyter/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:158\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl \u001b[38;5;241m=\u001b[39m update_fun\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# Gradient evaluation\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(grad):\n",
      "File \u001b[0;32m~/.venvs/jupyter/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[0;32m--> 251\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.venvs/jupyter/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venvs/jupyter/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "Cell \u001b[0;32mIn[12], line 12\u001b[0m, in \u001b[0;36mfit.<locals>.objective_function\u001b[0;34m(first_guess)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobjective_function\u001b[39m(first_guess):\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mHamilton_Filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfirst_guess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstandard_deviations\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 191\u001b[0m, in \u001b[0;36mHamilton_Filter\u001b[0;34m(data, random_guesses, standard_deviations)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m state \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_states):\n\u001b[1;32m    190\u001b[0m     corr_mat \u001b[38;5;241m=\u001b[39m R[state]\n\u001b[0;32m--> 191\u001b[0m     eta[state] \u001b[38;5;241m=\u001b[39m \u001b[43mccc_likelihood_contribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorr_mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstandard_deviations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m     partial_likelihood[state] \u001b[38;5;241m=\u001b[39m predicted_probabilities[state,t] \u001b[38;5;241m*\u001b[39m eta[state]\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m#filtering\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m, in \u001b[0;36mccc_likelihood_contribution\u001b[0;34m(t, data, R, standard_deviations)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m det_D \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m1e-8\u001b[39m:\n\u001b[1;32m     10\u001b[0m     det_D \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-6\u001b[39m\n\u001b[0;32m---> 11\u001b[0m inv_D \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m det_R \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mdet(R)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m det_R \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-8\u001b[39m:\n",
      "File \u001b[0;32m~/.venvs/jupyter/lib/python3.11/site-packages/numpy/linalg/linalg.py:561\u001b[0m, in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    559\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    560\u001b[0m extobj \u001b[38;5;241m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[0;32m--> 561\u001b[0m ainv \u001b[38;5;241m=\u001b[39m \u001b[43m_umath_linalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(ainv\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[0;32m~/.venvs/jupyter/lib/python3.11/site-packages/numpy/linalg/linalg.py:112\u001b[0m, in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSingular matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "fitted = fit(data)\n",
    "fitted\n",
    "p1, p2, result_matrix = parameterize(fitted.x)\n",
    "def plot_heatmaps(result_matrix, labels):\n",
    "    # Calculate the correlation matrix for the DataFrame\n",
    "    dims, dimz = result_matrix[0].shape\n",
    "    print(dims)\n",
    "    # Set up the matplotlib figure with subplots\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "    # Plot the Unconditional Correlation heatmap\n",
    "    sns.heatmap(result_matrix[0], ax=ax[0], annot=True, cmap='coolwarm', xticklabels=labels, yticklabels=labels)\n",
    "    ax[0].set_title('Conditional Correlation in State 0')\n",
    "    \n",
    "    # Plot the Conditional Correlation heatmap\n",
    "    sns.heatmap(result_matrix[1], ax=ax[1], annot=True, cmap='coolwarm', xticklabels=labels, yticklabels=labels)\n",
    "    ax[1].set_title('Conditional Correlation in State 1')\n",
    "    \n",
    "    # Adjust layout for better appearance\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(f'{trie} Hamilton Heatmaps {dims}.png')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "trie+=1\n",
    "# Example usage (note: you need to have a DataFrame `df` and a `result_matrix` variable ready for this to work):\n",
    "# plot_side_by_side_heatmaps(df, result_matrix, labels)\n",
    "\n",
    "# This function assumes you have a DataFrame `df`, a result matrix `result_matrix`, and a list of labels `labels`.\n",
    "# Replace 'df', 'result_matrix', and 'labels' with your actual data variables when using this function.\n",
    "plot_heatmaps(result_matrix, labels)\n",
    "print(fitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c178e26-f1d7-44ba-9e88-827f524a2508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing_step(data, estimates, univ_params):\n",
    "    # Get Shape of Data\n",
    "    N, T = data.shape\n",
    "\n",
    "    # Get Number of Correlation Parameters\n",
    "    number_of_correlation_parameters = N * (N - 1) / 2\n",
    "\n",
    "    # Calculate Standard Deviations\n",
    "    standard_deviations = np.zeros((N,T))\n",
    "    standard_deviations = calculate_standard_deviations(data, univ_params)\n",
    "\n",
    "    # Set number of states to 2 for simplicity\n",
    "    n_states = 2\n",
    "        \n",
    "    # Array for Predicted Probabilities\n",
    "    predicted_probabilities = np.zeros([n_states, T + 1])\n",
    "    \n",
    "    # Array for Filtered Probabilities\n",
    "    filtered_probabilities = np.zeros([n_states, T])\n",
    "\n",
    "    # Array for Smoothed Probabilities\n",
    "    smoothed_probabilities = np.zeros([n_states, T])\n",
    "\n",
    "    # Array for Log-Likelihood Contributions\n",
    "    log_likelihood_contributions = np.zeros(T)\n",
    "    \n",
    "    filtered_volatility = np.zeros([N,T])\n",
    "    # Get Transition Probabilities & R matrix from your parameterization function\n",
    "    p_00, p_11, R = parameterize(estimates)\n",
    "    transition_matrix = create_transition_matrix(p_00, p_11)\n",
    "    print(p_00)\n",
    "    print(p_11)\n",
    "    print(R)\n",
    "    print(transition_matrix)\n",
    "    predicted_probabilities[:, 0] = calculate_initial_probabilities(transition_matrix)\n",
    "\n",
    "    # The Hamilton Filter Loop\n",
    "    for t in range(T):\n",
    "        eta = np.zeros(n_states)\n",
    "        \n",
    "        # Calculate the state densities (Eta) and partial likelihoods\n",
    "        for state in range(n_states):\n",
    "            corr_mat = R[state]\n",
    "            eta[state] = ccc_likelihood_contribution(t, data, corr_mat, standard_deviations)\n",
    "        \n",
    "        # Adjust eta by adding the log of predicted probabilities\n",
    "        log_predicted_probabilities = np.log(predicted_probabilities[:, t] + 1e-9)  # Adding a small value to avoid log(0)\n",
    "        weighted_log_likelihoods = eta + log_predicted_probabilities\n",
    "        \n",
    "        # Apply log-sum-exp for numerical stability\n",
    "        M = np.max(weighted_log_likelihoods)\n",
    "        log_sum_exp = M + np.log(np.sum(np.exp(weighted_log_likelihoods - M)))\n",
    "        log_likelihood_contributions[t] = log_sum_exp\n",
    "        \n",
    "        # Compute the filtered probabilities\n",
    "        exp_eta = np.exp(eta - log_sum_exp)  # Normalizing the exponentiated eta values\n",
    "        filtered_probabilities[:, t] = exp_eta / np.sum(exp_eta)  # Should already sum to 1, but this ensures numerical accuracy\n",
    "        \n",
    "        # Prediction step\n",
    "        predicted_probabilities[:, t+1] = transition_matrix.dot(filtered_probabilities[:, t])\n",
    "\n",
    "        # filtered_volatility[t] = filtered_probabilities[[0],t] * sigma[0] + (1 - filtered_probabilities[[0],t] * sigma[1])\n",
    "        \n",
    "        # Backwards Smoother\n",
    "        smoothed_probabilities[:,T-1]=filtered_probabilities[:,T-1]\n",
    "        for t in range(T-2, 0, -1):\n",
    "            # print(filtered_probabilities[:,t])\n",
    "            # print(smoothed_probabilities[:,t+1])\n",
    "            # print(predicted_probabilities[:,t+1])\n",
    "            smoothed_probabilities[:,t] = filtered_probabilities[:,t] * (transition_matrix.T.dot(smoothed_probabilities[:,t+1] / predicted_probabilities[:,t+1]))\n",
    "\n",
    "\n",
    "        # print(f' Likelihood Value :  {likelihood_contributions[t]}')\n",
    "        # print(f'  Predicted Probability:  {predicted_probabilities[:, t+1]}')\n",
    "        # print(f' Filtered Probability :  {filtered_probabilities[:,t] }')\n",
    "        # print(f'Eta  :  {eta}')\n",
    "        # print(f'Partial Likelihood  :  {partial_likelihood}')\n",
    "    # Find the Total Log Likelihood\n",
    "    likelihood = np.sum(log_likelihood_contributions)\n",
    "    \n",
    "    # Return the Sum of the Log-Likelihood\n",
    "    return predicted_probabilities, filtered_probabilities, smoothed_probabilities, likelihood\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39d2b31b-a761-4234-8758-ad01ddaeec71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fitted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predicted, filtered, smoothed, likelihood \u001b[38;5;241m=\u001b[39m smoothing_step(data, \u001b[43mfitted\u001b[49m\u001b[38;5;241m.\u001b[39mx, univ_params)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(likelihood)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(predicted)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fitted' is not defined"
     ]
    }
   ],
   "source": [
    "predicted, filtered, smoothed, likelihood = smoothing_step(data, fitted.x, univ_params)\n",
    "print(likelihood)\n",
    "print(predicted)\n",
    "print(filtered)\n",
    "print(smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007527e5-9c47-492f-a8c5-0c680dfb507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_my_probabilities(data, predicted_probabilities, filtered_probabilities, smoothed_probabilities):\n",
    "    N, num_obs = data.shape\n",
    "    \n",
    "    x = np.arange(num_obs)\n",
    "    fig, ax = plt.subplots(3, figsize=(16, 9))\n",
    "    fig.subplots_adjust(hspace=0.3)  # Adjust space between plots\n",
    "\n",
    "    # Using fill_between for all plots to remove lines\n",
    "    ax[0].fill_between(x, 0, 1 - predicted_probabilities[0, :-1], color='darkblue', alpha=0.7)\n",
    "    ax[0].fill_between(x, 1 - predicted_probabilities[0, :-1], 1, color='darkorange', alpha=0.0)\n",
    "    ax[1].fill_between(x, 0, 1 - filtered_probabilities[0, :], color='darkblue', alpha=0.7)\n",
    "    ax[1].fill_between(x, 1 - filtered_probabilities[0, :], 1, color='darkorange', alpha=0.0)\n",
    "    ax[2].fill_between(x, 0, 1 - smoothed_probabilities[0, :], color='darkblue', alpha=0.7)\n",
    "    ax[2].fill_between(x, 1 - smoothed_probabilities[0, :], 1, color='darkorange', alpha=0.0)\n",
    "\n",
    "    # Setting titles and limits\n",
    "    titles = ['Predicted state probability, $P(s_t=1|x_{t-1},x_{t-2},...,x_{1})$',\n",
    "              'Filtered state probability, $P(s_t=1|x_{t},x_{t-1},...,x_{1})$',\n",
    "              'Smoothed state probability, $P(s_t=1|x_{T},x_{T-1},...,x_{1})$']\n",
    "    for i, axi in enumerate(ax):\n",
    "        axi.set_xlim(0, num_obs)\n",
    "        axi.set_ylim(0, 1)\n",
    "        axi.title.set_text(titles[i])\n",
    "        axi.axhline(0, color='black', linestyle=\"--\")\n",
    "        axi.axhline(1, color='black', linestyle=\"--\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_my_probabilities(data, predicted, filtered, smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52326141-615c-4aa8-9c74-8a6dc18f3fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('C4.csv')\n",
    "\n",
    "# data, labels = df_to_array(df)\n",
    "# N, T = data.shape\n",
    "# univ_params = estimate_univariate_parameters(data, labels)\n",
    "# fitted = fit(data)\n",
    "# p1, p2, result_matrix = parameterize(fitted.x)\n",
    "# plot_heatmaps(result_matrix, labels)\n",
    "# print(fitted)\n",
    "# predicted, filtered, smoothed, likelihood = smoothing_step(data, fitted.x, univ_params)\n",
    "# print(likelihood)\n",
    "# print(predicted)\n",
    "# print(filtered)\n",
    "# print(smoothed)\n",
    "# plot_my_probabilities(data, predicted, filtered, smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927b510c-b1f7-432d-9457-4e8d016f2673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('C2A.csv')\n",
    "\n",
    "# data, labels = df_to_array(df)\n",
    "# N, T = data.shape\n",
    "# univ_params = estimate_univariate_parameters(data, labels)\n",
    "# fitted = fit(data)\n",
    "# p1, p2, result_matrix = parameterize(fitted.x)\n",
    "# plot_heatmaps(result_matrix, labels)\n",
    "# print(fitted)\n",
    "# predicted, filtered, smoothed, likelihood = smoothing_step(data, fitted.x, univ_params)\n",
    "# print(likelihood)\n",
    "# print(predicted)\n",
    "# print(filtered)\n",
    "# print(smoothed)\n",
    "# plot_my_probabilities(data, predicted, filtered, smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff1c1c8-a6ea-4b8e-ae08-68f81e8c849c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('C2B.csv')\n",
    "\n",
    "# data, labels = df_to_array(df)\n",
    "# N, T = data.shape\n",
    "# univ_params = estimate_univariate_parameters(data, labels)\n",
    "# fitted = fit(data)\n",
    "# p1, p2, result_matrix = parameterize(fitted.x)\n",
    "# plot_heatmaps(result_matrix, labels)\n",
    "# print(fitted)\n",
    "# predicted, filtered, smoothed, likelihood = smoothing_step(data, fitted.x, univ_params)\n",
    "# print(likelihood)\n",
    "# print(predicted)\n",
    "# print(filtered)\n",
    "# print(smoothed)\n",
    "# plot_my_probabilities(data, predicted, filtered, smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e7e955-4a91-48f7-937b-d89be69ef779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('C4.csv')\n",
    "\n",
    "# data, labels = df_to_array(df)\n",
    "# N, T = data.shape\n",
    "# univ_params = estimate_univariate_parameters(data, labels)\n",
    "# fitted = fit(data)\n",
    "# p1, p2, result_matrix = parameterize(fitted.x)\n",
    "# plot_heatmaps(result_matrix, labels)\n",
    "# print(fitted)\n",
    "# predicted, filtered, smoothed, likelihood = smoothing_step(data, fitted.x, univ_params)\n",
    "# print(likelihood)\n",
    "# print(predicted)\n",
    "# print(filtered)\n",
    "# print(smoothed)\n",
    "# plot_my_probabilities(data, predicted, filtered, smoothed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
