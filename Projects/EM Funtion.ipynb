{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a11a020f-869c-4dc8-b187-c7aeb7fdef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tools import RSDC as RSDCsim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "\n",
    "def create_transition_matrix(n_states, diagonal):\n",
    "    \"\"\"Create an N-dimensional transition matrix with a specified diagonal value.\"\"\"\n",
    "    matrix_1 = diagonal * np.eye(n_states)\n",
    "    matrix_2 = (1 - diagonal) * (np.ones((n_states, n_states)) - np.eye(n_states)) / (n_states - 1)\n",
    "    transition_matrix = matrix_1 + matrix_2\n",
    "    return transition_matrix\n",
    "\n",
    "def df_to_array(dataframe):\n",
    "    \"\"\"Convert a DataFrame to a NumPy array without transposing.\"\"\"\n",
    "    data_array = dataframe.to_numpy()  # No transpose here\n",
    "    return data_array\n",
    "\n",
    "def get_parameters(data, n_states=4):\n",
    "    \"\"\"Estimate parameters for each state based on the single time series data.\"\"\"\n",
    "    # Assuming the data is a 1D array\n",
    "    overall_mean = np.mean(data)\n",
    "    overall_std = np.std(data)\n",
    "    \n",
    "    params = np.zeros((n_states, 3))  # mu, phi (set to 0), sigma\n",
    "    \n",
    "    # Example strategy: distribute mu around the overall mean, and sigma as variations of overall std\n",
    "    for state in range(n_states):\n",
    "        params[state, 0] = overall_mean + np.random.uniform(-1, 1) * overall_std * 0.1  # mu\n",
    "        params[state, 2] = overall_std * (1 + np.random.uniform(-0.1, 0.1))  # sigma\n",
    "    \n",
    "    return params\n",
    "\n",
    "def get_densities(data, params):\n",
    "    n_states = params.shape[0]\n",
    "    T = len(data)\n",
    "    # Initialize densities array\n",
    "    densities = np.zeros((n_states, T))\n",
    "    \n",
    "    # Extract parameters for vectorized computation\n",
    "    mu = params[:, 0]  # Shape (n_states,)\n",
    "    sigma = params[:, 2]  # Shape (n_states,)\n",
    "    \n",
    "    # Ensure sigma > 0 to avoid division by zero\n",
    "    sigma = np.clip(sigma, 1e-6, np.inf)\n",
    "    \n",
    "    # Reshape data to align for broadcasting\n",
    "    # Data reshaped to (1, T) to broadcast over states\n",
    "    X = data.reshape(1, T)  # Shape now (1, T)\n",
    "    \n",
    "    # Calculate densities using Gaussian PDF formula, vectorized across states\n",
    "    # mu reshaped to (n_states, 1) and sigma reshaped to (n_states, 1) for broadcasting\n",
    "    densities = (1. / (sigma[:, np.newaxis] * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((X - mu[:, np.newaxis]) / sigma[:, np.newaxis]) ** 2) + 1e-6\n",
    "    \n",
    "    return densities\n",
    "def scaled_forward_backward(densities, transition_matrix):\n",
    "    n_states, T = densities.shape\n",
    "    alpha = np.zeros((n_states, T))\n",
    "    beta = np.zeros((n_states, T))\n",
    "    scale_factors = np.zeros(T)\n",
    "    \n",
    "    # Forward Pass\n",
    "    alpha[:, 0] = densities[:, 0]\n",
    "    scale_factors[0] = 1.0 / np.sum(alpha[:, 0])\n",
    "    alpha[:, 0] *= scale_factors[0]\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        alpha[:, t] = np.dot(transition_matrix.T, alpha[:, t-1]) * densities[:, t]  # Using np.dot for clarity\n",
    "        scale_factors[t] = 1.0 / np.sum(alpha[:, t])\n",
    "        alpha[:, t] *= scale_factors[t]\n",
    "    \n",
    "    # Backward Pass\n",
    "    beta[:, T-1] = scale_factors[T-1]\n",
    "    \n",
    "    for t in range(T-2, -1, -1):\n",
    "        beta[:, t] = np.dot(transition_matrix, densities[:, t+1] * beta[:, t+1])\n",
    "        beta[:, t] *= scale_factors[t]\n",
    "    \n",
    "    return alpha, beta, scale_factors\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def calculate_smoothed_probabilities(alpha, beta, transition_matrix, densities):\n",
    "    T = alpha.shape[1]\n",
    "    N = alpha.shape[0]\n",
    "    \n",
    "    # Smoothed state probabilities\n",
    "    gamma = (alpha * beta) / np.sum(alpha * beta, axis=0)\n",
    "    \n",
    "    # Preallocate xi array\n",
    "    xi = np.zeros((N, N, T-1))\n",
    "    \n",
    "    # Calculate xi for t in [0, T-2] (since it involves t and t+1)\n",
    "    for t in range(T-1):\n",
    "        xi_denominator = np.sum(alpha[:, t].reshape(-1, 1) * transition_matrix * densities[:, t+1] * beta[:, t+1], axis=(0, 1))\n",
    "        xi[:, :, t] = (alpha[:, t].reshape(-1, 1) * transition_matrix * densities[:, t+1] * beta[:, t+1].reshape(1, -1)) / xi_denominator\n",
    "    \n",
    "    return gamma, xi\n",
    "import numpy as np\n",
    "def update_initial_and_transition_matrices(gamma, xi):\n",
    "    \"\"\"\n",
    "    Update initial state probabilities and transition matrix based on smoothed probabilities.\n",
    "    \n",
    "    Parameters:\n",
    "    - gamma: Smoothed state probabilities with shape (N, T), where N is the number of states and T is the number of time steps.\n",
    "    - xi: Smoothed transition probabilities with shape (N, N, T-1).\n",
    "    \n",
    "    Returns:\n",
    "    - initial_state_probabilities: Updated initial state probabilities based on gamma.\n",
    "    - transition_matrix: Updated and normalized transition matrix based on xi.\n",
    "    \"\"\"\n",
    "    N, T = gamma.shape\n",
    "    \n",
    "    # Initial state probabilities are simply the smoothed probabilities at t=0\n",
    "    initial_state_probabilities = gamma[:, 0]\n",
    "    \n",
    "    # Sum xi across time to get cumulative transitions, then average by T-1 (number of transitions)\n",
    "    transition_matrix = np.sum(xi, axis=2) / (T-1)\n",
    "    \n",
    "    # Normalize the rows of the transition matrix to sum to 1\n",
    "    transition_matrix = transition_matrix / transition_matrix.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    return initial_state_probabilities, transition_matrix\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def calculate_parameters(data, smoothed_states):\n",
    "    N, T = smoothed_states.shape\n",
    "    params = np.zeros((N, 3))  # Initializing for mu, phi, sigma\n",
    "    \n",
    "    if data.ndim > 1:\n",
    "        data = data.flatten()  # Ensure data is 1D\n",
    "    \n",
    "    for state in range(N):\n",
    "        weights = smoothed_states[state, :]\n",
    "        \n",
    "        if weights.ndim > 1 or data.ndim > 1:\n",
    "            print(\"Unexpected array dimensions:\", weights.ndim, data.ndim)\n",
    "            continue\n",
    "        \n",
    "        # Calculate weighted mean (mu)\n",
    "        weighted_sum = np.dot(weights, data)\n",
    "        total_weight = weights.sum()\n",
    "        mu = weighted_sum / total_weight if total_weight > 0 else 0\n",
    "\n",
    "        # Initialize sums for phi calculation\n",
    "        phi_numerator, phi_denominator = 0.0, 0.0\n",
    "        for t in range(1, T):  # Start from 1 since we use x[t-1]\n",
    "            x_t = data[t] - mu\n",
    "            x_t_minus_1 = data[t-1] - mu\n",
    "            phi_numerator += weights[t] * x_t * x_t_minus_1\n",
    "            phi_denominator += weights[t] * x_t_minus_1**2\n",
    "        \n",
    "        # Calculate phi\n",
    "        phi = phi_numerator / phi_denominator if phi_denominator > 0 else 0\n",
    "\n",
    "        # Weighted calculation for sigma (standard deviation)\n",
    "        weighted_variance = np.dot(weights, (data - mu)**2) / total_weight if total_weight > 0 else 0\n",
    "        sigma = np.sqrt(weighted_variance) if weighted_variance > 0 else 0\n",
    "\n",
    "        # Assign calculated parameters for the current state\n",
    "        params[state, :] = mu, phi, sigma\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def calculate_log_likelihood(scale_factors):\n",
    "    \"\"\"\n",
    "    Calculate the log likelihood of the observed data given the model.\n",
    "    \n",
    "    Parameters:\n",
    "    - scale_factors: Scaling factors from the forward pass, a 1D NumPy array of length T.\n",
    "    \n",
    "    Returns:\n",
    "    - log_likelihood: The log likelihood of the observed data.\n",
    "    \"\"\"\n",
    "    log_likelihood = np.sum(np.log(scale_factors))\n",
    "    return log_likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3ea4c41-53b2-4311-b2a8-e4e8d938da03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.05765 seconds\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "# reload(EM)\n",
    "# from EM import Base\n",
    "O = 1000\n",
    "K = 2\n",
    "N = 2\n",
    "# phi_vals = np.zeros((K,N))\n",
    "sim = ARsim(n_states=N, K_series = K, num_obs = O, transition_diagonal=0.99, deterministic=True) #sigmas = [[0,0]]\n",
    "sim.simulate()\n",
    "df = sim.data\n",
    "transition_guess=0.95\n",
    "n_states=4\n",
    "max_iterations=100 \n",
    "tolerance=1e-5\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Print elapsed time with 5 digits after the decimal point\n",
    "print(f\"Elapsed time: {elapsed_time:.5f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3e9713e-adcb-4554-b7fa-0608eef1abe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = df_to_array(df)  # Step 1\n",
    "# transition_matrix = create_transition_matrix(n_states, transition_guess)  # Provided Step 2\n",
    "# print(transition_matrix)\n",
    "# parameters = get_parameters(data, n_states)  # Step 4\n",
    "# densities = get_densities(data, parameters)  # Step 5\n",
    "# alpha, beta, scale_factors = scaled_forward_backward(densities, transition_matrix)  # Step 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aa25587-8ec8-4993-8f76-c52a6952b319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.06178 seconds\n",
      "[[ 0.0303717   0.          5.35243681]\n",
      " [-0.49561526  0.          5.16528636]\n",
      " [-0.22051072  0.          4.94637482]\n",
      " [-0.66507866  0.          5.26669214]]\n",
      "[[ 0.06536125 -0.1793779   5.23445705]\n",
      " [-0.41450497 -0.12723425  5.05021325]\n",
      " [-0.10282253 -0.07381964  4.78113331]\n",
      " [-0.5936103  -0.14641698  5.16793715]]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Place the code you want to time here\n",
    "# Example dummy operation: a loop that runs for a bit\n",
    "\n",
    "data = df_to_array(df)  # Step 1\n",
    "transition_matrix = create_transition_matrix(n_states, transition_guess)  # Provided Step 2\n",
    "# print(transition_matrix)\n",
    "parameters = get_parameters(data, n_states)  # Step 4\n",
    "\n",
    "densities = get_densities(data, parameters)  # Step 5\n",
    "alpha, beta, scale_factors = scaled_forward_backward(densities, transition_matrix)  # Step 6\n",
    "smoothed_states, smoothed_transitions = calculate_smoothed_probabilities(alpha, beta, transition_matrix, densities)\n",
    "initial_state_probabilities, updated_transition_matrix = update_initial_and_transition_matrices(smoothed_states, smoothed_transitions )\n",
    "param_2 = calculate_parameters(data, smoothed_states)\n",
    "ll = calculate_log_likelihood(scale_factors)\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Print elapsed time with 5 digits after the decimal point\n",
    "print(f\"Elapsed time: {elapsed_time:.5f} seconds\")\n",
    "print(parameters)\n",
    "print(param_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "402b2e4e-1cd3-4449-899a-63fa5981a3a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09720349,  0.03333938, -0.04448732,  0.09408589]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sim.mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "663c4cd3-51d4-41cf-8841-9d828826b47f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06295246, -0.66076108,  0.2766486 , -0.39040169]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sim.phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7257666e-1c81-4aa8-8d67-23c8929f693b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3039.454334501066"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a627ac4-9497-4945-b3f7-ab650d0e2785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.67787423, 0.47807888, 4.3887414 , 5.014632  ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sim.sigmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff96afea-ad94-4a9c-bfa8-d8b6ee0a204c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.sum(updated_transition_matrix, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f700dd4-ee1b-4f1e-b4be-be738626e065",
   "metadata": {},
   "source": [
    "# RSDC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2df37cb7-f921-411d-8857-16745f45025f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_60707/3332288789.py:20: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  std_series[t] = np.sqrt(omega + alpha * (data[t-1] - 0)**2 + beta * (std_series[t-1])**2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_garch_std(data, omega, alpha, beta):\n",
    "    \"\"\"\n",
    "    Calculate the conditional standard deviation for each time point in a time series\n",
    "    using the GARCH(1,1) model parameters.\n",
    "\n",
    "    Parameters:\n",
    "    - data: 1D NumPy array of time series data.\n",
    "    - omega, alpha, beta: GARCH(1,1) model parameters.\n",
    "\n",
    "    Returns:\n",
    "    - std_series: 1D NumPy array of conditional standard deviations.\n",
    "    \"\"\"\n",
    "    T = len(data)\n",
    "    std_series = np.zeros(T)\n",
    "    std_series[0] = np.sqrt(omega / (1 - alpha - beta))  # Initial standard deviation\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        std_series[t] = np.sqrt(omega + alpha * (data[t-1] - 0)**2 + beta * (std_series[t-1])**2)\n",
    "    \n",
    "    return std_series\n",
    "def calculate_standardized_residuals(data, std_series):\n",
    "    \"\"\"\n",
    "    Calculate the standardized residuals for a time series given\n",
    "    the conditional standard deviations from a GARCH(1,1) model.\n",
    "\n",
    "    Parameters:\n",
    "    - data: 1D NumPy array of time series data.\n",
    "    - std_series: 1D NumPy array of conditional standard deviations.\n",
    "\n",
    "    Returns:\n",
    "    - standardized_residuals: 1D NumPy array of standardized residuals.\n",
    "    \"\"\"\n",
    "    standardized_residuals = data / std_series\n",
    "    return standardized_residuals\n",
    "import numpy as np\n",
    "\n",
    "# def calculate_state_correlations(standardized_residuals, smoothed_probabilities):\n",
    "#     \"\"\"\n",
    "#     Calculate state-specific correlation matrices.\n",
    "\n",
    "#     Parameters:\n",
    "#     - standardized_residuals: 2D NumPy array (T x M) of standardized residuals, where T is the number of time points and M is the number of series.\n",
    "#     - smoothed_probabilities: 2D NumPy array (N x T) of smoothed state probabilities, where N is the number of states.\n",
    "\n",
    "#     Returns:\n",
    "#     - correlation_matrices: 3D NumPy array (N x M x M) of correlation matrices for each state.\n",
    "#     \"\"\"\n",
    "#     N, T = smoothed_probabilities.shape\n",
    "#     _, M = standardized_residuals.shape\n",
    "#     correlation_matrices = np.zeros((N, M, M))\n",
    "\n",
    "#     for n in range(N):\n",
    "#         # Initialize sum of outer products and sum of probabilities for normalization\n",
    "#         sum_outer_products = np.zeros((M, M))\n",
    "#         sum_probabilities = 0.0\n",
    "        \n",
    "#         for t in range(T):\n",
    "#             prob = smoothed_probabilities[n, t]  # Probability of state n at time t\n",
    "#             residuals = standardized_residuals[t, :].reshape(M, 1)  # Column vector of residuals at time t\n",
    "#             outer_product = residuals @ residuals.T  # Outer product\n",
    "            \n",
    "#             # Weighted sum of outer products\n",
    "#             sum_outer_products += prob * outer_product\n",
    "#             # Sum of probabilities for normalization\n",
    "#             sum_probabilities += prob\n",
    "        \n",
    "#         # Normalize to get the correlation matrix for state n\n",
    "#         correlation_matrices[n] = sum_outer_products / sum_probabilities if sum_probabilities > 0 else np.zeros((M, M))\n",
    "        \n",
    "#         # Optionally, adjust the diagonal to 1 if the correlation matrix definition requires it\n",
    "#         np.fill_diagonal(correlation_matrices[n], 1)\n",
    "\n",
    "#     return correlation_matrices\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "def calculate_state_correlations(standardized_residuals, smoothed_probabilities):\n",
    "\n",
    "    N, T = smoothed_probabilities.shape  # N states, T time points\n",
    "    K = standardized_residuals.shape[1]  # K time series\n",
    "\n",
    "    # Initialize matrices to accumulate results\n",
    "    weighted_outer_products = np.zeros((N, K, K))\n",
    "    sum_probabilities = np.zeros((N, 1, 1))\n",
    "\n",
    "    for t in range(T):\n",
    "        # Residuals at time t: shape (K, 1)\n",
    "        u_t = standardized_residuals[t, :].reshape((K, 1))\n",
    "        # Outer product at time t: shape (K, K)\n",
    "        outer_product_t = u_t @ u_t.T\n",
    "        \n",
    "        # Update weighted sums and probabilities\n",
    "        for n in range(N):\n",
    "            weight = smoothed_probabilities[n, t]\n",
    "            weighted_outer_products[n] += weight * outer_product_t\n",
    "            sum_probabilities[n] += weight\n",
    "\n",
    "    # Normalize to compute correlation matrices: shape (N, K, K)\n",
    "    correlation_matrices = np.array([weighted_outer_products[n] / sum_probabilities[n]\n",
    "                                     if sum_probabilities[n] > 0 else np.eye(K)\n",
    "                                     for n in range(N)])\n",
    "\n",
    "    # Ensure diagonal elements are 1\n",
    "    for n in range(N):\n",
    "        np.fill_diagonal(correlation_matrices[n], 1)\n",
    "\n",
    "    return correlation_matrices\n",
    "\n",
    "\n",
    "# Example parameters\n",
    "omega = 0.0001\n",
    "alpha = 0.05\n",
    "beta = 0.94\n",
    "\n",
    "# Calculate conditional standard deviations\n",
    "std_series = calculate_garch_std(data, omega, alpha, beta)\n",
    "\n",
    "# Calculate standardized residuals\n",
    "standardized_residuals = calculate_standardized_residuals(data, std_series)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8df30068-7168-4bab-aeef-af275e335c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-159.34661229030573"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(standardized_residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f3a1e7a-c03d-453f-8d55-004f9912e664",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = calculate_state_correlations(standardized_residuals, smoothed_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b9fb5d4-30d9-434d-a3ad-6bf9ca29b82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1000, 1000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60583bff-29ed-4cd3-a18a-fe6d33bc6b82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6ca663-fea6-46ad-b201-8f7545bff714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
