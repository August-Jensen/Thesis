{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6696e6a-9fec-4cef-8dd0-aec6b7b1f359",
   "metadata": {},
   "source": [
    "# CCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8890c262-b0db-49fe-a35e-433ac011f57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from arch import arch_model\n",
    "# start_time = time.time()\n",
    "# parameters = np.array([[0.05, 0.15, 0.75],\n",
    "#                       [0.1, 0.25, 0.6],\n",
    "#                       [0.15, 0.15, 0.8]])\n",
    "\n",
    "# correlation_matrix =  np.array([[[1, 0.6, 0.4],\n",
    "#                                  [0.6, 1, -0.3],\n",
    "#                                  [0.4, -0.3, 1]]])\n",
    "\n",
    "\n",
    "# decomp = np.linalg.cholesky(correlation_matrix)\n",
    "# decomp\n",
    "# # Number of observations\n",
    "# T = 1000000\n",
    "# K = correlation_matrix.shape[1]\n",
    "# # Initialize arrays to store the processes and variances\n",
    "# processes = np.zeros((K, T))\n",
    "# variances = np.zeros((K, T))\n",
    "# innovations = np.zeros((K, T))\n",
    "\n",
    "# # Initial variance (can be set equal to the long-term variance)\n",
    "# variances[:, 0] = parameters[:, 0] / (1 - parameters[:, 1] - parameters[:, 2])\n",
    "\n",
    "# # Simulate the GARCH processes\n",
    "# for t in range(1, T):\n",
    "#     # Simulate standard normal innovations\n",
    "#     z = np.random.normal(0, 1, K)\n",
    "#     # Apply the Cholesky decomposition to introduce correlation\n",
    "#     correlated_z = decomp @ z\n",
    "#     # Update the variance\n",
    "#     variances[:, t] = parameters[:, 0] + parameters[:, 1] * (processes[:, t-1]**2) + parameters[:, 2] * variances[:, t-1]\n",
    "#     # Calculate the process values\n",
    "#     processes[:, t] = np.sqrt(variances[:, t]) * correlated_z[:]\n",
    "\n",
    "# # Output the first few values to check\n",
    "# print(processes[:, :5])\n",
    "\n",
    "# # End timing\n",
    "# end_time = time.time()\n",
    "\n",
    "# # Calculate elapsed time\n",
    "# elapsed_time = end_time - start_time\n",
    "\n",
    "# # Print elapsed time with 5 digits after the decimal point\n",
    "# print(f\"Elapsed time: {elapsed_time:.5f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58ceb9eb-9971-4895-b842-38958142bbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_deviations(data, univariate_parameters):\n",
    "    K,T = data.shape\n",
    "    sigmas = np.zeros((K,T))\n",
    "    sigmas[:,0] = np.var(data, axis=1)\n",
    "    for t in range(1, T):\n",
    "        sigmas[:,t] = univariate_parameters[:,0] + univariate_parameters[:, 1] * data[:, t-1]**2 + univariate_parameters[:, 2] * sigmas[:, t-1]    \n",
    "    return np.sqrt(sigmas)\n",
    "\n",
    "def calculate_res(data, sigma):\n",
    "    return data / sigma\n",
    "\n",
    "def density(residuals):\n",
    "    K, T = residuals.shape\n",
    "    correlation_matrix = np.sum(np.einsum('it,jt->ijt', residuals, residuals), axis=-1) / T\n",
    "    return correlation_matrix\n",
    "    \n",
    "def cholesky_scale(matrix):\n",
    "    K, E = matrix.shape\n",
    "    P = np.linalg.cholesky(matrix)\n",
    "    for j in range(K):\n",
    "        sum = np.sum(P[j, :j] ** 2)\n",
    "        P[j,j] = np.sqrt(1-sum) if 1 - sum > 0 else 0\n",
    "    scaled = np.dot(P, P.T)\n",
    "    return scaled\n",
    "\n",
    "\n",
    "\n",
    "def estimate_univariate(data):\n",
    "    K, T = data.shape\n",
    "    # Initialize an array to store the omega, alpha, and beta parameters for each series\n",
    "    params_array = np.zeros((K, 3))\n",
    "    \n",
    "    for k in range(K):\n",
    "        # Select the k-th series from the data\n",
    "        series = data[k, :]\n",
    "        \n",
    "        # Fit a GARCH(1,1) model to the series\n",
    "        model = arch_model(series, vol='Garch', p=1, q=1)\n",
    "        model_fit = model.fit(disp='off') # Set disp='off' to avoid printing the fit summary\n",
    "        \n",
    "        # Extract the parameters: omega (constant), alpha (ARCH), beta (GARCH)\n",
    "        omega = model_fit.params['omega']\n",
    "        alpha = model_fit.params['alpha[1]']\n",
    "        beta = model_fit.params['beta[1]']\n",
    "        \n",
    "        # Store the parameters in the params_array\n",
    "        params_array[k, :] = [omega, alpha, beta]\n",
    "        \n",
    "    return params_array\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c63de797-d801-42c0-9334-85c0fc131bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  2 of 2 completed\n",
      "[ 8.24204455e-18 -6.27965299e-18]\n",
      "[*********************100%%**********************]  2 of 2 completed\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "# Seleccionar Criterio de OptimizaciÃ³n\n",
    "optimization_criterion = 'cvar'  # Cambia a 'sharpe', 'cvar', 'sortino' o 'variance' para optimizar esos criterios\n",
    "\n",
    "# Elegir Acciones por agregar al Protafolio y Seleccionar periodo de muestra\n",
    "symbols = ['ACWI', 'TLT',]# 'EURGBP=X', 'EURCAD=X', 'EURCHF=X']\n",
    "start_date = '2015-01-01'\n",
    "end_date = '2024-01-01'\n",
    "\n",
    "data = yf.download(symbols, start=start_date, end=end_date)['Adj Close']\n",
    "\n",
    "# Calcular los retornos\n",
    "returns = data.pct_change().dropna()\n",
    "\n",
    "data = returns.to_numpy().T * 100\n",
    "# print(data)\n",
    "means = np.mean(data, axis=1)\n",
    "data = data - means[:,np.newaxis]\n",
    "\n",
    "print( np.mean(data, axis=1))\n",
    "# # Define the list of stock symbols you're interested in\n",
    "\n",
    "# # Fetch the historical data\n",
    "datas = yf.download(symbols, start=start_date, end=end_date, interval='1d')['Adj Close']\n",
    "\n",
    "# # Calculate the log of the prices\n",
    "log_data = np.log(datas)\n",
    "dates = log_data.diff().dropna().index\n",
    "\n",
    "# # Calculate the first difference of the log data\n",
    "# log = log_data.diff().dropna().to_numpy()\n",
    "# data = log.T * 100 + 1e-6\n",
    "# print(data)\n",
    "# Convert to numpy array\n",
    "#log_returns_array = log_returns.to_numpy()\n",
    "\n",
    "#print(log_returns_array)\n",
    "# data = pd.read_csv('C2A.csv').to_numpy().T * 100\n",
    "K, T = data.shape\n",
    "N = 2\n",
    "univariate_parameters = estimate_univariate(data)\n",
    "\n",
    "# correlation_matrix, transition_matrix, log_hist, u_hat, standard_deviations = fit(data, univariate_parameters, num_states=N, max_it=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bd4aac8-c170-4b94-84d1-4ed897f853e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.         -0.19451368]\n",
      " [-0.19451368  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "std = standard_deviations(data, univariate_parameters)\n",
    "res = calculate_res(data, std)\n",
    "correlation_matrix = density(res)\n",
    "corr = cholesky_scale(correlation_matrix)\n",
    "print(corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "750aab33-64df-46a7-b118-ddbc62ffef6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.928982  , -0.17152457],\n",
       "       [-0.17152457,  0.83703781]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "standard_deviations = np.mean(std, axis=1)\n",
    "diag = np.diag(standard_deviations)\n",
    "cov = diag @ corr @ diag\n",
    "cov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b354270f-d62b-4bec-89bf-6c17120df8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returns Combined :  \n",
      "[0.0387716  0.00290152]\n",
      "Optimal Weigts Combined :  \n",
      "[0.47820266 0.52179734]\n",
      "0.5956010378585088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.016835672919725767"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "import cvxpy as cp\n",
    "\n",
    "def optimize_portfolio(cov_matrix, expected_returns, target_return):\n",
    "    \"\"\"\n",
    "    Finds the optimal portfolio weights using Markowitz optimization.\n",
    "    \n",
    "    Parameters:\n",
    "    - cov_matrix: A covariance matrix of asset returns.\n",
    "    - expected_returns: An array of expected returns for each asset.\n",
    "    - target_return: The target return for the portfolio.\n",
    "    \n",
    "    Returns:\n",
    "    - A tuple (weights, risk), where 'weights' is an array of optimal asset weights, and 'risk' is the portfolio risk (standard deviation).\n",
    "    \"\"\"\n",
    "    num_assets = len(expected_returns)\n",
    "    \n",
    "    # Define the optimization variables\n",
    "    weights = cp.Variable(num_assets)\n",
    "    \n",
    "    # Define the objective function (minimize portfolio variance)\n",
    "    portfolio_variance = cp.quad_form(weights, cov_matrix)\n",
    "    objective = cp.Minimize(portfolio_variance)\n",
    "    \n",
    "    # Define the constraints\n",
    "    constraints = [\n",
    "        cp.sum(weights) == 1,  # Sum of weights must be 1\n",
    "        weights >= 0,          # No short selling\n",
    "        cp.matmul(weights, expected_returns) >= target_return  # Target return constraint\n",
    "    ]\n",
    "    \n",
    "    # Define and solve the problem\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    problem.solve()\n",
    "    \n",
    "    # Extract the optimal weights and calculate portfolio risk\n",
    "    optimal_weights = weights.value\n",
    "    portfolio_risk = cp.sqrt(portfolio_variance).value\n",
    "    \n",
    "    return optimal_weights, portfolio_risk\n",
    "\n",
    " # Correct matrix multiplication for covariance\n",
    "data =  data + means[:,np.newaxis]\n",
    "# mu_1 = np.mean(data * u_hat[0], axis=1)  # Expected returns vector\n",
    "# mu_2 = np.mean(data * u_hat[1], axis=1)  # Expected returns vector\n",
    "mu = np.mean((data),axis=1)\n",
    "mu_p = 0.02  # Target return\n",
    "print(f'Returns Combined :  \\n{mu}')\n",
    "# print(f'Returns in state 1:  \\n{mu_1}')\n",
    "# print(f'Returns in state 2:  \\n{mu_2}')\n",
    "\n",
    "\n",
    "# optimal_weights_1, portfolio_risk_1 = optimize_portfolio(cov_1, mu_1, mu_p)\n",
    "# optimal_weights_2, portfolio_risk_2 = optimize_portfolio(cov_2, mu_2, mu_p)\n",
    "optimal_weights, portfolio_risk = optimize_portfolio(cov, mu, mu_p)\n",
    "print(f'Optimal Weigts Combined :  \\n{optimal_weights}')\n",
    "# print(f'Optimal Weigts in state 1:  \\n{optimal_weights_1}')\n",
    "# print(f'Optimal Weigts in state 2:  \\n{optimal_weights_2}')\n",
    "print(portfolio_risk)#, portfolio_risk_1, portfolio_risk_2)\n",
    "retun = optimal_weights * mu\n",
    "mean_return = np.mean(retun)\n",
    "sharpe = mean_return / portfolio_risk\n",
    "sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fa65609-94d3-46c5-95f8-c214f7a6bb4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00033832175723903633"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "# T-statistic from the user's message\n",
    "t_stat = 3.58678288007606\n",
    "\n",
    "# Degrees of freedom from the user's message\n",
    "df = 4524\n",
    "\n",
    "# Calculate the p-value for the two-tailed test for the given t-statistic\n",
    "# Since it's a two-tailed test, we need to multiply by 2\n",
    "p_value = t.sf(t_stat, df) * 2  # sf is the survival function (1 - cdf)\n",
    "\n",
    "p_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d394b2-4777-4ab9-b2ae-787c8d9a7ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b45cd216-c5b6-4a27-bb23-392ed35c1047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returns: [0.03685569 0.00353043]\n",
      "Optimal Weigts Combined :  \n",
      "[0.48428832 0.51571168]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.017743666752914767"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "import cvxpy as cp\n",
    "\n",
    "def optimize_portfolio(cov_matrix, expected_returns, target_return):\n",
    "    \"\"\"\n",
    "    Finds the optimal portfolio weights using Markowitz optimization.\n",
    "    \n",
    "    Parameters:\n",
    "    - cov_matrix: A covariance matrix of asset returns.\n",
    "    - expected_returns: An array of expected returns for each asset.\n",
    "    - target_return: The target return for the portfolio.\n",
    "    \n",
    "    Returns:\n",
    "    - A tuple (weights, risk), where 'weights' is an array of optimal asset weights, and 'risk' is the portfolio risk (standard deviation).\n",
    "    \"\"\"\n",
    "    num_assets = len(expected_returns)\n",
    "    \n",
    "    # Define the optimization variables\n",
    "    weights = cp.Variable(num_assets)\n",
    "    \n",
    "    # Define the objective function (minimize portfolio variance)\n",
    "    portfolio_variance = cp.quad_form(weights, cov_matrix)\n",
    "    objective = cp.Minimize(portfolio_variance)\n",
    "    \n",
    "    # Define the constraints\n",
    "    constraints = [\n",
    "        cp.sum(weights) == 1,  # Sum of weights must be 1\n",
    "        weights >= 0,          # No short selling\n",
    "        cp.matmul(weights, expected_returns) >= target_return  # Target return constraint\n",
    "    ]\n",
    "    \n",
    "    # Define and solve the problem\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    problem.solve()\n",
    "    \n",
    "    # Extract the optimal weights and calculate portfolio risk\n",
    "    optimal_weights = weights.value\n",
    "    portfolio_risk = cp.sqrt(portfolio_variance).value\n",
    "    \n",
    "    return optimal_weights, portfolio_risk\n",
    "\n",
    " # Correct matrix multiplication for covariance\n",
    "\n",
    "\n",
    "mu = np.mean(data, axis=1)\n",
    "mu_p = 0.01  # Target return\n",
    "print(f'Returns: {mu}')\n",
    "\n",
    "optimal_weights, portfolio_risk = optimize_portfolio(cov, mu, mu_p)\n",
    "print(f'Optimal Weigts Combined :  \\n{optimal_weights}')\n",
    "\n",
    "portfolio_risk\n",
    "retun = optimal_weights * mu\n",
    "mean_return = np.mean(retun)\n",
    "sharpe = mean_return / portfolio_risk\n",
    "sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "016ddd7a-111a-4c95-8e14-b26e3b324353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71723279-c4f4-45a0-acfc-fff29f861968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.018035913471360313"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7424f577-2000-45d5-b86b-0a87580473d4",
   "metadata": {},
   "source": [
    "# Multi state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc0b51a7-9aa3-4f70-9dd1-d2cd7d376e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 384.48471 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "start_time = time.time()\n",
    "parameters = np.array([[0.05, 0.15, 0.75],\n",
    "                      [0.1, 0.25, 0.6],\n",
    "                      [0.15, 0.15, 0.8],\n",
    "                      [0.2, 0.3, 0.65]])\n",
    "\n",
    "true_correlation_matrix =  matrices = np.array([\n",
    "    # High correlations\n",
    "    [[1, 0.8, 0.7, 0.6],\n",
    "     [0.8, 1, 0.65, 0.55],\n",
    "     [0.7, 0.65, 1, 0.5],\n",
    "     [0.6, 0.55, 0.5, 1]],\n",
    "    \n",
    "    # Low correlations\n",
    "    [[1, 0.1, 0.15, 0.05],\n",
    "     [0.1, 1, 0.2, 0.1],\n",
    "     [0.15, 0.2, 1, 0.12],\n",
    "     [0.05, 0.1, 0.12, 1]],\n",
    "    \n",
    "    # Negative correlations\n",
    "    [[1, -0.5, -0.4, -0.3],\n",
    "     [-0.5, 1, -0.2, -0.1],\n",
    "     [-0.4, -0.2, 1, -0.25],\n",
    "     [-0.3, -0.1, -0.25, 1]],\n",
    "    \n",
    "    # In between\n",
    "    [[1, 0.4, -0.3, 0.2],\n",
    "     [0.4, 1, 0.25, -0.2],\n",
    "     [-0.3, 0.25, 1, 0.15],\n",
    "     [0.2, -0.2, 0.15, 1]]\n",
    "])\n",
    "diagonal = 0.99\n",
    "N = 4\n",
    "K = 4\n",
    "transition_matrix = diagonal * np.eye(N) + (1-diagonal) * (np.ones((N,N)) - np.eye(N,N)) / (N - 1)\n",
    "decomp = np.zeros((N, K, K))\n",
    "for n in range(N):\n",
    "    decomp[n,:,:] = choleski_form(true_correlation_matrix[n,:,:])\n",
    "\n",
    "\n",
    "# Number of observations\n",
    "T = 10000000\n",
    "\n",
    "# Initialize arrays to store the processes and variances\n",
    "processes = np.zeros((K, T))\n",
    "variances = np.zeros((K, T))\n",
    "states = np.zeros((T))\n",
    "innovations = np.zeros((K, T))\n",
    "\n",
    "# Initial variance (can be set equal to the long-term variance)\n",
    "variances[:, 0] = parameters[:, 0] / (1 - parameters[:, 1] - parameters[:, 2])\n",
    "states[0] = 0\n",
    "# Simulate the GARCH processes\n",
    "for t in range(1, T):\n",
    "    state = int(states[t-1])\n",
    "    current_state = np.random.choice(a=[0,1,2,3], p=transition_matrix[state])\n",
    "    states[t] = current_state\n",
    "    \n",
    "    # Simulate standard normal innovations\n",
    "    z = np.random.normal(0, 1, K)\n",
    "    # Apply the Cholesky decomposition to introduce correlation\n",
    "    correlated_z = decomp[current_state,:,:] @ z\n",
    "    # Update the variance\n",
    "    variances[:, t] = parameters[:, 0] + parameters[:, 1] * (processes[:, t-1]**2) + parameters[:, 2] * variances[:, t-1]\n",
    "    # Calculate the process values\n",
    "    processes[:, t] = np.sqrt(variances[:, t]) * correlated_z[:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Print elapsed time with 5 digits after the decimal point\n",
    "print(f\"Elapsed time: {elapsed_time:.5f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42147427-0a91-42c7-bccd-1f1325df56d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e04c7b44-13d7-478c-9498-9ed8f010db7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.          0.8019691   0.70110199  0.60104857]\n",
      "  [ 0.8019691   1.          0.64973254  0.54983106]\n",
      "  [ 0.70110199  0.64973254  1.          0.50393924]\n",
      "  [ 0.60104857  0.54983106  0.50393924  1.        ]]\n",
      "\n",
      " [[ 1.          0.10025644  0.15013322  0.04949751]\n",
      "  [ 0.10025644  1.          0.20025296  0.09852301]\n",
      "  [ 0.15013322  0.20025296  1.          0.12017861]\n",
      "  [ 0.04949751  0.09852301  0.12017861  1.        ]]\n",
      "\n",
      " [[ 1.         -0.49841708 -0.39797933 -0.29952701]\n",
      "  [-0.49841708  1.         -0.20194527 -0.09944296]\n",
      "  [-0.39797933 -0.20194527  1.         -0.24991191]\n",
      "  [-0.29952701 -0.09944296 -0.24991191  1.        ]]\n",
      "\n",
      " [[ 1.          0.40045791 -0.29903808  0.19978641]\n",
      "  [ 0.40045791  1.          0.2499614  -0.19981725]\n",
      "  [-0.29903808  0.2499614   1.          0.15010042]\n",
      "  [ 0.19978641 -0.19981725  0.15010042  1.        ]]]\n",
      "Elapsed time: 41.94825 seconds\n"
     ]
    }
   ],
   "source": [
    "def choleski_form(matrix):\n",
    "    return np.linalg.cholesky(matrix)\n",
    "def standard_deviations(data, univariate_parameters):\n",
    "    K,T = data.shape\n",
    "    sigmas = np.zeros((K,T))\n",
    "    sigmas[:,0] = np.var(data, axis=1)\n",
    "    for t in range(1, T):\n",
    "        sigmas[:,t] = univariate_parameters[:,0] + univariate_parameters[:, 1] * data[:, t-1]**2 + univariate_parameters[:, 2] * sigmas[:, t-1]    \n",
    "    return np.sqrt(sigmas)\n",
    "\n",
    "def calculate_res(data, sigma):\n",
    "    return data / sigma\n",
    "\n",
    "def density(residuals,states):\n",
    "    K, T = residuals.shape\n",
    "    sum_states = np.sum(states, axis=-1)\n",
    "    correlation_matrix = np.sum(np.einsum('it,jt,nt->nijt', residuals, residuals, states), axis=-1) /sum_states\n",
    "    return correlation_matrix\n",
    "    \n",
    "def cholesky_scale(matrix):\n",
    "    K, E = matrix.shape\n",
    "    P = np.linalg.cholesky(matrix)\n",
    "    for j in range(K):\n",
    "        sum = np.sum(P[j, :j] ** 2)\n",
    "        P[j,j] = np.sqrt(1-sum) if 1 - sum > 0 else 0\n",
    "    scaled = np.dot(P, P.T)\n",
    "    return scaled\n",
    "\n",
    "def form_states(states, N, T):\n",
    "    # Ensure states are integers within the correct range\n",
    "    states = np.clip(states.astype(int), 0, N-1)\n",
    "    \n",
    "    # Initialize the state array with zeros\n",
    "    state_array = np.zeros((N, T))\n",
    "    \n",
    "    # Mark ones for active states\n",
    "    for t, state in enumerate(states):\n",
    "        state_array[state, t] = 1\n",
    "    \n",
    "    return state_array\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "std = standard_deviations(processes, parameters)\n",
    "res = calculate_res(processes, std)\n",
    "true_states = form_states(states, N, T)\n",
    "correlation_matrix = density(res, true_states)\n",
    "scaled = np.zeros((N, K, K))\n",
    "for n in range(N):\n",
    "    scaled[n,:,:] = cholesky_scale(correlation_matrix[n,:,:])\n",
    "\n",
    "\n",
    "print(scaled)\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Print elapsed time with 5 digits after the decimal point\n",
    "print(f\"Elapsed time: {elapsed_time:.5f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f2d866-84ca-47f7-a06f-5e167c0c90ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760bd6fb-5664-4a86-be05-beee0a0b2e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac51218-c858-4bfb-81e4-dd4ec6cdecff",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a41c3d9b-27b0-4758-9ebd-fcb1bb7a6113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0039392418367781845,\n",
       " -0.0019452746091170592,\n",
       " array([[[ 0.00000000e+00,  1.96909865e-03,  1.10199299e-03,\n",
       "           1.04857276e-03],\n",
       "         [ 1.96909865e-03,  0.00000000e+00, -2.67456206e-04,\n",
       "          -1.68936372e-04],\n",
       "         [ 1.10199299e-03, -2.67456206e-04,  0.00000000e+00,\n",
       "           3.93924184e-03],\n",
       "         [ 1.04857276e-03, -1.68936372e-04,  3.93924184e-03,\n",
       "           2.22044605e-16]],\n",
       " \n",
       "        [[ 0.00000000e+00,  2.56437986e-04,  1.33222412e-04,\n",
       "          -5.02488696e-04],\n",
       "         [ 2.56437986e-04,  0.00000000e+00,  2.52957837e-04,\n",
       "          -1.47698584e-03],\n",
       "         [ 1.33222412e-04,  2.52957837e-04,  0.00000000e+00,\n",
       "           1.78610192e-04],\n",
       "         [-5.02488696e-04, -1.47698584e-03,  1.78610192e-04,\n",
       "          -1.11022302e-16]],\n",
       " \n",
       "        [[ 0.00000000e+00,  1.58292403e-03,  2.02067146e-03,\n",
       "           4.72987738e-04],\n",
       "         [ 1.58292403e-03,  0.00000000e+00, -1.94527461e-03,\n",
       "           5.57036258e-04],\n",
       "         [ 2.02067146e-03, -1.94527461e-03,  0.00000000e+00,\n",
       "           8.80871274e-05],\n",
       "         [ 4.72987738e-04,  5.57036258e-04,  8.80871274e-05,\n",
       "           0.00000000e+00]],\n",
       " \n",
       "        [[ 0.00000000e+00,  4.57905261e-04,  9.61919497e-04,\n",
       "          -2.13593998e-04],\n",
       "         [ 4.57905261e-04,  0.00000000e+00, -3.86041402e-05,\n",
       "           1.82745143e-04],\n",
       "         [ 9.61919497e-04, -3.86041402e-05,  0.00000000e+00,\n",
       "           1.00418850e-04],\n",
       "         [-2.13593998e-04,  1.82745143e-04,  1.00418850e-04,\n",
       "           0.00000000e+00]]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emission  = scaled - true_correlation_matrix\n",
    "np.max(emission),np.min(emission), emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0cbba4-2085-4f0e-b955-d89676881697",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
