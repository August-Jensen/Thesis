{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ffdcd62-78ce-4933-a42f-201665b26355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128eb767-5fb5-4997-91b6-25faa6819e70",
   "metadata": {},
   "source": [
    "# This is an attempt at QMLE using the Newton-Raphson Algorithm.\n",
    "We start by simulating a GARCH process by:\n",
    "$$x_t = \\sigma_t z_t, \\qquad \\sigma_t^2=\\omega+\\alpha x_{t-1} ^ 2 + \\beta \\sigma_{t-1}^2, \\qquad z_t \\sim i.i.d N(0,1)$$\n",
    "First the basic Function for simulating this process, and the settings for the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f905fd23-8918-427b-b393-468e42e125c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Settings: Set seed.\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "#Draw parameters\n",
    "omega_values = [0.01, 0.1]\n",
    "alpha_values = [0.05, 0.25]\n",
    "beta_values = [[0.6 - alpha, 1 - alpha] for alpha in alpha_values]\n",
    "\n",
    "\n",
    "# Drawing parameters uniformly from the specified ranges\n",
    "\n",
    "omega = np.random.uniform(low=0.01, high=0.1)\n",
    "\n",
    "alpha = np.random.uniform(low=0.05, high=0.25)\n",
    "\n",
    "beta = np.random.uniform(low=(0.6 - alpha), high=(1 - alpha))\n",
    "\n",
    "\n",
    "\n",
    "def simulate_garch(T, omega, alpha, beta):\n",
    "    \"\"\"\n",
    "    Simulates a GARCH(1,1) process.\n",
    "\n",
    "    Parameters:\n",
    "    - T: Number of observations.\n",
    "    - omega: Constant term in the GARCH(1,1) model.\n",
    "    - alpha: Coefficient for lagged squared innovations in the GARCH(1,1) model.\n",
    "    - beta: Coefficient for lagged conditional variances in the GARCH(1,1) model.\n",
    "\n",
    "    Returns:\n",
    "    - x: Simulated asset returns (rt).\n",
    "    - sigma: Conditional variances of the simulated asset returns (ht).\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize arrays\n",
    "    x = np.zeros(T)\n",
    "    sigma = np.zeros(T)\n",
    "    epsilon = np.random.normal(0, 1, T)\n",
    "\n",
    "    # Assuming the unconditional variance for initialization\n",
    "    sigma[0] = omega / (1 - alpha - beta)\n",
    "\n",
    "    # Generate the GARCH(1,1) process\n",
    "    for t in range(1, T):\n",
    "        sigma[t] = omega + alpha * (epsilon[t-1]**2) + beta * sigma[t-1]\n",
    "        x[t] = np.sqrt(sigma[t]) * epsilon[t]\n",
    "     \n",
    "    return x, sigma\n",
    "\n",
    "\n",
    "def log_likelihood_function(parameters, returns):\n",
    "    # Unpack Parameters\n",
    "    omega, alpha, beta = parameters\n",
    "    \n",
    "    # Number of observations\n",
    "    T = len(returns)\n",
    "    \n",
    "    # Pre-allocate h (conditional variances) with zeros\n",
    "    sigma = np.zeros(T)\n",
    "    \n",
    "    # Initialize h[0] to the variance of the returns as a simple starting value\n",
    "    sigma[0] = np.var(returns)\n",
    "    \n",
    "    # Calculate conditional variances h_t for t = 1 to T-1\n",
    "    for t in range(1, T):\n",
    "        sigma[t] = np.max((omega + alpha * (returns[t-1] ** 2) + beta * sigma[t-1],1e-6))\n",
    "    \n",
    "    # Calculate log likelihood contributions\n",
    "    log_likelihood_contributions = -0.5 * np.log(2 * np.pi) - 0.5 * np.log(sigma) - 0.5 * (returns ** 2) / sigma\n",
    "    \n",
    "    # Sum to get total log likelihood\n",
    "    total_log_likelihood = np.sum(log_likelihood_contributions)\n",
    "    \n",
    "    return total_log_likelihood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4f0f1d-3d9c-4a31-9d6b-bfe0a105d84c",
   "metadata": {},
   "source": [
    "# Simulate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2caeba00-3c58-4bc0-a2bb-583666610dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.04370861069626263,\n",
       " 0.24014286128198326,\n",
       " 0.6526547154425788,\n",
       " array([ 0.        ,  0.24839439,  0.19009481,  0.6107548 , -0.42182401]),\n",
       " array([0.40772036, 0.60669242, 0.46409147, 0.36529859, 0.52734221]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulate the GARCH(1,1) process\n",
    "num_obs = 1000  # Number of observations\n",
    "\n",
    "x, sigma = simulate_garch(num_obs, omega, alpha, beta)\n",
    "\n",
    "\n",
    "# Output the drawn parameters and the first 5 values of x and sigma\n",
    "\n",
    "omega, alpha, beta, x[:5], sigma[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffe85fd-d7d1-4278-a21a-44598611c288",
   "metadata": {},
   "source": [
    "# Newton Raphson Algorithm:\n",
    "The Algorithm is passed a function to maximize, the data, the initial guess and the bounds.\n",
    "\n",
    "**Grid Search:** Could start with grid_search (If True, uses bounds, and takes a value, for how fine the grid is). \n",
    "\n",
    "- Can use ficed grid, alternating grid, random grid \n",
    "\n",
    "Taylor expansion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c6ea2343-5a6c-4bcd-a153-baa0fe28a4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fit(objective_function, data, initial_guess=None, bounds=None, M=5, max_iterations=1000, tolerance=1e-4):\n",
    "    # Setup of before the estimation\n",
    "    theta, num_params, bounds, log_likelihood_history, parameter_history = setup(objective_function, data, initial_guess, M, bounds, max_iterations)\n",
    "    epsilon = np.sqrt(np.finfo(float).eps)\n",
    "   \n",
    "    # The Main steps of the Algorithm:\n",
    "    for iteration in range(max_iterations):\n",
    "        print(iteration)\n",
    "        # Calculate Gradient\n",
    "        gradient = calculate_gradient(objective_function, data, theta, epsilon)\n",
    "    \n",
    "        # Calculate Hessian\n",
    "        Hessian = calculate_hessian(objective_function, data, theta, epsilon)\n",
    "\n",
    "        \n",
    "        # Update Parameter histories, by np.linalg.solve for better numerical stability instead of directly inverting the Hessian\n",
    "        update_direction = np.linalg.solve(Hessian, -gradient)\n",
    "        theta = update_direction\n",
    "        parameter_history[iteration,:] = theta\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(update_direction, ord=2) < tolerance:\n",
    "            print(f\"Convergence reached after {iteration+1} iterations.\")\n",
    "            break\n",
    "    else:  # This else corresponds to the for-loop, executed if the loop completes normally without a break\n",
    "        print(f\"Maximum iterations reached without convergence.\")\n",
    "    return theta\n",
    "    \n",
    "def transform_parameters(theta):\n",
    "    # Assuming theta contains the unconstrained versions of omega, alpha, beta\n",
    "    omega = np.exp(theta[0])  # Omega must be positive\n",
    "    alpha = np.exp(theta[1]) / (1 + np.exp(theta[1]) + np.exp(theta[2]))\n",
    "    beta = np.exp(theta[2]) / (1 + np.exp(theta[1]) + np.exp(theta[2]))\n",
    "    return np.array([omega, alpha, beta])\n",
    "\n",
    "def inverse_transform_parameters(omega, alpha, beta):\n",
    "    # This is more challenging because you need to find theta such that\n",
    "    # the transformations yield the original omega, alpha, beta\n",
    "    # This step is typically not needed for optimization, but might be useful for interpretation\n",
    "    # Placeholder for conceptual purposes\n",
    "    return np.array([np.log(omega), np.log(alpha / (1 - alpha - beta)), np.log(beta / (1 - alpha - beta))])\n",
    "\n",
    "def modified_objective_function(theta, data):\n",
    "    transformed_params = transform_parameters(theta)\n",
    "    return -log_likelihood_function(transformed_params, data)  # Negate for minimization\n",
    "\n",
    "def setup(objective_function, data, initial_guess, M, bounds, max_iterations):\n",
    "    \"\"\"\n",
    "    Creates the logic setup of the algoritm\n",
    "        Parameter_estimates\n",
    "        Log_likelihood_values\n",
    "        Objective Function\n",
    "        Initial Guess\n",
    "        Bounds\n",
    "    \"\"\"\n",
    "    # Create bounds\n",
    "    if bounds is None:\n",
    "        bounds = ((0,1),(0,1),(0,1))\n",
    "        \n",
    "    # Set Initial Parameter Guess\n",
    "    if initial_guess is None:\n",
    "        initial_guess, initial_log_likelihood = grid_search(objective_function, data, M, bounds)\n",
    "    else: \n",
    "        initial_guess = initial_guess\n",
    "        initial_log_likelihood = objective_function(initial_guess, data)\n",
    "    # Store number of parameters for AIC & BIC \n",
    "    num_params = len(initial_guess)\n",
    "\n",
    "    # set objective_function\n",
    "\n",
    "\n",
    "    # Bounds?\n",
    "    # Do nothing\n",
    "\n",
    "    # Histories\n",
    "    log_likelihood_history = np.zeros(max_iterations)\n",
    "    log_likelihood_history[0] = initial_log_likelihood # Set the first log likelihood history\n",
    "    # Parameter History\n",
    "    parameter_history = np.zeros((max_iterations, num_params))\n",
    "    parameter_history[0,:] = initial_guess \n",
    "    return initial_guess, num_params, bounds, log_likelihood_history, parameter_history\n",
    "\n",
    "\n",
    "\n",
    "def grid_search(objective_function, data, M, bounds):\n",
    "    \"\"\"\n",
    "        This function Creates a Grid using create_grid\n",
    "        And then searches for the optimal starting values, based on the grid, using search_grid\n",
    "\n",
    "        create_grid takes M, bounds, and creates a grid. \n",
    "            For bounds (A,B) it create M instances between A and B, uniformly distributed\n",
    "            For bounds (A, None) it uses an exponential distribution, to have a finer mesh around A, and a coarser mesh, the further from A\n",
    "\n",
    "        search_grid runs the objective_function, and tracks what value of the parameters leads to the largest log likelihood value.\n",
    "            \n",
    "    \"\"\"\n",
    "    # Create The Grid\n",
    "    mesh_grid = create_grid(bounds, M)\n",
    "\n",
    "    # Search The Grid\n",
    "    initial_guess, initial_log_likelihood = search_grid(objective_function, data, mesh_grid)\n",
    "    return initial_guess, initial_log_likelihood\n",
    "\n",
    "def create_grid(bounds, M, focus_factor=5):\n",
    "    \"\"\"\n",
    "    Create a grid for each parameter within the given bounds, with M values for each parameter.\n",
    "    If a bound is (A, None), it creates a finer grid around A and a more coarse grid afterwards.\n",
    "\n",
    "    :param bounds: List of tuples representing the bounds for each parameter. (min, max), with None indicating no bound.\n",
    "    :param M: int, number of values for each parameter.\n",
    "    :param focus_factor: int, factor by which the grid is finer around the specified start of the interval.\n",
    "    :return: np.ndarray, grid for each parameter.\n",
    "    \"\"\"\n",
    "    grids = []\n",
    "    for min_val, max_val in bounds:\n",
    "        if max_val is None:\n",
    "            # Create a focused grid around min_val then expand coarsely\n",
    "            # Use logarithmic spacing for the coarser part\n",
    "            fine_grid_part = np.linspace(min_val, min_val * 2, M // focus_factor, endpoint=False)\n",
    "            coarse_grid_part = np.logspace(np.log10(min_val * 2), np.log10(min_val * 10), M - len(fine_grid_part))\n",
    "            grid = np.concatenate((fine_grid_part, coarse_grid_part))\n",
    "        else:\n",
    "            # Regular linear spacing\n",
    "            grid = np.linspace(min_val, max_val, M)\n",
    "        grids.append(grid)\n",
    "    return np.meshgrid(*grids, indexing='ij')\n",
    "\n",
    "\n",
    "def search_grid(objective_function, data, grid):\n",
    "    \"\"\"\n",
    "    Perform a grid search over the specified parameter grid, evaluating the objective_function\n",
    "    at each point and returning the parameters that maximize the objective function.\n",
    "\n",
    "    :param objective_function: Callable, the objective function to maximize. It should take as many\n",
    "                               arguments as there are parameters, and return a scalar value.\n",
    "    :param grid: List of np.ndarrays, the grid of parameters generated by create_grid.\n",
    "    :return: Tuple, the set of parameters that maximize the objective function and the maximum value.\n",
    "    \"\"\"\n",
    "    max_value = float('-inf')\n",
    "    best_params = None\n",
    "    \n",
    "    # Iterate over each point in the grid\n",
    "    for index in np.ndindex(*grid[0].shape):\n",
    "        # Extract the parameters for the current point\n",
    "        params = [grid[i][index] for i in range(len(grid))]\n",
    "        # Evaluate the objective function\n",
    "        value = objective_function(params, data)\n",
    "        # Update max_value and best_params if this point is better\n",
    "        if value > max_value:\n",
    "            max_value = value\n",
    "            best_params = params\n",
    "    \n",
    "    return best_params, max_value\n",
    "\n",
    "\n",
    "def calculate_gradient(objective_function, data, theta, epsilon):\n",
    "    gradient = np.zeros_like(theta)\n",
    "    for i in range(len(theta)):\n",
    "        theta_plus = np.copy(theta)\n",
    "        theta_minus = np.copy(theta)\n",
    "        theta_plus[i] += epsilon\n",
    "        theta_minus[i] -= epsilon\n",
    "        gradient[i] = (objective_function(theta_plus, data) - objective_function(theta_minus, data)) / (2 * epsilon)\n",
    "    return(gradient)\n",
    "\n",
    "def calculate_hessian(objective_function, data, theta, epsilon, regularization=1e-6):\n",
    "    Hessian = np.zeros((len(theta), len(theta)))\n",
    "    for i in range(len(theta)):\n",
    "        for j in range(len(theta)):\n",
    "            theta_plus_plus = np.copy(theta)\n",
    "            theta_plus_minus = np.copy(theta)\n",
    "            theta_minus_plus = np.copy(theta)\n",
    "            theta_minus_minus = np.copy(theta)\n",
    "            \n",
    "            theta_plus_plus[i] += epsilon\n",
    "            theta_plus_plus[j] += epsilon\n",
    "            \n",
    "            theta_plus_minus[i] += epsilon\n",
    "            theta_plus_minus[j] -= epsilon\n",
    "            \n",
    "            theta_minus_plus[i] -= epsilon\n",
    "            theta_minus_plus[j] += epsilon\n",
    "            \n",
    "            theta_minus_minus[i] -= epsilon\n",
    "            theta_minus_minus[j] -= epsilon\n",
    "            \n",
    "            Hessian[i, j] = (objective_function(theta_plus_plus,data) - objective_function(theta_plus_minus,data) - objective_function(theta_minus_plus,data) + objective_function(theta_minus_minus,data)) / (4 * epsilon**2)\n",
    "    \n",
    "    Hessian += np.eye(len(theta)) * regularization  # Regularization\n",
    "    return Hessian\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c437ca-fb00-411b-a278-c4107031cb2e",
   "metadata": {},
   "source": [
    "# The Log likleihood function for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f09150f9-5716-4360-83b2-f2110ee83ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "Convergence reached after 40 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.43705164e+00, -2.21179399e+08, -2.21179400e+08])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Fit(log_likelihood_function, x, M=3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "100fe756-7d54-4a80-8953-7185a845758e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36259355        nan        nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_224362/623609228.py:1: RuntimeWarning: invalid value encountered in log\n",
      "  print(np.log(a))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.04370861069626263, 0.24014286128198326, 0.6526547154425788)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.log(a))\n",
    "omega, alpha, beta,\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45abf07-9055-4a9c-a3d0-8e257e3f39f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "636f77b8-f2e1-4167-9a4b-c0d5e54addd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1545476366731633"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(omega+1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1fa074c9-6d31-4361-8353-3886cf906494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2714307755780394"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0b4156b9-498a-4cc6-bf93-3f212dff3cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.92063280071104"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(beta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
