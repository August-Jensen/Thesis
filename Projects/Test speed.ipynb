{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9f38d1f-58d8-4f66-bab9-57de0c663ec0",
   "metadata": {},
   "source": [
    "# Simulate 1 million draws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c3a4a91-3f5e-4e92-a424-8fd265469490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5605051517486572, 0.5061841011047363, 0.8745100498199463)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "time_1 = time.time()\n",
    "n = 1000000 # Sample size\n",
    "\n",
    "# AR(1) Model parameters\n",
    "phi_0_ar1 = 0.5\n",
    "phi_1_ar1 = 0.8\n",
    "sigma_ar1 = 0.5\n",
    "\n",
    "# ARCH(1) Model parameters\n",
    "alpha_0_arch1 = 0.2\n",
    "alpha_1_arch1 = 0.5\n",
    "\n",
    "# GARCH(1,1) Model parameters\n",
    "omega_garch11 = 0.1\n",
    "alpha_garch11 = 0.05\n",
    "beta_garch11 = 0.9\n",
    "\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "# Generate sample data for each model\n",
    "start_time = time.time()\n",
    "# AR(1)\n",
    "epsilon_ar1 = np.random.normal(0, sigma_ar1, n)\n",
    "y_ar1 = np.zeros(n)\n",
    "for t in range(1, n):\n",
    "    y_ar1[t] = phi_0_ar1 + phi_1_ar1 * y_ar1[t-1] + epsilon_ar1[t]\n",
    "ar1_time = time.time() - start_time\n",
    "\n",
    "# ARCH(1)\n",
    "epsilon_arch1 = np.random.normal(0, 1, n)  # Standard normal residuals\n",
    "sigma2_arch1 = np.zeros(n)\n",
    "sigma2_arch1[0] = alpha_0_arch1 / (1 - alpha_1_arch1)  # Initial variance\n",
    "for t in range(1, n):\n",
    "    sigma2_arch1[t] = alpha_0_arch1 + alpha_1_arch1 * epsilon_arch1[t-1]**2\n",
    "y_arch1 = np.random.normal(0, np.sqrt(sigma2_arch1))\n",
    "arch1_time = time.time() - start_time - ar1_time\n",
    "\n",
    "# GARCH(1,1)\n",
    "sigma2_garch11 = np.zeros(n)\n",
    "sigma2_garch11[0] = omega_garch11 / (1 - alpha_garch11 - beta_garch11)  # Initial variance\n",
    "for t in range(1, n):\n",
    "    sigma2_garch11[t] = omega_garch11 + alpha_garch11 * epsilon_arch1[t-1]**2 + beta_garch11 * sigma2_garch11[t-1]\n",
    "y_garch11 = np.random.normal(0, np.sqrt(sigma2_garch11))\n",
    "garch11_time = time.time() - start_time - ar1_time - arch1_time\n",
    "\n",
    "# Output the time taken for generating samples\n",
    "(ar1_time, arch1_time, garch11_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3247b34f-b929-4303-ac5c-93ff4305fa0a",
   "metadata": {},
   "source": [
    "# Total Log Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "939e13ac-fc8c-423b-b24c-64a04b85ff22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.013630390167236328, 0.39984917640686035, 0.5719385147094727)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log Likelihood functions for each model\n",
    "def log_likelihood_ar1(y, phi_0, phi_1, sigma2):\n",
    "    y_lag = np.roll(y, 1)  # Roll array to align y_(t-1)\n",
    "    y_lag[0] = 0  # Assume y_0 = 0 for simplicity\n",
    "    epsilon = y - (phi_0 + phi_1 * y_lag)  # Calculate residuals\n",
    "    ll = -0.5 * np.log(2 * np.pi * sigma2) - 0.5 * (epsilon[1:]**2 / sigma2)  # Ignore the first observation\n",
    "    return -np.sum(ll)  # Return negative for minimization\n",
    "\n",
    "def log_likelihood_arch1(y, alpha_0, alpha_1):\n",
    "    mu = np.mean(y)\n",
    "    epsilon = y - mu\n",
    "    sigma2 = np.empty_like(y)\n",
    "    sigma2[0] = np.var(y)  # Initial variance estimate\n",
    "    for t in range(1, len(y)):\n",
    "        sigma2[t] = alpha_0 + alpha_1 * epsilon[t-1]**2\n",
    "    ll = -0.5 * np.log(2 * np.pi * sigma2) - 0.5 * (epsilon**2 / sigma2)\n",
    "    return -np.sum(ll[1:])  # Ignore the first observation\n",
    "\n",
    "def log_likelihood_garch11(y, omega, alpha, beta):\n",
    "    mu = np.mean(y)\n",
    "    epsilon = y - mu\n",
    "    n = len(y)\n",
    "    sigma2 = np.zeros(n)\n",
    "    sigma2[0] = np.var(y)  # Initial variance estimate\n",
    "    for t in range(1, n):\n",
    "        sigma2[t] = omega + alpha * epsilon[t-1]**2 + beta * sigma2[t-1]\n",
    "    ll = -0.5 * np.log(2 * np.pi * sigma2) - 0.5 * (epsilon**2 / sigma2)\n",
    "    return -np.sum(ll[1:])  # Ignore the first observation\n",
    "\n",
    "# Calculate log likelihoods and time each calculation\n",
    "start_time = time.time()\n",
    "ll_ar1 = log_likelihood_ar1(y_ar1, phi_0_ar1, phi_1_ar1, sigma_ar1**2)\n",
    "ll_ar1_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "ll_arch1 = log_likelihood_arch1(y_arch1, alpha_0_arch1, alpha_1_arch1)\n",
    "ll_arch1_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "ll_garch11 = log_likelihood_garch11(y_garch11, omega_garch11, alpha_garch11, beta_garch11)\n",
    "ll_garch11_time = time.time() - start_time\n",
    "\n",
    "(ll_ar1_time, ll_arch1_time, ll_garch11_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ae1cbc-1dc7-4dcd-860c-e62c666c727c",
   "metadata": {},
   "source": [
    "# Density function array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6698ecc9-e6a9-4d30-8746-d2552ecd78ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(999999,) (999999,) (999999,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.019966840744018555, 0.3679928779602051, 0.618161678314209)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate density arrays for each model\n",
    "def density_ar1(y, phi_0, phi_1, sigma2):\n",
    "    y_lag = np.roll(y, 1)\n",
    "    y_lag[0] = 0\n",
    "    epsilon = y - (phi_0 + phi_1 * y_lag)\n",
    "    density = (1 / np.sqrt(2 * np.pi * sigma2)) * np.exp(-0.5 * (epsilon**2 / sigma2))\n",
    "    return density[1:]  # Ignore the first observation for calculation\n",
    "\n",
    "def density_arch1(y, alpha_0, alpha_1):\n",
    "    mu = np.mean(y)\n",
    "    epsilon = y - mu\n",
    "    sigma2 = np.zeros_like(y)\n",
    "    sigma2[0] = np.var(y)  # Starting variance\n",
    "    for t in range(1, len(y)):\n",
    "        sigma2[t] = alpha_0 + alpha_1 * epsilon[t-1]**2\n",
    "    density = (1 / np.sqrt(2 * np.pi * sigma2)) * np.exp(-0.5 * (epsilon**2 / sigma2))\n",
    "    return density[1:]  # Ignore the first observation for calculation\n",
    "\n",
    "def density_garch11(y, omega, alpha, beta):\n",
    "    mu = np.mean(y)\n",
    "    epsilon = y - mu\n",
    "    sigma2 = np.zeros(len(y))\n",
    "    sigma2[0] = np.var(y)  # Starting variance\n",
    "    for t in range(1, len(y)):\n",
    "        sigma2[t] = omega + alpha * epsilon[t-1]**2 + beta * sigma2[t-1]\n",
    "    density = (1 / np.sqrt(2 * np.pi * sigma2)) * np.exp(-0.5 * (epsilon**2 / sigma2))\n",
    "    return density[1:]  # Ignore the first observation for calculation\n",
    "\n",
    "\n",
    "# Timing the density array calculations for each model\n",
    "start_time = time.time()\n",
    "density_array_ar1 = density_ar1(y_ar1, phi_0_ar1, phi_1_ar1, sigma_ar1**2)\n",
    "density_ar1_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "density_array_arch1 = density_arch1(y_arch1, alpha_0_arch1, alpha_1_arch1)\n",
    "density_arch1_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "density_array_garch11 = density_garch11(y_garch11, omega_garch11, alpha_garch11, beta_garch11)\n",
    "density_garch11_time = time.time() - start_time\n",
    "\n",
    "# Verify the shape of the density arrays\n",
    "print(density_array_ar1.shape, density_array_arch1.shape, density_array_garch11.shape)\n",
    "(density_ar1_time, density_arch1_time, density_garch11_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8573ba4f-43aa-4fd9-8d83-e006823fcbf8",
   "metadata": {},
   "source": [
    "# Forward Pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03dfbd83-ab64-4a31-a05e-d7fdf526f263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2403244972229004, 390.55439949035645, 388.55611276626587)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def forward_pass_ar1(y, phi_0, phi_1):\n",
    "    n = len(y)\n",
    "    # Initialize predictions array\n",
    "    predictions = np.zeros(n)\n",
    "    \n",
    "    # Forward pass: Calculate predictions\n",
    "    for t in range(1, n):\n",
    "        predictions[t] = phi_0 + phi_1 * y[t-1]\n",
    "    \n",
    "    # Example of scaling: Normalize predictions to prevent potential numerical instability\n",
    "    # Here, normalization is chosen for simplicity; specific scaling approaches depend on application needs\n",
    "    scaled_predictions = (predictions - np.mean(predictions)) / np.std(predictions)\n",
    "    \n",
    "    return scaled_predictions\n",
    "\n",
    "\n",
    "\n",
    "def forward_pass_arch1(y, alpha_0, alpha_1):\n",
    "    n = len(y)\n",
    "    # Initial estimates\n",
    "    conditional_variances = np.zeros(n)\n",
    "    conditional_variances[0] = np.var(y)  # Use sample variance as initial estimate\n",
    "    \n",
    "    # Forward pass: Calculate conditional variances\n",
    "    for t in range(1, n):\n",
    "        # Here, y[t-1] - mu is the previous period's residual. Assume mu = mean(y) for simplicity.\n",
    "        conditional_variances[t] = alpha_0 + alpha_1 * (y[t-1] - np.mean(y))**2\n",
    "    \n",
    "    # Scaling for numerical stability\n",
    "    scaled_variances = conditional_variances / np.max(conditional_variances)\n",
    "    \n",
    "    return scaled_variances\n",
    "\n",
    "# Assuming y is your time series data and initial parameters are omega, alpha, beta\n",
    "\n",
    "def forward_pass_garch11(y, omega, alpha, beta):\n",
    "    n = len(y)\n",
    "    # Initialize arrays\n",
    "    conditional_variances = np.zeros(n)\n",
    "    conditional_variances[0] = np.var(y)  # Initial estimate\n",
    "    \n",
    "    # Forward pass: Calculate conditional variances\n",
    "    for t in range(1, n):\n",
    "        conditional_variances[t] = omega + alpha * (y[t-1] - np.mean(y[:t]))**2 + beta * conditional_variances[t-1]\n",
    "    \n",
    "    # Scaling for numerical stability (example)\n",
    "    scaled_variances = conditional_variances / np.max(conditional_variances)\n",
    "    \n",
    "    return scaled_variances\n",
    "\n",
    "\n",
    "# Correctly defining the forward pass functions for ARCH(1) and AR(1) models\n",
    "\n",
    "def forward_pass_arch1(y, alpha_0, alpha_1):\n",
    "    n = len(y)\n",
    "    conditional_variances = np.zeros(n)\n",
    "    conditional_variances[0] = np.var(y)  # Initial estimate\n",
    "    for t in range(1, n):\n",
    "        conditional_variances[t] = alpha_0 + alpha_1 * (y[t-1] - np.mean(y))**2\n",
    "    scaled_variances = conditional_variances / np.max(conditional_variances)\n",
    "    return scaled_variances\n",
    "\n",
    "def forward_pass_ar1(y, phi_0, phi_1):\n",
    "    n = len(y)\n",
    "    predictions = np.zeros(n)\n",
    "    for t in range(1, n):\n",
    "        predictions[t] = phi_0 + phi_1 * y[t-1]\n",
    "    scaled_predictions = (predictions - np.mean(predictions)) / np.std(predictions)\n",
    "    return scaled_predictions\n",
    "def forward_pass_garch11(y, omega, alpha, beta):\n",
    "    n = len(y)\n",
    "    conditional_variances = np.zeros(n)\n",
    "    conditional_variances[0] = np.var(y)  # Initial estimate\n",
    "    for t in range(1, n):\n",
    "        conditional_variances[t] = omega + alpha * (y[t-1] - np.mean(y))**2 + beta * conditional_variances[t-1]\n",
    "    scaled_variances = conditional_variances / np.max(conditional_variances)\n",
    "    return scaled_variances\n",
    "\n",
    "# Timed execution for GARCH(1,1)\n",
    "start_time = time.time()\n",
    "scaled_variances_garch11 = forward_pass_garch11(y_garch11, omega_garch11, alpha_garch11, beta_garch11)\n",
    "time_garch11 = time.time() - start_time\n",
    "\n",
    "# Timed execution for ARCH(1)\n",
    "start_time = time.time()\n",
    "scaled_variances_arch1 = forward_pass_arch1(y_arch1, alpha_0_arch1, alpha_1_arch1)\n",
    "time_arch1 = time.time() - start_time\n",
    "\n",
    "# Timed execution for AR(1)\n",
    "start_time = time.time()\n",
    "scaled_predictions_ar1 = forward_pass_ar1(y_ar1, phi_0_ar1, phi_1_ar1)\n",
    "time_ar1 = time.time() - start_time\n",
    "\n",
    "time_ar1, time_arch1, time_garch11\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39211c29-367c-488d-9aac-6ba12f195b5b",
   "metadata": {},
   "source": [
    "# Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30582a4a-7260-417f-b9dd-117e37e1f9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AR(1) Backward Pass Time: 0.3909339904785156\n",
      "ARCH(1) Backward Pass Time: 351.88525557518005\n",
      "GARCH(1,1) Backward Pass Time: 383.9906690120697\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# AR(1) Scaled Backward Pass\n",
    "def scaled_backward_pass_ar1(y, phi_0, phi_1):\n",
    "    n = len(y)\n",
    "    residuals = np.zeros(n)\n",
    "    for t in range(n - 2, -1, -1):  # Iterate backwards\n",
    "        residuals[t] = y[t] - (phi_0 + phi_1 * y[t + 1])\n",
    "    scaled_residuals = residuals / np.max(np.abs(residuals))\n",
    "    return scaled_residuals\n",
    "\n",
    "# ARCH(1) Scaled Backward Pass\n",
    "def scaled_backward_pass_arch1(y, alpha_0, alpha_1):\n",
    "    n = len(y)\n",
    "    conditional_variances = np.zeros(n)\n",
    "    conditional_variances[-1] = alpha_0 / (1 - alpha_1)  # Starting estimate for the last element\n",
    "    for t in range(n - 2, -1, -1):  # Iterate backwards\n",
    "        conditional_variances[t] = alpha_0 + alpha_1 * (y[t] - np.mean(y))**2\n",
    "    scaled_variances = conditional_variances / np.max(conditional_variances)\n",
    "    return scaled_variances\n",
    "\n",
    "# GARCH(1,1) Scaled Backward Pass\n",
    "def scaled_backward_pass_garch11(y, omega, alpha, beta):\n",
    "    n = len(y)\n",
    "    conditional_variances = np.zeros(n)\n",
    "    conditional_variances[-1] = omega / (1 - alpha - beta)  # Starting estimate for the last element\n",
    "    for t in range(n - 2, -1, -1):  # Iterate backwards\n",
    "        conditional_variances[t] = omega + alpha * (y[t] - np.mean(y))**2 + beta * conditional_variances[t + 1]\n",
    "    scaled_variances = conditional_variances / np.max(conditional_variances)\n",
    "    return scaled_variances\n",
    "\n",
    "\n",
    "\n",
    "# Timing and running the backward pass for each model\n",
    "start_time = time.time()\n",
    "scaled_residuals_ar1 = scaled_backward_pass_ar1(y_ar1, phi_0=0.5, phi_1=0.8)\n",
    "time_ar1 = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "scaled_variances_arch1 = scaled_backward_pass_arch1(y_arch1, alpha_0=0.2, alpha_1=0.5)\n",
    "time_arch1 = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "scaled_variances_garch11 = scaled_backward_pass_garch11(y_garch11, omega=0.1, alpha=0.05, beta=0.9)\n",
    "time_garch11 = time.time() - start_time\n",
    "\n",
    "# Output the timing for each\n",
    "print(f\"AR(1) Backward Pass Time: {time_ar1}\")\n",
    "print(f\"ARCH(1) Backward Pass Time: {time_arch1}\")\n",
    "print(f\"GARCH(1,1) Backward Pass Time: {time_garch11}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b7429e-abc0-4719-8f30-a6ee3fa7c6fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ff1395-d199-4eaf-8c39-f4f356bba97a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8fae3d4-58fb-45dc-9c99-2380f4eef2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1519.602458000183\n"
     ]
    }
   ],
   "source": [
    "time_2 = time.time()\n",
    "\n",
    "print(time_2 - time_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7e85d5-f2c8-4e13-80a7-3ad8063f5cd8",
   "metadata": {},
   "source": [
    "100k observations 13.884862899780273 seconds\n",
    "1mil observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d606f4f-df72-49ab-99f7-a8827c515382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.        , 0.0398544 , 0.32017995, 0.18801481, 0.35462698],\n",
       "        [0.11502064, 0.        , 0.67406007, 0.59703711, 0.46125526],\n",
       "        [0.35606545, 0.79527672, 0.        , 0.48884447, 0.66941539],\n",
       "        [0.03979099, 0.21561173, 0.15655972, 0.        , 0.22958734],\n",
       "        [0.14381336, 0.45482681, 0.26770714, 0.90627292, 0.        ]]),\n",
       " array([[0.16527464, 0.17199458, 0.22764526, 0.19946193, 0.23562359],\n",
       "        [0.14984843, 0.13356706, 0.26208365, 0.24265502, 0.21184585],\n",
       "        [0.17355363, 0.26926552, 0.12156161, 0.19819784, 0.2374214 ],\n",
       "        [0.1822796 , 0.21731817, 0.20485666, 0.17516892, 0.22037665],\n",
       "        [0.15369732, 0.20976726, 0.17396932, 0.32945664, 0.13310946]]),\n",
       " array([[0.        , 0.0398544 , 0.32017995, 0.18801481, 0.35462698],\n",
       "        [0.11502064, 0.        , 0.67406007, 0.59703711, 0.46125526],\n",
       "        [0.35606545, 0.79527672, 0.        , 0.48884447, 0.66941539],\n",
       "        [0.03979099, 0.21561173, 0.15655972, 0.        , 0.22958734],\n",
       "        [0.14381336, 0.45482681, 0.26770714, 0.90627292, 0.        ]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def T_to_Gamma(T):\n",
    "    \"\"\"\n",
    "    Transform T (working parameters) to Gamma (natural parameters).\n",
    "    \n",
    "    Parameters:\n",
    "    - T: numpy array of shape (m, m) representing the matrix of working parameters.\n",
    "    \n",
    "    Returns:\n",
    "    - Gamma: numpy array of shape (m, m) representing the matrix of natural parameters.\n",
    "    \"\"\"\n",
    "    exp_T = np.exp(T)\n",
    "    sum_exp_T = np.sum(exp_T, axis=1, keepdims=True) - np.diag(exp_T).reshape(-1, 1)\n",
    "    Gamma = exp_T / (1 + sum_exp_T)\n",
    "    np.fill_diagonal(Gamma, 1 - Gamma.sum(axis=1) + np.diag(Gamma))  # Adjust diagonal to ensure rows sum to 1\n",
    "    return Gamma\n",
    "\n",
    "\n",
    "def Gamma_to_T(Gamma):\n",
    "    \"\"\"\n",
    "    Correctly transform Gamma (natural parameters) to T (working parameters),\n",
    "    ensuring numerical stability and avoiding division by zero or negative numbers.\n",
    "    \n",
    "    Parameters:\n",
    "    - Gamma: numpy array of shape (m, m) representing the matrix of natural parameters.\n",
    "    \n",
    "    Returns:\n",
    "    - T: numpy array of shape (m, m) representing the matrix of working parameters.\n",
    "    \"\"\"\n",
    "    # Subtract row sum (excluding diagonal) from 1 to get diagonal elements\n",
    "    row_sums_excl_diag = Gamma.sum(axis=1) - np.diag(Gamma)\n",
    "    diag_elements = 1 - row_sums_excl_diag\n",
    "    T = np.log(Gamma / diag_elements[:, None])\n",
    "    np.fill_diagonal(T, 0)  # Ensure diagonal elements are 0\n",
    "    \n",
    "    return T\n",
    "\n",
    "\n",
    "m = 5  # Size of the matrix\n",
    "T_example = np.random.rand(m, m)\n",
    "np.fill_diagonal(T_example, 0)  # Ensure diagonal elements are 0\n",
    "\n",
    "Gamma_transformed = T_to_Gamma(T_example)\n",
    "T_transformed_back = Gamma_to_T(Gamma_transformed)\n",
    "\n",
    "T_example, Gamma_transformed, T_transformed_back\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96c4cc76-fe53-46e3-9c76-94d21e34d158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "[[0.    0.01  0.03  0.04  0.05 ]\n",
      " [0.01  0.    0.03  0.04  0.05 ]\n",
      " [0.02  0.001 0.    0.004 0.002]\n",
      " [0.01  0.02  0.12  0.    0.02 ]\n",
      " [0.001 0.004 0.002 0.01  0.   ]]\n",
      "[[0.19483354 0.19679165 0.2007671  0.20278485 0.20482287]\n",
      " [0.19679165 0.19483354 0.2007671  0.20278485 0.20482287]\n",
      " [0.20293582 0.19911643 0.19891742 0.19971468 0.19931565]\n",
      " [0.19506759 0.19702805 0.21774967 0.19312663 0.19702805]\n",
      " [0.19951931 0.20011877 0.19971893 0.20132309 0.19931989]]\n",
      "[1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.        , 0.87392922, 0.90834636, 0.85999983, 0.98355135],\n",
       "        [0.54884945, 0.        , 0.46329687, 0.45816368, 0.57391113],\n",
       "        [0.29435199, 0.36437901, 0.        , 0.56312429, 0.66468966],\n",
       "        [0.77481207, 0.51090214, 0.07032278, 0.        , 0.82493266],\n",
       "        [0.76263927, 0.84931322, 0.51039425, 0.86080962, 0.        ]]),\n",
       " array([[0.        , 0.87392922, 0.90834636, 0.85999983, 0.98355135],\n",
       "        [0.54884945, 0.        , 0.46329687, 0.45816368, 0.57391113],\n",
       "        [0.29435199, 0.36437901, 0.        , 0.56312429, 0.66468966],\n",
       "        [0.77481207, 0.51090214, 0.07032278, 0.        , 0.82493266],\n",
       "        [0.76263927, 0.84931322, 0.51039425, 0.86080962, 0.        ]]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_off_diagonal_params(M):\n",
    "    \"\"\"\n",
    "    Extract off-diagonal parameters from a matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    - M: numpy array of shape (m, m)\n",
    "    \n",
    "    Returns:\n",
    "    - params: List of off-diagonal elements of M.\n",
    "    \"\"\"\n",
    "    m = M.shape[0]\n",
    "    params = [M[i, j] for i in range(m) for j in range(m) if i != j]\n",
    "    return params\n",
    "\n",
    "def construct_matrix_from_params(params, m):\n",
    "    \"\"\"\n",
    "    Construct a matrix from a list of off-diagonal parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    - params: List of off-diagonal elements.\n",
    "    - m: Size of the matrix (m x m)\n",
    "    \n",
    "    Returns:\n",
    "    - M: numpy array of shape (m, m) with off-diagonal elements filled from params.\n",
    "    \"\"\"\n",
    "    if len(params) != m * (m - 1):\n",
    "        raise ValueError(\"Number of parameters does not match the expected number for an m x m matrix.\")\n",
    "    \n",
    "    M = np.zeros((m, m))\n",
    "    param_index = 0\n",
    "    for i in range(m):\n",
    "        for j in range(m):\n",
    "            if i != j:\n",
    "                M[i, j] = params[param_index]\n",
    "                param_index += 1\n",
    "    return M\n",
    "\n",
    "# Example usage:\n",
    "m = 5# Matrix size\n",
    "T_example = np.random.rand(m, m)\n",
    "np.fill_diagonal(T_example, 0)  # Ensure diagonal elements are 0 for the example\n",
    "\n",
    "# Extract parameters and then reconstruct the matrix\n",
    "params = extract_off_diagonal_params(T_example)\n",
    "print(len(params))\n",
    "T_reconstructed = construct_matrix_from_params(params, m)\n",
    "a = np.array((0.01,0.03, 0.04, 0.05,0.01,0.03, 0.04, 0.05,0.02,0.001,0.004,0.002,0.01, 0.02, 0.12,0.02,0.001,0.004,0.002,0.01))\n",
    "b = construct_matrix_from_params(a, m)\n",
    "print(b)\n",
    "c = T_to_Gamma(b)\n",
    "print(c)\n",
    "print(np.sum(c, axis=1))\n",
    "T_example, T_reconstructed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "746a18ef-651c-44f0-92f8-2d7e18bb2d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0. 10. 55.]\n",
      " [39.  0. 32.]\n",
      " [14. 13.  0.]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f06d0a8-9ccb-48a9-9540-f3514c548d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.29958143e-24 2.86251858e-20 1.00000000e+00]\n",
      " [9.99088949e-01 1.22560006e-16 9.11051194e-04]\n",
      " [7.31058134e-01 2.68941258e-01 6.07895834e-07]]\n",
      "[1. 1. 1.]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
